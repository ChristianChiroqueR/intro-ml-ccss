[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción al Machine Learning aplicado a las Ciencias Sociales",
    "section": "",
    "text": "Notas preliminares",
    "crumbs": [
      "Notas preliminares"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Análisis Exploratorio de datos para ML I",
    "section": "",
    "text": "…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de datos para ML I</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#bienvenida",
    "href": "index.html#bienvenida",
    "title": "Introducción al ML aplicado a las CCSS",
    "section": "Bienvenida",
    "text": "Bienvenida\n¡Hola y bienvenid@!\nEs un gusto darte la bienvenida al curso Introducción al Machine Learning aplicado a las Ciencias Sociales de la Pontificia Universidad Católica del Perú.\nEn este espacio encontrarás todo el material del curso: presentaciones, código en R, textos explicativos y recursos audiovisuales complementarios que te ayudarán a reforzar los conceptos vistos en clase.",
    "crumbs": [
      "Sobre el curso"
    ]
  },
  {
    "objectID": "index.html#sílabo",
    "href": "index.html#sílabo",
    "title": "Introducción al Machine Learning aplicado a las Ciencias Sociales",
    "section": "Sílabo",
    "text": "Sílabo",
    "crumbs": [
      "Notas preliminares"
    ]
  },
  {
    "objectID": "index.html#r-un-lenguaje-de-programación",
    "href": "index.html#r-un-lenguaje-de-programación",
    "title": "Introducción al ML aplicado a las CCSS",
    "section": "R, un lenguaje de programación",
    "text": "R, un lenguaje de programación\n\nR es un entorno y lenguaje de programación con un enfoque al análisis estadístico. Primero debemos instalar el R (el software estadístico), el cual lo podemos descargar en el siguiente enlace:\nHaz click aquí para descargar R\nLa versión más reciente, al 13 de junio de 2025, es la 4.5.1 (“Great Square Root”).\nRecuerda que R es un software libre por eso otorga a los usuarios la libertad de usar, estudiar, modificar y distribuir el código fuente. Estas libertades permiten colaboración y mejora continua del software por parte de la comunidad.",
    "crumbs": [
      "Sobre el curso"
    ]
  },
  {
    "objectID": "index.html#r-studio",
    "href": "index.html#r-studio",
    "title": "Introducción al ML aplicado a las CCSS",
    "section": "R Studio",
    "text": "R Studio\n\nRStudio es un entorno de desarrollo integrado (IDE) para el lenguaje de programación R. Facilita el trabajo con R al proporcionar una interfaz gráfica amigable que incluye un editor de código avanzado, herramientas de depuración, visualización de datos y gestión de paquetes, lo que optimiza y simplifica el proceso de análisis de datos y programación estadística.\nLo puedes descargar desde el siguiente link:\nHaz click aquí para descargar R Studio\n\n\n\n\n\n\nAdvertencia\n\n\n\n¡Recuerda que primero debes instalar R y luego R Studio!",
    "crumbs": [
      "Sobre el curso"
    ]
  },
  {
    "objectID": "index.html#manual-de-instalación-de-r-y-r-studio",
    "href": "index.html#manual-de-instalación-de-r-y-r-studio",
    "title": "Introducción al ML aplicado a las CCSS",
    "section": "Manual de instalación de R y R Studio",
    "text": "Manual de instalación de R y R Studio\nTe sugiero ver el siguiente video para que puedas instalar los programas en tu computadora:",
    "crumbs": [
      "Sobre el curso"
    ]
  },
  {
    "objectID": "C1_intro.html",
    "href": "C1_intro.html",
    "title": "1  Introducción al R",
    "section": "",
    "text": "1.1 Objetivos de la sesión",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "index.html#docente",
    "href": "index.html#docente",
    "title": "Introducción al Machine Learning aplicado a las Ciencias Sociales",
    "section": "Docente",
    "text": "Docente\nMi nombre es Christian Chiroque, soy politólogo especializado en análisis de datos, machine learning y su aplicación en ciencias sociales y políticas públicas.\nActualmente trabajo como Analista en la Unidad de Inteligencia Financiera del Perú, unidad especializada de la Superintendencia de Banca, Seguros y AFP (SBS).\nMe apasiona enseñar, el basket y el Starcraft.",
    "crumbs": [
      "Notas preliminares"
    ]
  },
  {
    "objectID": "C1_intro.html#ficha-resumen",
    "href": "C1_intro.html#ficha-resumen",
    "title": "1  Introducción",
    "section": "",
    "text": "Knuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "C1_intro.html#objetivos-de-la-sesión",
    "href": "C1_intro.html#objetivos-de-la-sesión",
    "title": "1  Introducción al R",
    "section": "",
    "text": "Nota\n\n\n\nEl propósito de esta primera sesión es brindar un primer acercamiento al lenguaje R y a la plataforma RStudio, explorando las nociones básicas de la programación.\nLa idea es que los estudiantes se familiaricen con la lógica fundamental que subyace en el desarrollo de código, lo cual servirá como base para avanzar en el análisis y entendimiento de los datos, y posteriormente en la aplicación de técnicas de machine learning",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "C1_intro.html#elementos-básicos",
    "href": "C1_intro.html#elementos-básicos",
    "title": "1  Introducción al R",
    "section": "1.3 Elementos básicos",
    "text": "1.3 Elementos básicos\n\n1.3.1 Objetos\nVamos a examinar la clase de algunos de los elementos más básicos en R.\nUn objeto puede ser un número. En este caso el objeto es de tipo numeric.\n\n5\n\n[1] 5\n\n\nO también podría ser un nombre de un país. En este caso el objeto es de tipo character. Vas a notar que se trata de un caractér porque vas a visualizar el resultado entre comillas.\n\n\"Perú\"\n\n[1] \"Perú\"\n\n\nLos objetos también pueden almacenarse en la memoria del programa con ciertos “nombres”. Por ejemplo:\n\nyear&lt;-2024\nyear\n\n[1] 2024\n\n\n\ncountry&lt;-\"Perú\"\ncountry\n\n[1] \"Perú\"\n\n\nUno puede asignar un nombre a un objeto en el R con la flecha de asignación (&lt;-)\n\n\n\n\n\n\nNota\n\n\n\nHay otro tipo de objetos conocidos como factores que los estudiaremos líneas más abajo!.\n\n\n\n\n1.3.2 Vectores\n\nUn vector es una colección de uno o más datos del mismo tipo.\nTipo. Un vector tiene el mismo tipo que los datos que contiene. Si tenemos un vector que contiene datos de tipo numérico, el vector será también de tipo numérico.\n\nEjemplo: Vamos a crear tres vectores: uno numérico, uno de caracter.\n\nvector_numerico &lt;- c(1, 2, 3, 4, 5)\nvector_numerico\n\n[1] 1 2 3 4 5\n\n\n\nvector_caracter &lt;- c(\"arbol\", \"casa\", \"persona\")\nvector_caracter\n\n[1] \"arbol\"   \"casa\"    \"persona\"\n\n\n\n\n1.3.3 Funciones\nUna función es como una máquina a la que le das un insumo, o input para que realice un procedimiento específico. Luego de realizar el procedimiento, la máquina te da un resultado que le vamos a llamar output.\nPor ejemplo, podemos utilizar la función sqrt() para obtener la raíz cuadrada de un número. En este caso aplicamos una función sobre un sólo número.\n\nsqrt(16)\n\n[1] 4\n\n\nPero también podemos aplicar una función sobre un vector. Por ejemplo, podemos solicitar la función sum() para obtener la suma de todos los elementos de un vector numérico:\n\nsum(vector_numerico)\n\n[1] 15\n\n\nTambién podemos utilizar la función class() para corroborar que la clase del vector que tenemos.\n\nclass(vector_numerico)\n\n[1] \"numeric\"\n\nclass(vector_caracter)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nSiempre te vas a dar cuenta que estás frente a una función porque usualmente está seguida de paréntesis en el cual se colocan los argumentos.\n\n\n\n\n1.3.4 Dataframes\nLos data frames son estructuras de datos de dos dimensiones (rectangulares) que pueden contener vectores de diferentes tipos.\nEs la estructura más usada para ciencia de datos y la que vamos a ver de forma más recurrente en el curso.\nLo más importante que debes recordar es que las filas en un dataframe representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables.\nPor ejemplo, tenemos la siguiente información sobre ciertos departamentos del Perú y sus niveles de pobreza:\n\ndepartamentos&lt;-c(\"Huancavelica\", \"Ayacucho\", \"Pasco\")\npobreza&lt;-c(47.7, 46.4, 44.8)\nmi_df&lt;-data.frame(departamentos, pobreza)\nmi_df\n\n  departamentos pobreza\n1  Huancavelica    47.7\n2      Ayacucho    46.4\n3         Pasco    44.8\n\n\nUna forma de examinar rápidamente un dataframe es utilizando la función str():\n\nstr(mi_df)\n\n'data.frame':   3 obs. of  2 variables:\n $ departamentos: chr  \"Huancavelica\" \"Ayacucho\" \"Pasco\"\n $ pobreza      : num  47.7 46.4 44.8\n\n\nEl output de esta función te indica las dimensiones del dataframe (número de observaciones y número de variables), así como los nombres de las variables, el tipo y algunos valores de muestra.\nOtra función básica para explorar es names(), la cual te arroja exclusivamente los nombres de las variables del dataframe:\n\nnames(mi_df)\n\n[1] \"departamentos\" \"pobreza\"      \n\n\n\n\n\n\n\n\nImportante\n\n\n\nUn error frecuente es no identificar correctamente las unidades de análisis con las que estamos trabajando. Al abrir un conjunto de datos, lo primero que debes preguntarte es: ¿A qué se refiere esta información? ¿A personas, países, instituciones?\n\n\n\n\n1.3.5 Índices\n\nUsar índices para obtener subconjuntos es el procedimiento más universal en R, pues funciona para todas las estructuras de datos.\nUn índice en R representa una posición.\nCuando usamos índices le pedimos a R que extraiga de una estructura los datos que se encuentran en una o varias posiciones específicas dentro de ella.\n\nEjemplos:\n\nSeleccionar la columna 2:\n\n\nmi_df [,2]\n\n[1] 47.7 46.4 44.8\n\n\nPara seleccionar una columna, también podemos usar el símbolo de $.\n\nmi_df$pobreza\n\n[1] 47.7 46.4 44.8\n\n\nNormalmente lo usamos cuando queremos aplicar una función a sólo una columna. Como por ejemplo:\n\nmean(mi_df$pobreza)\n\n[1] 46.3\n\n\n\nSeleccionar sólo el caso (fila) 2:\n\n\nmi_df [2,]\n\n  departamentos pobreza\n2      Ayacucho    46.4\n\n\n\nSeleccionar el elemento que se encuentra en la fila 2 y la columna 2:\n\n\nmi_df [2,2]\n\n[1] 46.4\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecuerda que en los [,] primero se mencionan las filas y luego las columnas.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "C1_intro.html#procedimientos-básicos",
    "href": "C1_intro.html#procedimientos-básicos",
    "title": "1  Introducción al R",
    "section": "1.4 Procedimientos básicos",
    "text": "1.4 Procedimientos básicos\nHasta aquí hemos aprendido los elementos básicos del R, ahora procederemos a analizar los procedimientos más cotidianos que realizaremos en un proceso de análisis de datos.\n\n1.4.1 Apertura de paquetes\nLíneas arriba te había comentado que existían funciones que podías aplicar sobre objetos en el R. Dabas un input y la función te arrojaba un determinado resultado.\nAhora bien, lo más interesante del R es que existen diferentes “conjuntos de funciones” para tareas específicas y que uno puede instalar y utilizar en tu proceso de análisis.\nPara instalar un paquete necesitas escribir install.packages(\"nombre_del_paquete\"). Luego de instalarlo, para comenzar a utilizarlo debes abrirlo con el siguiente comando library(nombre_del_paquete).\nHagamos la prueba con el paquete rio, el cual es un paquete creado para importar/exportar archivos de diversos tipos.\nPrimero lo vamos a instalar. No te olvides que cuando instalas un paquete el nombre del mismo va entre comillas:\n\n#install.packages(\"rio\")\n\n\n\n\n\n\n\nTip\n\n\n\nRecuerda que la instalación de paquetes se realiza sólo una vez. Esto quiere decir que si instalas hoy el paquete “rio” ya no será necesario que realices esta operación nuevamente.\n\n\nLuego de instalarlo lo debemos abrir para utilizar las funciones que están dentro de él.\n\nlibrary(rio)\n\n\n\n1.4.2 Apertura de archivos\nLo más común es que se te va a entregar un archivo para que lo puedas abrir en el R.\nPara ello, una primera forma sencilla de abrir un archivo es haciendo uso de la función import del paquete rio:\n\ndata&lt;-import(\"data/regiones.xlsx\") \n#Dentro del () colocas la ubicación del archivo.\n\nUna vez que abrimos una data y corroboramos que está en nuestro Environment podemos explorarla.\nViendo un encabezado de las primeras filas:\n\nhead(data)\n\n     region macroregion poblacion pobreza nivel_pobreza  agua desague\n1  Amazonas     Oriente    379384    47.3             3 51.84   36.69\n2    Ancash       Norte   1083519    23.5             2 71.56   56.38\n3  Apurímac         Sur    405759    42.8             3 56.33   36.12\n4  Arequipa         Sur   1382730     9.1             1 72.47   65.85\n5  Ayacucho         Sur    616176    51.9             3 66.99   45.35\n6 Cajamarca       Norte   1341012    52.9             3 52.89   32.48\n  electrificacion acceso_internet telefonia_movil pc_tablet hospitales\n1           73.67            4.45           69.39     11.02          8\n2           85.20           18.33           79.60     25.00         23\n3           80.43            8.93           71.21     14.74          8\n4           89.98           32.88           91.28     40.52         24\n5           80.94           10.42           77.65     17.84         11\n6           80.68            9.29           74.66     14.10         25\n\n\nAnalizando su estructura:\n\nstr(data)\n\n'data.frame':   24 obs. of  12 variables:\n $ region         : chr  \"Amazonas\" \"Ancash\" \"Apurímac\" \"Arequipa\" ...\n $ macroregion    : chr  \"Oriente\" \"Norte\" \"Sur\" \"Sur\" ...\n $ poblacion      : num  379384 1083519 405759 1382730 616176 ...\n $ pobreza        : num  47.3 23.5 42.8 9.1 51.9 52.9 18.8 46.6 40.1 4.7 ...\n $ nivel_pobreza  : num  3 2 3 1 3 3 2 3 3 1 ...\n $ agua           : num  51.8 71.6 56.3 72.5 67 ...\n $ desague        : num  36.7 56.4 36.1 65.8 45.4 ...\n $ electrificacion: num  73.7 85.2 80.4 90 80.9 ...\n $ acceso_internet: num  4.45 18.33 8.93 32.88 10.42 ...\n $ telefonia_movil: num  69.4 79.6 71.2 91.3 77.7 ...\n $ pc_tablet      : num  11 25 14.7 40.5 17.8 ...\n $ hospitales     : num  8 23 8 24 11 25 20 5 9 25 ...\n\n\n\nnames(data)\n\n [1] \"region\"          \"macroregion\"     \"poblacion\"       \"pobreza\"        \n [5] \"nivel_pobreza\"   \"agua\"            \"desague\"         \"electrificacion\"\n [9] \"acceso_internet\" \"telefonia_movil\" \"pc_tablet\"       \"hospitales\"     \n\n\n\n\n1.4.3 Identificación teórica de la variable\n\nAntes de seguir en el análisis debemos corroborar los tipos de variables con los que estamos trabajando a nivel teórico.\nEn una data real, esto normalmente lo encontramos en el Cuestionario o Diccionario de Variables. Según la teoría estadistica podemos tener dos grandes opciones.\n\n1.4.3.1 Numéricas\nLas variables numéricas son aquellas que representan cantidades medidas o contadas, y pueden ser de tipo entero o decimal. Permiten realizar operaciones matemáticas y son fundamentales en el análisis estadístico y cuantitativo.\nSe clasifican en continuas y discretas, basándose en los valores que pueden tomar.\nLas variables discretas representan información que se puede contar en unidades enteras, como el número de hospitales en nuestra base de datos.\nPor otro lado, las variables continuas pueden tomar cualquier valor dentro de un rango, incluyendo decimales. En nuestra base de datos contamos con variables como * como la altura o el peso pobreza, agua, entre otros. Esto significa que pueden medir con precisión infinita dentro de su escala, adaptándose a una variedad más amplia de datos y mediciones.\n\n\n1.4.3.2 Categóricas\nUna variable categórica clasifica las observaciones en grupos o categorías que no tienen un orden matemático inherente. Se dividen en nominales y ordinales.\nLas variables nominales representan categorías sin un orden específico entre ellas, como colores, nombres de países o géneros. En nuestra data una variable nominal sería macroregion.\nEn cambio, las variables ordinales sí poseen un orden o jerarquía entre las categorías, aunque la distancia entre estas no es necesariamente uniforme; por ejemplo, niveles de educación o calificaciones de satisfacción. Continuando con el ejemplo, la variable ordinal nivel_pobreza clasifica en categorías donde el 1 corresponde a “Bajo”, el 2 a “Medio” y el 3 a “Alto”.\n\n\n\n1.4.4 Configuración de las variable en R\nAhora veamos qué tenemos en nuestra data.\nVeamos las siguientes tres variables: poblacion (numérica), macroregión (nominal) y nivel de pobreza (ordinal).\nDichas variables qué tipo de objeto son actualmente en el R?\n\n1.4.4.1 Numeric\n\nclass(data$poblacion)\n\n[1] \"numeric\"\n\n\nPara el caso de población cuenta con la configuración adecuada pues es numeric.\nTen en cuenta que para el caso de una variable numérica discreta como hospitales la configuración adecuada también es numeric.\n\n\n1.4.4.2 Factors\nPara el caso de las variables categóricas, para poder trabajar con estas en el R debemos convertirlas a un tipo especial de objeto denominado factor.\nBásicamente, un factor es una variable que tiene grupos, los cuales pueden estar ordenados o no ordenados.\nFACTORES NO ORDENADOS\nPara el caso de la variable nominal macroregión que inicialmente está mal configurada (pues tiene el tipo character).\n\nclass(data$macroregion)\n\n[1] \"character\"\n\n\nvamos a convertirla en un factor no ordenado.\n\ndata$macroregion&lt;-factor(data$macroregion)\n\n\n\n\n\n\n\nTip\n\n\n\nHemos empleado la función factor() y el operador de asignación porque estamos modificando una parte de nuestro conjunto de datos. En otras palabras, estamos actualizando la variable macroregión con su configuración correcta.\n\n\nPodemos corroborar el tipo final pidiendo otra vez la función str():\n\nstr(data$macroregion)\n\n Factor w/ 4 levels \"Centro\",\"Norte\",..: 3 2 4 4 4 2 4 1 1 1 ...\n\n\nEn este caso nos menciona que ahora la variable macroregion es un factor con cuatro niveles (Centro, Norte, Sur, Oriente).\n\n\n\n\n\n\nImportante\n\n\n\nSi bien aquí vemos la palabra “niveles” esto no quiere decir que para R esos niveles tengan un orden, sino más bien que son categorías diferentes.\n\n\nFACTORES ORDENADOS\nAhora bien, el caso del nivel de pobreza es diferente, ya que, aunque también es un factor, sus niveles presentan un orden de magnitud específico.\nEn este caso, además de convertirla en factor, es necesario especificar el orden de los niveles, indicando que efectivamente se trata de una secuencia ordenada.\n\ndata$nivel_pobreza&lt;-factor(data$nivel_pobreza,\n                          levels = c(1,2,3),\n                          ordered = TRUE)\n\nMediante la función str(), confirmamos que nuestra variable nivel_pobreza se ha convertido efectivamente en un factor ordenado con tres niveles, donde 1 es menor que 2 y, a su vez, 2 es menor que 3.\n\nstr(data$nivel_pobreza)\n\n Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 2 3 1 3 3 2 3 3 1 ...\n\n\n\n\n\n\n\n\nImportante\n\n\n\nAunque para nosotros los niveles parecen ser números (1, 2 o 3), para R no lo son. Esto significa que no es posible realizar operaciones matemáticas con ellos.\n\n\nAhora con esta configuración ya estamos listos para el siguiente paso: Manipular y preparar la data.\n\nstr(data)\n\n'data.frame':   24 obs. of  12 variables:\n $ region         : chr  \"Amazonas\" \"Ancash\" \"Apurímac\" \"Arequipa\" ...\n $ macroregion    : Factor w/ 4 levels \"Centro\",\"Norte\",..: 3 2 4 4 4 2 4 1 1 1 ...\n $ poblacion      : num  379384 1083519 405759 1382730 616176 ...\n $ pobreza        : num  47.3 23.5 42.8 9.1 51.9 52.9 18.8 46.6 40.1 4.7 ...\n $ nivel_pobreza  : Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 2 3 1 3 3 2 3 3 1 ...\n $ agua           : num  51.8 71.6 56.3 72.5 67 ...\n $ desague        : num  36.7 56.4 36.1 65.8 45.4 ...\n $ electrificacion: num  73.7 85.2 80.4 90 80.9 ...\n $ acceso_internet: num  4.45 18.33 8.93 32.88 10.42 ...\n $ telefonia_movil: num  69.4 79.6 71.2 91.3 77.7 ...\n $ pc_tablet      : num  11 25 14.7 40.5 17.8 ...\n $ hospitales     : num  8 23 8 24 11 25 20 5 9 25 ...",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "C1_intro.html#ficha-resumen-cheat-sheet",
    "href": "C1_intro.html#ficha-resumen-cheat-sheet",
    "title": "1  Introducción al R",
    "section": "1.5 Ficha resumen (Cheat Sheet)",
    "text": "1.5 Ficha resumen (Cheat Sheet)\n¡Esta ficha resume muy bien lo que deberías saber para proseguir con los siguientes pasos!",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "C1_intro.html#sobre-r",
    "href": "C1_intro.html#sobre-r",
    "title": "1  Introducción al R",
    "section": "1.2 Sobre R",
    "text": "1.2 Sobre R\n\n1.2.1 Qué es?\nR es un lenguaje de programación y un entorno de software libre orientado principalmente al análisis estadístico y la representación gráfica de datos.\nR es un lenguaje de programación que adopta el paradigma de la “programación orientada a objetos”. Esto significa que, en R, todo se considera un “objeto”, ya sea un número, una base de datos o un modelo estadístico.\nCada objeto tiene atributos y comportamientos asociados que determinan cómo se puede interactuar con él.\nImagina que cada objeto en R es como un coche. Los “atributos” de ese coche pueden incluir su color, marca, modelo, y año de fabricación. Estos atributos describen las características específicas del coche. Ahora, piensa en los “comportamientos asociados” como las acciones que puedes realizar con ese coche: encenderlo, acelerar, frenar o encender las luces. Del mismo modo, en R, un objeto, como un conjunto de datos, podría tener atributos que describan su tamaño, tipo y estructura. Y los comportamientos asociados de ese conjunto de datos podrían incluir operaciones como filtrar, ordenar o aplicar una función estadística.\n\n\n1.2.2 Dónde escribir mi código\n\nExisten varias formas de escribir código en el R. Para ello tenemos algunas opciones simples, como el Script y otras un poco más elaboradas como Quarto.\nPara fines de este curso se va a utilizar Quarto, la cual es una herramienta de publicación científica y técnica que permite crear documentos, presentaciones, páginas web y libros a partir de código y texto.\nEs la evolución de R Markdown, pero más general, ya que no depende solo de R: funciona también con Python, Julia, entre otros lenguajes de programación.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "index.html#instalación-de-r",
    "href": "index.html#instalación-de-r",
    "title": "Introducción al Machine Learning aplicado a las Ciencias Sociales",
    "section": "Instalación de R",
    "text": "Instalación de R\n\nR es un entorno y lenguaje de programación con un enfoque al análisis estadístico. Primero debemos instalar el R (el software estadístico), el cual lo podemos descargar en el siguiente enlace:\nHaz click aquí para descargar R\nLa versión más reciente, al 13 de junio de 2025, es la 4.5.1 (“Great Square Root”).\nRecuerda que R es un software libre por eso otorga a los usuarios la libertad de usar, estudiar, modificar y distribuir el código fuente. Estas libertades permiten colaboración y mejora continua del software por parte de la comunidad.",
    "crumbs": [
      "Notas preliminares"
    ]
  },
  {
    "objectID": "index.html#instalación-de-r-studio",
    "href": "index.html#instalación-de-r-studio",
    "title": "Introducción al Machine Learning aplicado a las Ciencias Sociales",
    "section": "Instalación de R Studio",
    "text": "Instalación de R Studio\n\nRStudio es un entorno de desarrollo integrado (IDE) para el lenguaje de programación R. Facilita el trabajo con R al proporcionar una interfaz gráfica amigable que incluye un editor de código avanzado, herramientas de depuración, visualización de datos y gestión de paquetes, lo que optimiza y simplifica el proceso de análisis de datos y programación estadística.\nLo puedes descargar desde el siguiente link:\nHaz click aquí para descargar R Studio\n\n\n\n\n\n\nAdvertencia\n\n\n\n¡Recuerda que primero debes instalar R y luego R Studio!\n\n\nTe sugiero ver el siguiente video para que puedas instalar los programas en tu computadora:",
    "crumbs": [
      "Notas preliminares"
    ]
  },
  {
    "objectID": "index.html#bienvenida.",
    "href": "index.html#bienvenida.",
    "title": "Introducción al Machine Learning aplicado a las Ciencias Sociales",
    "section": "Bienvenida.",
    "text": "Bienvenida.\n¡Hola y bienvenid@!\nEs un gusto darte la bienvenida al curso Introducción al Machine Learning aplicado a las Ciencias Sociales de la Pontificia Universidad Católica del Perú.\nEn este espacio encontrarás todo el material del curso: presentaciones, código en R, textos explicativos y recursos audiovisuales complementarios que te ayudarán a reforzar los conceptos vistos en clase.",
    "crumbs": [
      "Notas preliminares"
    ]
  },
  {
    "objectID": "C1_intro.html#reflexión-final-alphago",
    "href": "C1_intro.html#reflexión-final-alphago",
    "title": "1  Introducción al R",
    "section": "1.6 Reflexión final: AlphaGo",
    "text": "1.6 Reflexión final: AlphaGo\nComo complemento, les sugiero ver el documental AlphaGo (2017). Allí podrán observar cómo la inteligencia artificial pasó de la teoría a un caso concreto, utilizando deep learning para resolver un desafío considerado inalcanzable para las máquinas.\nMira el video aquí\nMás allá del resultado, muestra también cómo la aplicación de estos algoritmos plantea preguntas sobre la creatividad, la intuición y el futuro de la relación entre humanos y tecnología.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al R</span>"
    ]
  },
  {
    "objectID": "C2_EDA1.html",
    "href": "C2_EDA1.html",
    "title": "2  Primeros pasos con tidyverse",
    "section": "",
    "text": "2.1 Presentación",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Primeros pasos con tidyverse</span>"
    ]
  },
  {
    "objectID": "C2_EDA1.html#presentación-1",
    "href": "C2_EDA1.html#presentación-1",
    "title": "2  Análisis Exploratorio de datos para ML I",
    "section": "2.2 Presentación",
    "text": "2.2 Presentación",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de datos para ML I</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html",
    "href": "C3_EDA2.html",
    "title": "3  Exploración avanzada",
    "section": "",
    "text": "3.1 Visualización de datos\nLa visualización de datos es un paso clave dentro del Análisis Exploratorio de Datos (EDA) en Machine Learning, porque nos permite ir más allá de los números y comprender de manera intuitiva la información con la que vamos a trabajar. A través de gráficos podemos detectar patrones, identificar tendencias, encontrar valores atípicos y comparar distribuciones de las variables, lo que facilita tomar decisiones sobre el preprocesamiento y la selección de características.\nEn este curso utilizaremos ggplot2, un paquete del ecosistema tidyverse que se ha convertido en un estándar para la visualización en R. Su fortaleza radica en la gramática de los gráficos, un enfoque que nos invita a construir visualizaciones combinando capas (datos, variables estéticas, geometrías, temas) de manera flexible y reproducible.\nPara Machine Learning, esta herramienta es fundamental: nos ayuda a explorar relaciones entre variables predictoras y la variable objetivo, a observar cómo se distribuyen los datos y a detectar posibles problemas que, de no atenderse, afectarían el desempeño de nuestros modelos.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C4_RL.html",
    "href": "C4_RL.html",
    "title": "4  Regresión Lineal",
    "section": "",
    "text": "4.1 Métodos Supervisados\nUn método supervisado es un tipo de técnica en machine learning en la que el modelo aprende a partir de ejemplos en los que ya conocemos la respuesta correcta. Es decir, trabajamos con un conjunto de datos que incluye tanto las variables de entrada (lo que usamos para predecir) como una variable de salida o etiqueta (lo que queremos predecir).\nEl aprendizaje se llama “supervisado” porque el modelo tiene un “supervisor”: los datos con la respuesta ya conocida. Así, el modelo ajusta sus parámetros comparando sus predicciones con las respuestas reales y corrigiendo sus errores. Una vez entrenado, podemos darle nuevos datos (sin respuesta) y el modelo intentará predecir el valor de la salida.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#ggplot2",
    "href": "C3_EDA2.html#ggplot2",
    "title": "3  Visualización de datos",
    "section": "",
    "text": "3.1.1 Retomamos nuestro dataset\n\nlibrary(tidyverse)\nlibrary(readxl)\ndata &lt;- read_xlsx(\"data/AML_2.xlsx\")\n\n\n\n3.1.2 Anatomía de un ggplot\n\nggplot2 es un popular paquete de visualización de datos para el lenguaje de programación R, basado en los principios de la “Gramática de Gráficos”. Esta filosofía de diseño permite a los usuarios construir gráficos complejos y estéticamente agradables a partir de componentes básicos de forma intuitiva y flexible.\nEl núcleo de ggplot2 radica en su sistema de capas, donde cada gráfico se construye agregando capas que pueden incluir, entre otros, los datos, las estéticas (como color, forma y tamaño), los objetos geométricos (como puntos, líneas y barras), las escalas, y las anotaciones. Este enfoque modular no solo facilita la personalización y optimización de los gráficos sino que también promueve una estructura de código clara y comprensible.\nVamos a hacer un ejemplo paso a paso:\n\n3.1.2.1 1ra capa: Datos\nEs el conjunto de datos a visualizar.\nNuestra primera capa siempre va a ser la data. Sobre esta iniciamos la función ggplot y corroboramos que tenemos un lienzo en blanco.\n\ndata |&gt; \n  ggplot()\n\n\n\n\n\n\n\n\n\n\n3.1.2.2 2da capa: Estéticas\nEs el diseño básico del gráfico (Aesthetics).\nMapeo de variables a propiedades visuales como color, forma o tamaño, definidas con aes().\nA diferencia del lienzo en blanco, ya contamos con un diseño. En este caso, hemos indicado al R que el eje X será la variable Pobreza.\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn ggplot2, las capas de un gráfico se van adicionando secuencialmente utilizando el operador +.\n\n\n\n\n3.1.2.3 3ra capa: Geometrías (Geoms)\nSon representaciones gráficas de los datos, como puntos, líneas o barras (geom_point(), geom_line(), geom_bar(), etc.).\nEn nuestro ejemplo, podemos agregar la geometría de puntos para hacer un scatterplot o diagrama de dispersión:\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn el paquete {ggplot2} existen 30 geometrías disponibles. Puedes ver el detalle de estos en la documentación del paquete.\n\n\nEsta estructura de capas hace que ggplot2 sea extremadamente útil para explorar y presentar datos de manera efectiva, permitiendo a los usuarios desde principiantes hasta expertos crear visualizaciones de datos complejas y personalizadas con relativa facilidad.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos</span>"
    ]
  },
  {
    "objectID": "C2_EDA1.html#notas-de-la-clase-manipulación-de-datos",
    "href": "C2_EDA1.html#notas-de-la-clase-manipulación-de-datos",
    "title": "2  Análisis Exploratorio de datos para ML I",
    "section": "2.2 Notas de la clase: Manipulación de datos",
    "text": "2.2 Notas de la clase: Manipulación de datos\nLa primera etapa del Análisis Exploratorio de Datos (EDA) en Machine Learning se centra en los pasos básicos con tidyverse, ya que este ecosistema de herramientas facilita la importación, organización y transformación de la información. En este punto, resulta esencial familiarizarnos con la estructura de los datos, reconocer las variables disponibles y aplicar operaciones que nos permitan manipular y preparar la base de forma eficiente. Este proceso no solo brinda una comprensión profunda de la data, sino que también sienta los cimientos para un análisis riguroso y para el éxito de las fases posteriores de modelamiento.\nTe sugiero ver este video introductorio sobre el Tidyverse:\n\nAbrimos la librería tidyverse:\n\nlibrary(tidyverse)\n\ndplyr es un paquete del Tidyverse que sirve para manipular tablas y transformarlas. Tiene una amplia gama de verbos con los cuales podemos realizar las tareas más recurrentes de la manipulación de datos.\n\n\n2.2.1 Importación de archivos\nPodríamos abrir este archivo con la función read_xlsx():\n\nlibrary(readxl)\ndata&lt;-read_xlsx(\"data/CPI.xlsx\")\n\nVisualizamos la data y solicitamos que nos diga la clase:\n\ndata\n\n# A tibble: 1,086 × 5\n   country               year iso3  region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AFG   AP            24\n 2 Albania               2022 ALB   ECA           36\n 3 United Arab Emirates  2022 ARE   MENA          67\n 4 Angola                2022 AGO   SSA           33\n 5 Argentina             2022 ARG   AME           38\n 6 Armenia               2022 ARM   ECA           46\n 7 Australia             2022 AUS   AP            75\n 8 Austria               2022 AUT   WE/EU         71\n 9 Azerbaijan            2022 AZE   ECA           23\n10 Bahamas               2022 BHS   AME           64\n# ℹ 1,076 more rows\n\nclass(data)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n\nNota\n\n\n\nUn tibble es una versión moderna del dataframe en R, parte del tidyverse, diseñado para facilitar el trabajo con datos tabulares.\n\n\nExploración del dataset y configuración de variables\nVemos la estructura rápidamente:\n\nstr(data)\n\ntibble [1,086 × 5] (S3: tbl_df/tbl/data.frame)\n $ country  : chr [1:1086] \"Afghanistan\" \"Albania\" \"United Arab Emirates\" \"Angola\" ...\n $ year     : num [1:1086] 2022 2022 2022 2022 2022 ...\n $ iso3     : chr [1:1086] \"AFG\" \"ALB\" \"ARE\" \"AGO\" ...\n $ region   : chr [1:1086] \"AP\" \"ECA\" \"MENA\" \"SSA\" ...\n $ cpi_score: num [1:1086] 24 36 67 33 38 46 75 71 23 64 ...\n\n\nAl ejecutar names() sobre un conjunto de datos, se nos devuelve un vector con los nombres de todas las columnas en el orden en que aparecen.\n\nnames(data)\n\n[1] \"country\"   \"year\"      \"iso3\"      \"region\"    \"cpi_score\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nAntes de proseguir con el análisis exploratorio de datos, es fundamental que comprendas claramente qué representan las filas y las columnas en tus datos.\n\n\nCorroboramos que el score del AML esté adecuadamente configurado.\n\nclass(data$cpi_score)\n\n[1] \"numeric\"\n\n\nDe acuerdo, podemos proseguir.\nExaminemos la base original. Vamos a editar la tabla con diversos verbos de dplyr.\n\n\n2.2.2 Select()\nLa función select() es utilizada para seleccionar o excluir columnas de un data frame o tibble en R. Va más allá de simplemente escoger columnas por nombre, ya que permite una amplia gama de criterios y operaciones.\nFuncionamiento básico:\n\nEntrada: Un data frame o tibble y un conjunto de nombres de columnas o criterios para seleccionar columnas.\nSalida: Un objeto de la misma clase que el de entrada (data frame o tibble) que contiene solo las columnas seleccionadas.\n\nVamos a seleccionar sólo ciertas columnas:\n\ndata1&lt;-data |&gt; \n  select(country, year, region, cpi_score)\ndata1\n\n# A tibble: 1,086 × 4\n   country               year region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AP            24\n 2 Albania               2022 ECA           36\n 3 United Arab Emirates  2022 MENA          67\n 4 Angola                2022 SSA           33\n 5 Argentina             2022 AME           38\n 6 Armenia               2022 ECA           46\n 7 Australia             2022 AP            75\n 8 Austria               2022 WE/EU         71\n 9 Azerbaijan            2022 ECA           23\n10 Bahamas               2022 AME           64\n# ℹ 1,076 more rows\n\n\nTener en cuenta que puedes:\n\nSeleccionar por nombre\nSeleccionar por el número de la columna\n(Des)seleccionar colocando un “-” antes del nombre/número de columna.\nSeleccionar un rango colocando por ejemplo 2:4 lo que significa “desde la columna 2 hasta la columna 4).\nPuedes combinar todas las anteriores y tener más de un criterio a la vez separándolo por coma.\n\n\n\n2.2.3 Filter()\nLa función filter() se utiliza para filtrar filas de un data frame o tibble en R en función de condiciones específicas, permitiendo crear un subconjunto de datos.\nAl crear subconjuntos nuestros datos de forma precisa, podemos focalizar nuestro análisis, mejorar la eficiencia computacional y obtener resultados más claros y relevantes.\nCaracterísticas principales:\n\nCondiciones múltiples: Puedes usar múltiples condiciones para filtrar tus datos. Estas se combinan utilizando operadores lógicos como & (y), | (o) y ! (no).\nUso de operadores de comparación: Los operadores estándar como ==, &gt;, &lt;, &gt;=, &lt;=, y != se utilizan para establecer condiciones.\nFunciones auxiliares: dplyr proporciona funciones como between(), que pueden ser útiles para establecer condiciones. Por ejemplo, between(x, 1, 10) es equivalente a x &gt;= 1 & x &lt;= 10.\n\nEn este caso vamos a seleccionar aquellos países cuya medición es del año 2022.\n\ndata2&lt;-data1 %&gt;%                   \n  filter(year==2022)\ndata2\n\n# A tibble: 181 × 4\n   country               year region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AP            24\n 2 Albania               2022 ECA           36\n 3 United Arab Emirates  2022 MENA          67\n 4 Angola                2022 SSA           33\n 5 Argentina             2022 AME           38\n 6 Armenia               2022 ECA           46\n 7 Australia             2022 AP            75\n 8 Austria               2022 WE/EU         71\n 9 Azerbaijan            2022 ECA           23\n10 Bahamas               2022 AME           64\n# ℹ 171 more rows\n\n\n\n\n2.2.4 Arrange()\nSe utiliza para ordenar (o reordenar) un data frame o tibble según una o más columnas.\nFuncionamiento básico:\n\nOrdenación simple: Si proporcionas una columna a arrange(), ordenará el data frame en función de esa columna en orden ascendente por defecto.\nOrdenación descendente: Si deseas ordenar en dirección descendente, puedes usar la función desc(). Por ejemplo: df |&gt; arrange(desc(edad)) ordenará el data frame por la columna “edad” en orden descendente.\nOrdenación múltiple: Puedes proporcionar múltiples columnas para ordenar, y arrange() las usará en el orden proporcionado para determinar el ordenamiento. Por ejemplo, si deseas ordenar primero por “grupo” y luego por “edad” dentro de cada grupo, usarías: df |&gt; arrange(grupo, edad).\n\n\ndata3&lt;-data2 |&gt;    \n  arrange(desc(cpi_score))\ndata3\n\n# A tibble: 181 × 4\n   country      year region cpi_score\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU         90\n 2 Finland      2022 WE/EU         87\n 3 New Zealand  2022 AP            87\n 4 Norway       2022 WE/EU         84\n 5 Singapore    2022 AP            83\n 6 Sweden       2022 WE/EU         83\n 7 Switzerland  2022 WE/EU         82\n 8 Netherlands  2022 WE/EU         80\n 9 Germany      2022 WE/EU         79\n10 Ireland      2022 WE/EU         77\n# ℹ 171 more rows\n\n\n\n\n2.2.5 Mutate()\nLa función mutate() está diseñada para crear o modificar columnas dentro de un data frame o tibble en R. Mientras que el data frame original se mantiene inalterado, mutate() devuelve una copia con las columnas especificadas añadidas o alteradas.\nEn este caso vamos a crear una variable cambiando la escala del score del CPI.\nEn la medida original 0 representaba alta corrupción y 100 escasa corrupción. Ahora, si realizamos la operación “100 - cpi_score”, los valores cercanos a 0 tendrán poca corrupción y los cercanos a 100 alta corrupción, siendo más intuitivo.\nEsta transformación puede ser útil para ajustar la interpretación de los datos a contextos donde es más intuitivo trabajar con escalas donde un número mayor indica mayor intensidad de un fenómeno (Corrupción, en este caso), dependiendo del análisis que se desea realizar.\n\ndata4&lt;-data3 |&gt;   \n  mutate(cpi_score2=100-cpi_score) \ndata4\n\n# A tibble: 181 × 5\n   country      year region cpi_score cpi_score2\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU         90         10\n 2 Finland      2022 WE/EU         87         13\n 3 New Zealand  2022 AP            87         13\n 4 Norway       2022 WE/EU         84         16\n 5 Singapore    2022 AP            83         17\n 6 Sweden       2022 WE/EU         83         17\n 7 Switzerland  2022 WE/EU         82         18\n 8 Netherlands  2022 WE/EU         80         20\n 9 Germany      2022 WE/EU         79         21\n10 Ireland      2022 WE/EU         77         23\n# ℹ 171 more rows\n\n\n\n\n2.2.6 Summarise()\nSe utiliza para crear resúmenes estadísticos de un data frame o tibble.\nDentro de los resúmenes puedes disponer de por ejemplo:\nMedidas de tendencia central: Estas funciones describen un valor central o típico dentro de un conjunto de datos.\n\nMedia: mean(x)\nMediana: median(x)\n\nCómo calcularíamos la media de forma directa (tradicional)?\n\nsummary(data4$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   45.00   60.50   57.02   70.25   88.00       1 \n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n¿Qué significa NA´s? ¿Por qué se produce esto en una data como la de Corruption Perception Index?\n\n\nCómo lo calculamos con tidyverse?\n\ndata4 |&gt;   \n  summarise(mean(cpi_score2, na.rm = T))    \n\n# A tibble: 1 × 1\n  `mean(cpi_score2, na.rm = T)`\n                          &lt;dbl&gt;\n1                          57.0\n\n\n\n\n\n\n\n\nNota\n\n\n\nCuando aplicamos un summarise lo que nos devuelve es un valor o conjunto de valores. Por otro lado, el argumento na.rm = TRUE se utiliza para especificar que los valores perdidos (NA) deben ser ignorados en el cálculo. De esta forma, le indicamos a la función que proceda con la operación excluyendo dichos valores ausentes.\n\n\n\n\n2.2.7 Utilizando pipe: |&gt;\nProbablemente hayas observado que hemos creado un conjunto de datos para cada verbo utilizado, lo cual en situaciones reales resultaría excesivamente repetitivo. Haciendo una analogía con la escritura de un libro, sería como si estuviéramos limitados a usar únicamente oraciones, lo cual haría el proceso tedioso.\nEl operador |&gt; (pipe) en R, introducido en la versión 4.1, permite realizar operaciones en cadena, facilitando la secuencia de funciones y transformaciones en un flujo más legible y ordenado.\nEs evidente que, mediante el uso del operador pipe, podemos encadenar verbos de manera fluida y evitar la creación innecesaria de objetos, ya que este operador permite que el resultado a la izquierda se convierta automáticamente en el argumento de la función a la derecha.\n\ndata |&gt;\n  select(country, year, region, cpi_score) |&gt; \n  filter(year==2022) |&gt; \n  arrange(desc(cpi_score)) |&gt; \n  mutate(cpi_score2=100-cpi_score) |&gt; \n  summarise(mean(cpi_score2, na.rm=T))  \n\n# A tibble: 1 × 1\n  `mean(cpi_score2, na.rm = T)`\n                          &lt;dbl&gt;\n1                          57.0\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n¿Qué sucede si a esta cadena de pipes le doy un nombre? ¿Cuál sería el objeto creado?\n\n\n\ndata_final&lt;-data |&gt;\n  select(country, year, region, cpi_score) |&gt; \n  filter(year==2022) |&gt;\n  arrange(desc(cpi_score)) |&gt;\n  mutate(cpi_score2=100-cpi_score) \n\n\n\n2.2.8 Agrupando con group_by()\nSe utiliza para dividir un conjunto de datos en grupos según valores de una o más variables (normalmente de tipo categórica). Una vez que los datos están agrupados, es posible realizar operaciones específicas dentro de cada grupo.\n\ndata |&gt;   \n  group_by(year) |&gt; \n  summarise(Media=mean(cpi_score, na.rm = T)) \n\n# A tibble: 6 × 2\n   year Media\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2017  43.1\n2  2018  43.1\n3  2019  43.2\n4  2020  43.3\n5  2021  43.3\n6  2022  43.0\n\n\n\n\n2.2.9 Contar con count()\nFacilita el conteo de observaciones dentro de categorías específicas de una o más variables en un dataframe. Esta función agrupa el conjunto de datos por las variables especificadas y luego calcula el número de observaciones dentro de cada categoría, retornando un nuevo dataframe con las categorías y sus respectivos conteos. Es una herramienta esencial para obtener resúmenes rápidos y frecuencias de variables categóricas en datos estructurados.\n\ndata_final |&gt;   \n  count(region) |&gt;  \n  arrange(desc(n))  \n\n# A tibble: 6 × 2\n  region     n\n  &lt;chr&gt;  &lt;int&gt;\n1 SSA       49\n2 AME       32\n3 AP        32\n4 WE/EU     31\n5 ECA       19\n6 MENA      18\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn la última línea de código, indicamos a R que ordene los datos de acuerdo a la variable ‘n’, la cual fue definida en la línea de código precedente. Es importante recordar que la ejecución de acciones o funciones en R se realiza de manera secuencial y acumulativa.\n\n\n\n\n2.2.10 Renombrar con rename()\nPermite cambiar los nombres de las columnas de un dataframe. Para ello, se especifica el nuevo nombre deseado y el nombre actual de la columna. Esta función es útil cuando se necesita ajustar o estandarizar los nombres de las columnas en un conjunto de datos, facilitando así análisis posteriores y asegurando la claridad y consistencia en la manipulación de los datos.\nPrimero debes escribir el nuevo nombre y luego el nombre original de la variable.\n\ndata_final |&gt;   \n  rename(zona=region)   # Renombro la columna \"region\" (nombre original) como \"zona\" (nombre nuevo)\n\n# A tibble: 181 × 5\n   country      year zona  cpi_score cpi_score2\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU        90         10\n 2 Finland      2022 WE/EU        87         13\n 3 New Zealand  2022 AP           87         13\n 4 Norway       2022 WE/EU        84         16\n 5 Singapore    2022 AP           83         17\n 6 Sweden       2022 WE/EU        83         17\n 7 Switzerland  2022 WE/EU        82         18\n 8 Netherlands  2022 WE/EU        80         20\n 9 Germany      2022 WE/EU        79         21\n10 Ireland      2022 WE/EU        77         23\n# ℹ 171 more rows\n\n\n\n\n2.2.11 Recodificar con case_when()\nLa función case_when() del paquete tidyverse en R sirve para recodificar datos y crear nuevas variables o modificar variables existentes basándose en múltiples condiciones.\nPermite evaluar varias condiciones utilizando una sintaxis similar a una instrucción “if-else”. Esta función es particularmente útil cuando necesitamos recodificar una variable en varias categorías o cuando tenemos múltiples condiciones a evaluar.\nSe coloca primero la condición (fórmula) seguido del símbolo ~ (alt+126) y la etiqueta.\nAl final se coloca TRUE, lo que indica todos aquellos casos que no cumplen con las condiciones anteriores.\n\n\nsummary(data_final$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   45.00   60.50   57.02   70.25   88.00       1 \n\n\n\ndata_final&lt;-data_final |&gt; \n            drop_na(cpi_score2)\n            summary(data_final$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   45.00   60.50   57.02   70.25   88.00 \n\n\nPodemos realizar:\n\ndata_final&lt;-data_final  |&gt;   \n  mutate(corrupcion=case_when(cpi_score2&lt;30~\"Bajo\", \n                              cpi_score2&lt;60~\"Medio\", \n                              cpi_score2&lt;=100~\"Alto\")) \n\nLe asignamos la configuración adecuada:\n\ndata_final$corrupcion&lt;-factor(data_final$corrupcion,\n                          levels = c(\"Bajo\", \"Medio\", \"Alto\"),\n                          ordered = TRUE)\n\nFinalmente, ya contamos una nueva nueva variable ordinal creada a partir de una variable numérica:\n\nstr(data_final$corrupcion)\n\n Ord.factor w/ 3 levels \"Bajo\"&lt;\"Medio\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de datos para ML I</span>"
    ]
  },
  {
    "objectID": "C2_EDA1.html#ficha-resumen-cheat-sheet",
    "href": "C2_EDA1.html#ficha-resumen-cheat-sheet",
    "title": "2  Primeros pasos con tidyverse",
    "section": "2.3 Ficha resumen (Cheat Sheet)",
    "text": "2.3 Ficha resumen (Cheat Sheet)",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Primeros pasos con tidyverse</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#notas-de-clase",
    "href": "C3_EDA2.html#notas-de-clase",
    "title": "3  Análisis Exploratorio de datos para ML II",
    "section": "3.2 Notas de clase",
    "text": "3.2 Notas de clase",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Análisis Exploratorio de datos para ML II</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#visualización",
    "href": "C3_EDA2.html#visualización",
    "title": "3  Visualización de datos",
    "section": "",
    "text": "3.1.1 Continuamos con nuestro dataset\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\ndata &lt;- read_xlsx(\"data/AML_2.xlsx\")\n\n\n\n3.1.2 Anatomía de un ggplot\n\nggplot2 es un popular paquete de visualización de datos para el lenguaje de programación R, basado en los principios de la “Gramática de Gráficos”. Esta filosofía de diseño permite a los usuarios construir gráficos complejos y estéticamente agradables a partir de componentes básicos de forma intuitiva y flexible.\nEl núcleo de ggplot2 radica en su sistema de capas, donde cada gráfico se construye agregando capas que pueden incluir, entre otros, los datos, las estéticas (como color, forma y tamaño), los objetos geométricos (como puntos, líneas y barras), las escalas, y las anotaciones. Este enfoque modular no solo facilita la personalización y optimización de los gráficos sino que también promueve una estructura de código clara y comprensible.\nVamos a hacer un ejemplo paso a paso:\n\n3.1.2.1 1ra capa: Datos\nEs el conjunto de datos a visualizar.\nNuestra primera capa siempre va a ser la data. Sobre esta iniciamos la función ggplot y corroboramos que tenemos un lienzo en blanco.\n\ndata |&gt; \n  ggplot()\n\n\n\n\n\n\n\n\n\n\n3.1.2.2 2da capa: Estéticas\nEs el diseño básico del gráfico (Aesthetics).\nMapeo de variables a propiedades visuales como color, forma o tamaño, definidas con aes().\nA diferencia del lienzo en blanco, ya contamos con un diseño. En este caso, hemos indicado al R que el eje X será la variable Pobreza.\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn ggplot2, las capas de un gráfico se van adicionando secuencialmente utilizando el operador +.\n\n\n\n\n3.1.2.3 3ra capa: Geometrías (Geoms)\nSon representaciones gráficas de los datos, como puntos, líneas o barras (geom_point(), geom_line(), geom_bar(), etc.).\nEn nuestro ejemplo, podemos agregar la geometría de puntos para hacer un scatterplot o diagrama de dispersión:\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn el paquete {ggplot2} existen 30 geometrías disponibles. Puedes ver el detalle de estos en la documentación del paquete.\n\n\nEsta estructura de capas hace que ggplot2 sea extremadamente útil para explorar y presentar datos de manera efectiva, permitiendo a los usuarios desde principiantes hasta expertos crear visualizaciones de datos complejas y personalizadas con relativa facilidad.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#exploración-univariada",
    "href": "C3_EDA2.html#exploración-univariada",
    "title": "3  Visualización de datos",
    "section": "3.2 Exploración univariada",
    "text": "3.2 Exploración univariada\n\n3.2.1 Target\nLa primera mirada debe dirigirse al target, la variable que queremos predecir. Revisar su distribución nos dice si los valores están concentrados en un rango estrecho o dispersos, si hay sesgos o valores extremos que podrían distorsionar los resultados. Detectar estos patrones desde el inicio nos prepara para tomar decisiones informadas más adelante en el preprocesamiento.\n\n3.2.1.1 Histogramas\nEn este caso, nuestra variable target es el AML Index, por ello, podríamos comenzar con un histograma.\nEste primer gráfico nos informa sobre la distribución de la variable. Dicho de otra manera, describe con qué frecuencia aparecen ciertos valores o rangos de valores.\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nHistograma de la variable AML Index\n\n\n\n\nCuando realices un histograma te puedes guiar de las siguientes preguntas:\n\n¿Qué forma tiene la distribución? (simétrica, sesgada a la derecha o izquierda, con varias “montañas” o unimodal).\n¿Hay valores extremos que aparecen en los bordes?\n\nEn el ejemplo de arriba, podemos observar que ….\n\n\n3.2.1.2 Boxplots\nLuego de tener una idea sobre cómo se distribuye la variable, podríamos entrar a analizar su dispersión. La dispersión es indica qué tan extendidos o agrupados están los datos alrededor de un punto central (la media o la mediana). Si bien ya podemos tener una idea de ello viendo un histograma, siempre es útil utilizar un boxplot.\nEl boxplot es un gráfico estadístico que resume la información esencial de una variable numérica en un solo esquema. Se construye a partir de los cuartiles. Estos son medidas que dividen el conjunto de datos en partes ordenadas:\n\nPrimer cuartil (Q1): el valor por debajo del cual se encuentra el 25% de los datos.\nSegundo cuartil (Q2): es la mediana (50%).\nTercer cuartil (Q3): el valor por debajo del cual se encuentra el 75% de los datos.\n\nLa caja representa el rango intercuartílico (Q3 – Q1), que concentra al 50% central de los datos. La línea dentro de la caja marca la mediana. Los bigotes se extienden hasta los valores que aún se consideran normales, y los puntos fuera de ellos son los outliers.\n\ndata |&gt; \n  ggplot()+\n  aes(y=aml_index)+ #Para que aparezca vertical\n  geom_boxplot()\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCuando realices un boxplot te puedes guiar de las siguientes preguntas:\n\nMediana: ¿está en el centro o hacia un lado de la caja? → indica simetría o sesgo.\nCaja (Q1–Q3): ¿es ancha o angosta? → muestra la variabilidad del 50% central.\nBigotes: ¿son largos o cortos? ¿más extendidos en un lado? → refleja dispersión y asimetría.\nOutliers: ¿hay puntos fuera de los bigotes? ¿cuántos y qué tan alejados?\n\n\n\n\n\n\n\nTip\n\n\n\nSi no lo tienes claro, te aconsejo que revisites los conceptos de media, mediana y moda como medidas de tendencia central. Su importancia radica en que simplifican el análisis de grandes volúmenes de información, proporcionando un único valor representativo que ayuda a entender la naturaleza de los datos, compararlos con otros grupos, identificar valores atípicos y tomar decisiones informadas.\n\n\n\n\n\n3.2.2 Predictoras\nAnalizar las distribuciones de los predictores de manera individual nos permite descubrir escalas muy diferentes, valores atípicos o categorías poco representadas. Esta revisión inicial es clave para comprender la calidad y naturaleza de los insumos que alimentarán el modelo.\nComencemos con una variable categórica como continente. Antes habíamos visto frecuencias utilizando summary() o count():\n\ndata |&gt; \n  count(continente)\n\n# A tibble: 5 × 2\n  continente     n\n  &lt;chr&gt;      &lt;int&gt;\n1 Africa        29\n2 Americas      20\n3 Asia          17\n4 Europe        26\n5 Oceania        3\n\n\nAhora podemos también solicitar un gráfico de barras:\n\ndata |&gt; \n  ggplot()+\n  aes(x=continente)+\n  geom_bar()\n\n\n\n\n\n\n\n\nO también puedes utilizar una extensión de ggplot como ggchart:\n\n#install.packages(\"ggcharts\")\nlibrary(ggcharts)\n\nY solicitamos un diagrama similar:\n\ndata |&gt; \n  bar_chart(x=continente)+\n  geom_text(aes(label = n, hjust = \"left\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#exploración-multivariada",
    "href": "C3_EDA2.html#exploración-multivariada",
    "title": "3  Exploración avanzada",
    "section": "3.4 Exploración multivariada",
    "text": "3.4 Exploración multivariada\nCuando avanzamos a exploración multivariada nuestro propósito debe ser analizar las interacciones entre las variables y analizar si encontramos alguna asociación.\nEsto es importante porque nuestro algoritmo de ML, sea un modelo tradicional o más avanzado (deep learning) va a utilizar esas interacciones para realizar las predicciones.\nAquí hay una norma: si los predictores son malos, el modelo será malo.\n\n3.4.1 Gráfico de dispersión\nSe utiliza para ver la relación entre dos variables numéricas. Aquí debemos recordar conceptos básicos como el plano cartesiano, el cual es un sistema de coordenadas que se utiliza para representar y visualizar puntos en un espacio bidimensional.\nEstá compuesto por dos ejes perpendiculares, el eje horizontal o eje de las abscisas (X) y el eje vertical o eje de las ordenadas (Y).\nEstos ejes se cruzan en un punto llamado origen, que se representa con las coordenadas (0,0). Cada punto en el plano cartesiano se representa mediante un par ordenado (x, y), donde “x” indica la posición horizontal del punto a lo largo del eje X y “y” indica la posición vertical del punto a lo largo del eje Y.\nEl plano cartesiano proporciona un marco de referencia visual que facilita la representación gráfica de datos, funciones matemáticas, relaciones y patrones geométricos, permitiendo el análisis y la interpretación de información en el contexto bidimensional.\n\ndata |&gt; \n  ggplot()+\n  aes(x=cpi_index, y=aml_index)+\n  geom_point()\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGuíate con estas preguntas básicas:\n\n¿Los puntos que vemos en el plano forman alguna figura como una línea? Mientras la línea sea más clara, diremos que la relación lineal será alta En cambio si vemos una línea difícilmente, o simplemente una gran nube de puntos, podríamos decir que la relación lineal es baja.\n¿Una variable aumenta mientras la otra aumenta? Estamos frente a una relación positiva o directa.\n¿Una variable aumenta mientras la otra disminuye? Estamos frente a una relación negativa o indirecta.\nOjo, puede mostrar también una relación no lineal.\n\n\n\n\n\n3.4.2 Extra: Coeficiente de correlación\nUna medida que suele solicitarse junto con el gráfico de dispersión, es el coeficiente de correlación. La correlación es una medida que describe la relación o asociación entre dos variables numéricas.\nEl coeficiente de correlación de Pearson indica la fuerza y la dirección de la relación lineal entre las variables y se mide a través de un coeficiente denominado coeficiente de correlación de Pearson.\nEl coeficiente puede tomar los valores en el rango de -1 a 1.\n\nSolicitemos el coeficiente de correlación entre aml_index y cpi_index\n\ncor.test(data$aml_index, data$cpi_index)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$aml_index and data$cpi_index\nt = 9.7102, df = 75, p-value = 6.715e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6271554 0.8313204\nsample estimates:\n      cor \n0.7463016 \n\n\n\n\n\n\n\n\nTip\n\n\n\nGuíate con estas preguntas básicas:\n\nFUERZA: Mientras el valor del coeficiente se aleje más del 0 (sea más grande como valor absoluto) ello indicará una mayor correlación entre las dos variables numéricas.\nDIRECCIÓN: Cuando el coeficiente tiene signo positivo, ello indicará que la relación tiene sentido directo, es decir, mientras una variable aumenta, la otra aumenta. Si el signo es negativo, mientras una variable aumenta la otra disminuye.\n\n\n\nTambién puedes utilizar la siguiente escala referencial para guiar tu interpretación:\n\nEsto es importante para algunos algoritmos que suponen linealidad, como el caso de la Regresión Lineal.\n\n\n3.4.3 Ejercicio en clase\nCrea los gráficos de dispersión y calcula los coeficientes de correlación que se obtiene del cruce de nuestra variable target (aml_index) y las predictoras:\n\nmatricula\npbi_pc\npobreza\nurbano\neducación\nrule_of_law\ndemocracy_index\norganized_crime_index\n\n\n\n3.4.4 Gráfico + Coeficiente\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nsolo_numericas &lt;- data |&gt; select(where(is.numeric))\n\ncorrplot(cor(solo_numericas, \n             use = \"complete.obs\"), \n         method=\"number\", \n         type=\"lower\",\n         tl.cex=0.8,\n        number.digits = 1)",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C2_EDA1.html#notas-de-la-clase",
    "href": "C2_EDA1.html#notas-de-la-clase",
    "title": "2  Análisis Exploratorio de datos para ML I",
    "section": "2.2 Notas de la clase",
    "text": "2.2 Notas de la clase\nLa primera etapa del Análisis Exploratorio de Datos (EDA) en Machine Learning se centra en los pasos básicos con tidyverse, ya que este ecosistema de herramientas facilita la importación, organización y transformación de la información. En este punto, resulta esencial familiarizarnos con la estructura de los datos, reconocer las variables disponibles y aplicar operaciones que nos permitan manipular y preparar la base de forma eficiente. Este proceso no solo brinda una comprensión profunda de la data, sino que también sienta los cimientos para un análisis riguroso y para el éxito de las fases posteriores de modelamiento.\nTe sugiero ver este video introductorio sobre el Tidyverse:\n\nAbrimos la librería tidyverse:\n\nlibrary(tidyverse)\n\ndplyr es un paquete del Tidyverse que sirve para manipular tablas y transformarlas. Tiene una amplia gama de verbos con los cuales podemos realizar las tareas más recurrentes de la manipulación de datos.\n\n\n2.2.1 Importación de archivos\nPodríamos abrir este archivo con la función read_xlsx():\n\nlibrary(readxl)\ndata&lt;-read_xlsx(\"data/CPI.xlsx\")\n\nVisualizamos la data y solicitamos que nos diga la clase:\n\ndata\n\n# A tibble: 1,086 × 5\n   country               year iso3  region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AFG   AP            24\n 2 Albania               2022 ALB   ECA           36\n 3 United Arab Emirates  2022 ARE   MENA          67\n 4 Angola                2022 AGO   SSA           33\n 5 Argentina             2022 ARG   AME           38\n 6 Armenia               2022 ARM   ECA           46\n 7 Australia             2022 AUS   AP            75\n 8 Austria               2022 AUT   WE/EU         71\n 9 Azerbaijan            2022 AZE   ECA           23\n10 Bahamas               2022 BHS   AME           64\n# ℹ 1,076 more rows\n\nclass(data)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n\nNota\n\n\n\nUn tibble es una versión moderna del dataframe en R, parte del tidyverse, diseñado para facilitar el trabajo con datos tabulares.\n\n\nExploración del dataset y configuración de variables\nVemos la estructura rápidamente:\n\nstr(data)\n\ntibble [1,086 × 5] (S3: tbl_df/tbl/data.frame)\n $ country  : chr [1:1086] \"Afghanistan\" \"Albania\" \"United Arab Emirates\" \"Angola\" ...\n $ year     : num [1:1086] 2022 2022 2022 2022 2022 ...\n $ iso3     : chr [1:1086] \"AFG\" \"ALB\" \"ARE\" \"AGO\" ...\n $ region   : chr [1:1086] \"AP\" \"ECA\" \"MENA\" \"SSA\" ...\n $ cpi_score: num [1:1086] 24 36 67 33 38 46 75 71 23 64 ...\n\n\nAl ejecutar names() sobre un conjunto de datos, se nos devuelve un vector con los nombres de todas las columnas en el orden en que aparecen.\n\nnames(data)\n\n[1] \"country\"   \"year\"      \"iso3\"      \"region\"    \"cpi_score\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nAntes de proseguir con el análisis exploratorio de datos, es fundamental que comprendas claramente qué representan las filas y las columnas en tus datos.\n\n\nCorroboramos que el score del AML esté adecuadamente configurado.\n\nclass(data$cpi_score)\n\n[1] \"numeric\"\n\n\nDe acuerdo, podemos proseguir.\nExaminemos la base original. Vamos a editar la tabla con diversos verbos de dplyr.\n\n\n2.2.2 Select()\nLa función select() es utilizada para seleccionar o excluir columnas de un data frame o tibble en R. Va más allá de simplemente escoger columnas por nombre, ya que permite una amplia gama de criterios y operaciones.\nFuncionamiento básico:\n\nEntrada: Un data frame o tibble y un conjunto de nombres de columnas o criterios para seleccionar columnas.\nSalida: Un objeto de la misma clase que el de entrada (data frame o tibble) que contiene solo las columnas seleccionadas.\n\nVamos a seleccionar sólo ciertas columnas:\n\ndata1&lt;-data |&gt; \n  select(country, year, region, cpi_score)\ndata1\n\n# A tibble: 1,086 × 4\n   country               year region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AP            24\n 2 Albania               2022 ECA           36\n 3 United Arab Emirates  2022 MENA          67\n 4 Angola                2022 SSA           33\n 5 Argentina             2022 AME           38\n 6 Armenia               2022 ECA           46\n 7 Australia             2022 AP            75\n 8 Austria               2022 WE/EU         71\n 9 Azerbaijan            2022 ECA           23\n10 Bahamas               2022 AME           64\n# ℹ 1,076 more rows\n\n\nTener en cuenta que puedes:\n\nSeleccionar por nombre\nSeleccionar por el número de la columna\n(Des)seleccionar colocando un “-” antes del nombre/número de columna.\nSeleccionar un rango colocando por ejemplo 2:4 lo que significa “desde la columna 2 hasta la columna 4).\nPuedes combinar todas las anteriores y tener más de un criterio a la vez separándolo por coma.\n\n\n\n2.2.3 Filter()\nLa función filter() se utiliza para filtrar filas de un data frame o tibble en R en función de condiciones específicas, permitiendo crear un subconjunto de datos.\nAl crear subconjuntos nuestros datos de forma precisa, podemos focalizar nuestro análisis, mejorar la eficiencia computacional y obtener resultados más claros y relevantes.\nCaracterísticas principales:\n\nCondiciones múltiples: Puedes usar múltiples condiciones para filtrar tus datos. Estas se combinan utilizando operadores lógicos como & (y), | (o) y ! (no).\nUso de operadores de comparación: Los operadores estándar como ==, &gt;, &lt;, &gt;=, &lt;=, y != se utilizan para establecer condiciones.\nFunciones auxiliares: dplyr proporciona funciones como between(), que pueden ser útiles para establecer condiciones. Por ejemplo, between(x, 1, 10) es equivalente a x &gt;= 1 & x &lt;= 10.\n\nEn este caso vamos a seleccionar aquellos países cuya medición es del año 2022.\n\ndata2&lt;-data1 %&gt;%                   \n  filter(year==2022)\ndata2\n\n# A tibble: 181 × 4\n   country               year region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AP            24\n 2 Albania               2022 ECA           36\n 3 United Arab Emirates  2022 MENA          67\n 4 Angola                2022 SSA           33\n 5 Argentina             2022 AME           38\n 6 Armenia               2022 ECA           46\n 7 Australia             2022 AP            75\n 8 Austria               2022 WE/EU         71\n 9 Azerbaijan            2022 ECA           23\n10 Bahamas               2022 AME           64\n# ℹ 171 more rows\n\n\n\n\n2.2.4 Arrange()\nSe utiliza para ordenar (o reordenar) un data frame o tibble según una o más columnas.\nFuncionamiento básico:\n\nOrdenación simple: Si proporcionas una columna a arrange(), ordenará el data frame en función de esa columna en orden ascendente por defecto.\nOrdenación descendente: Si deseas ordenar en dirección descendente, puedes usar la función desc(). Por ejemplo: df |&gt; arrange(desc(edad)) ordenará el data frame por la columna “edad” en orden descendente.\nOrdenación múltiple: Puedes proporcionar múltiples columnas para ordenar, y arrange() las usará en el orden proporcionado para determinar el ordenamiento. Por ejemplo, si deseas ordenar primero por “grupo” y luego por “edad” dentro de cada grupo, usarías: df |&gt; arrange(grupo, edad).\n\n\ndata3&lt;-data2 |&gt;    \n  arrange(desc(cpi_score))\ndata3\n\n# A tibble: 181 × 4\n   country      year region cpi_score\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU         90\n 2 Finland      2022 WE/EU         87\n 3 New Zealand  2022 AP            87\n 4 Norway       2022 WE/EU         84\n 5 Singapore    2022 AP            83\n 6 Sweden       2022 WE/EU         83\n 7 Switzerland  2022 WE/EU         82\n 8 Netherlands  2022 WE/EU         80\n 9 Germany      2022 WE/EU         79\n10 Ireland      2022 WE/EU         77\n# ℹ 171 more rows\n\n\n\n\n2.2.5 Mutate()\nLa función mutate() está diseñada para crear o modificar columnas dentro de un data frame o tibble en R. Mientras que el data frame original se mantiene inalterado, mutate() devuelve una copia con las columnas especificadas añadidas o alteradas.\nEn este caso vamos a crear una variable cambiando la escala del score del CPI.\nEn la medida original 0 representaba alta corrupción y 100 escasa corrupción. Ahora, si realizamos la operación “100 - cpi_score”, los valores cercanos a 0 tendrán poca corrupción y los cercanos a 100 alta corrupción, siendo más intuitivo.\nEsta transformación puede ser útil para ajustar la interpretación de los datos a contextos donde es más intuitivo trabajar con escalas donde un número mayor indica mayor intensidad de un fenómeno (Corrupción, en este caso), dependiendo del análisis que se desea realizar.\n\ndata4&lt;-data3 |&gt;   \n  mutate(cpi_score2=100-cpi_score) \ndata4\n\n# A tibble: 181 × 5\n   country      year region cpi_score cpi_score2\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU         90         10\n 2 Finland      2022 WE/EU         87         13\n 3 New Zealand  2022 AP            87         13\n 4 Norway       2022 WE/EU         84         16\n 5 Singapore    2022 AP            83         17\n 6 Sweden       2022 WE/EU         83         17\n 7 Switzerland  2022 WE/EU         82         18\n 8 Netherlands  2022 WE/EU         80         20\n 9 Germany      2022 WE/EU         79         21\n10 Ireland      2022 WE/EU         77         23\n# ℹ 171 more rows\n\n\n\n\n2.2.6 Summarise()\nSe utiliza para crear resúmenes estadísticos de un data frame o tibble.\nDentro de los resúmenes puedes disponer de por ejemplo:\nMedidas de tendencia central: Estas funciones describen un valor central o típico dentro de un conjunto de datos.\n\nMedia: mean(x)\nMediana: median(x)\n\nCómo calcularíamos la media de forma directa (tradicional)?\n\nsummary(data4$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   45.00   60.50   57.02   70.25   88.00       1 \n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n¿Qué significa NA´s? ¿Por qué se produce esto en una data como la de Corruption Perception Index?\n\n\nCómo lo calculamos con tidyverse?\n\ndata4 |&gt;   \n  summarise(mean(cpi_score2, na.rm = T))    \n\n# A tibble: 1 × 1\n  `mean(cpi_score2, na.rm = T)`\n                          &lt;dbl&gt;\n1                          57.0\n\n\n\n\n\n\n\n\nNota\n\n\n\nCuando aplicamos un summarise lo que nos devuelve es un valor o conjunto de valores. Por otro lado, el argumento na.rm = TRUE se utiliza para especificar que los valores perdidos (NA) deben ser ignorados en el cálculo. De esta forma, le indicamos a la función que proceda con la operación excluyendo dichos valores ausentes.\n\n\n\n\n2.2.7 Utilizando pipe: |&gt;\nProbablemente hayas observado que hemos creado un conjunto de datos para cada verbo utilizado, lo cual en situaciones reales resultaría excesivamente repetitivo. Haciendo una analogía con la escritura de un libro, sería como si estuviéramos limitados a usar únicamente oraciones, lo cual haría el proceso tedioso.\nEl operador |&gt; (pipe) en R, introducido en la versión 4.1, permite realizar operaciones en cadena, facilitando la secuencia de funciones y transformaciones en un flujo más legible y ordenado.\nEs evidente que, mediante el uso del operador pipe, podemos encadenar verbos de manera fluida y evitar la creación innecesaria de objetos, ya que este operador permite que el resultado a la izquierda se convierta automáticamente en el argumento de la función a la derecha.\n\ndata |&gt;\n  select(country, year, region, cpi_score) |&gt; \n  filter(year==2022) |&gt; \n  arrange(desc(cpi_score)) |&gt; \n  mutate(cpi_score2=100-cpi_score) |&gt; \n  summarise(mean(cpi_score2, na.rm=T))  \n\n# A tibble: 1 × 1\n  `mean(cpi_score2, na.rm = T)`\n                          &lt;dbl&gt;\n1                          57.0\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n¿Qué sucede si a esta cadena de pipes le doy un nombre? ¿Cuál sería el objeto creado?\n\n\n\ndata_final&lt;-data |&gt;\n  select(country, year, region, cpi_score) |&gt; \n  filter(year==2022) |&gt;\n  arrange(desc(cpi_score)) |&gt;\n  mutate(cpi_score2=100-cpi_score) \n\n\n\n2.2.8 Agrupando con group_by()\nSe utiliza para dividir un conjunto de datos en grupos según valores de una o más variables (normalmente de tipo categórica). Una vez que los datos están agrupados, es posible realizar operaciones específicas dentro de cada grupo.\n\ndata |&gt;   \n  group_by(year) |&gt; \n  summarise(Media=mean(cpi_score, na.rm = T)) \n\n# A tibble: 6 × 2\n   year Media\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2017  43.1\n2  2018  43.1\n3  2019  43.2\n4  2020  43.3\n5  2021  43.3\n6  2022  43.0\n\n\n\n\n2.2.9 Contar con count()\nFacilita el conteo de observaciones dentro de categorías específicas de una o más variables en un dataframe. Esta función agrupa el conjunto de datos por las variables especificadas y luego calcula el número de observaciones dentro de cada categoría, retornando un nuevo dataframe con las categorías y sus respectivos conteos. Es una herramienta esencial para obtener resúmenes rápidos y frecuencias de variables categóricas en datos estructurados.\n\ndata_final |&gt;   \n  count(region) |&gt;  \n  arrange(desc(n))  \n\n# A tibble: 6 × 2\n  region     n\n  &lt;chr&gt;  &lt;int&gt;\n1 SSA       49\n2 AME       32\n3 AP        32\n4 WE/EU     31\n5 ECA       19\n6 MENA      18\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn la última línea de código, indicamos a R que ordene los datos de acuerdo a la variable ‘n’, la cual fue definida en la línea de código precedente. Es importante recordar que la ejecución de acciones o funciones en R se realiza de manera secuencial y acumulativa.\n\n\n\n\n2.2.10 Renombrar con rename()\nPermite cambiar los nombres de las columnas de un dataframe. Para ello, se especifica el nuevo nombre deseado y el nombre actual de la columna. Esta función es útil cuando se necesita ajustar o estandarizar los nombres de las columnas en un conjunto de datos, facilitando así análisis posteriores y asegurando la claridad y consistencia en la manipulación de los datos.\nPrimero debes escribir el nuevo nombre y luego el nombre original de la variable.\n\ndata_final |&gt;   \n  rename(zona=region)   # Renombro la columna \"region\" (nombre original) como \"zona\" (nombre nuevo)\n\n# A tibble: 181 × 5\n   country      year zona  cpi_score cpi_score2\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU        90         10\n 2 Finland      2022 WE/EU        87         13\n 3 New Zealand  2022 AP           87         13\n 4 Norway       2022 WE/EU        84         16\n 5 Singapore    2022 AP           83         17\n 6 Sweden       2022 WE/EU        83         17\n 7 Switzerland  2022 WE/EU        82         18\n 8 Netherlands  2022 WE/EU        80         20\n 9 Germany      2022 WE/EU        79         21\n10 Ireland      2022 WE/EU        77         23\n# ℹ 171 more rows\n\n\n\n\n2.2.11 Recodificar con case_when()\nLa función case_when() del paquete tidyverse en R sirve para recodificar datos y crear nuevas variables o modificar variables existentes basándose en múltiples condiciones.\nPermite evaluar varias condiciones utilizando una sintaxis similar a una instrucción “if-else”. Esta función es particularmente útil cuando necesitamos recodificar una variable en varias categorías o cuando tenemos múltiples condiciones a evaluar.\nSe coloca primero la condición (fórmula) seguido del símbolo ~ (alt+126) y la etiqueta.\nAl final se coloca TRUE, lo que indica todos aquellos casos que no cumplen con las condiciones anteriores.\n\n\nsummary(data_final$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   45.00   60.50   57.02   70.25   88.00       1 \n\n\n\ndata_final&lt;-data_final |&gt; \n            drop_na(cpi_score2)\n            summary(data_final$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   45.00   60.50   57.02   70.25   88.00 \n\n\nPodemos realizar:\n\ndata_final&lt;-data_final  |&gt;   \n  mutate(corrupcion=case_when(cpi_score2&lt;30~\"Bajo\", \n                              cpi_score2&lt;60~\"Medio\", \n                              cpi_score2&lt;=100~\"Alto\")) \n\nLe asignamos la configuración adecuada:\n\ndata_final$corrupcion&lt;-factor(data_final$corrupcion,\n                          levels = c(\"Bajo\", \"Medio\", \"Alto\"),\n                          ordered = TRUE)\n\nFinalmente, ya contamos una nueva nueva variable ordinal creada a partir de una variable numérica:\n\nstr(data_final$corrupcion)\n\n Ord.factor w/ 3 levels \"Bajo\"&lt;\"Medio\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de datos para ML I</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#exploración-univariada-target",
    "href": "C3_EDA2.html#exploración-univariada-target",
    "title": "3  Exploración avanzada",
    "section": "3.2 Exploración univariada: Target",
    "text": "3.2 Exploración univariada: Target\nLa primera mirada debe dirigirse al target, la variable que queremos predecir. Revisar su distribución nos dice si los valores están concentrados en un rango estrecho o dispersos, si hay sesgos o valores extremos que podrían distorsionar los resultados. Detectar estos patrones desde el inicio nos prepara para tomar decisiones informadas más adelante en el preprocesamiento.\n\n3.2.1 Histogramas\nEn este caso, nuestra variable target es el AML Index, por ello, podríamos comenzar con un histograma.\nEste primer gráfico nos informa sobre la distribución de la variable. Dicho de otra manera, describe con qué frecuencia aparecen ciertos valores o rangos de valores.\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nHistograma de la variable AML Index\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGuíate con estas preguntas básicas:\n\n¿Qué forma tiene la distribución? (simétrica como campana, se nota una cola a la derecha o izquierda, con varias “montañas” o unimodal).\n¿Hay valores extremos que aparecen en los bordes?\n\n\n\n\n\n3.2.2 Boxplots\nLuego de tener una idea sobre la forma cómo se distribuye la variable, podríamos entrar a analizar su dispersión. La dispersión es indica qué tan extendidos o agrupados están los datos alrededor de un punto central (la media o la mediana). Si bien ya podemos tener una idea de ello viendo un histograma, siempre es útil utilizar un boxplot.\nEl boxplot es un gráfico estadístico que resume la información esencial de una variable numérica en un solo esquema. Se construye a partir de los cuartiles. Estos son medidas que dividen el conjunto de datos en partes ordenadas:\n\nPrimer cuartil (Q1): el valor por debajo del cual se encuentra el 25% de los datos.\nSegundo cuartil (Q2): es la mediana (50%).\nTercer cuartil (Q3): el valor por debajo del cual se encuentra el 75% de los datos.\n\nLa caja representa el rango intercuartílico (Q3 – Q1), que concentra al 50% central de los datos. La línea dentro de la caja marca la mediana. Los bigotes se extienden hasta los valores que aún se consideran normales, y los puntos fuera de ellos son los outliers.\n\ndata |&gt; \n  ggplot()+\n  aes(y=aml_index)+ #Para que aparezca vertical\n  geom_boxplot()\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGuíate con estas preguntas básicas:\n\nMediana: ¿está en el centro o hacia un lado de la caja? → indica simetría o sesgo.\nCaja (Q1–Q3): ¿es ancha o angosta? → muestra la variabilidad del 50% central.\nBigotes: ¿son largos o cortos? ¿más extendidos en un lado? → refleja dispersión y asimetría.\nOutliers: ¿hay puntos fuera de los bigotes? ¿cuántos y qué tan alejados?\n\n\n\nSi no lo tienes claro, te aconsejo que revisites los conceptos de media, mediana y moda como medidas de tendencia central. Su importancia radica en que simplifican el análisis de grandes volúmenes de información, proporcionando un único valor representativo que ayuda a entender la naturaleza de los datos.\n\n\n3.2.3 Ejercicio en clase\nDibujemos un boxplot desde 0.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#exploración-univariada-predictoras",
    "href": "C3_EDA2.html#exploración-univariada-predictoras",
    "title": "3  Exploración avanzada",
    "section": "3.3 Exploración univariada: Predictoras",
    "text": "3.3 Exploración univariada: Predictoras\nAnalizar las distribuciones de los predictores de manera individual nos permite descubrir escalas muy diferentes, valores atípicos o categorías poco representadas. Esta revisión inicial es clave para comprender la calidad y naturaleza de los insumos que alimentarán el modelo.\n\n3.3.1 Gráfico de barras\nComencemos con una variable categórica como continente. Antes habíamos visto frecuencias utilizando summary() o count():\n\ndata |&gt; \n  count(continente)\n\n# A tibble: 5 × 2\n  continente     n\n  &lt;chr&gt;      &lt;int&gt;\n1 Africa        29\n2 Americas      20\n3 Asia          17\n4 Europe        26\n5 Oceania        3\n\n\nAhora podemos también solicitar un gráfico de barras:\n\ndata |&gt; \n  ggplot()+\n  aes(x=continente)+\n  geom_bar()\n\n\n\n\n\n\n\n\nO también puedes utilizar una extensión de ggplot como ggchart:\n\n#install.packages(\"ggcharts\")\nlibrary(ggcharts)\n\nY solicitamos un diagrama similar, pero con una mejor presentación:\n\ndata |&gt; \n  bar_chart(x=continente)+\n  geom_text(aes(label = n, hjust = \"left\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGuíate con estas preguntas básicas:\n\n¿Existen categorías con muy pocas observaciones?\nCuando una categoría tiene muy pocas observaciones, puede causar problemas en el modelado (coeficientes inestables en regresión, riesgo de sobreajuste en árboles o dummies casi vacías).\n\n\n\n\n\n3.3.2 Ejercicio en clase\nYa exploramos visualmente la variable target y también una predictora, ahora genera tus propios gráficos para:\n\nmatricula\npbi_pc\npobreza\neducacion\ndemocracy_index_cat\norganized_crime_index_cat",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#guíate-con-estas-preguntas",
    "href": "C3_EDA2.html#guíate-con-estas-preguntas",
    "title": "3  Visualización de datos",
    "section": "3.5 Guíate con estas preguntas:",
    "text": "3.5 Guíate con estas preguntas:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos</span>"
    ]
  },
  {
    "objectID": "C2_EDA1.html#comprendiendo-nuestra-data",
    "href": "C2_EDA1.html#comprendiendo-nuestra-data",
    "title": "2  Primeros pasos con tidyverse",
    "section": "2.2 Comprendiendo nuestra data",
    "text": "2.2 Comprendiendo nuestra data\nLa primera etapa del Análisis Exploratorio de Datos (EDA) en Machine Learning se centra en los pasos básicos con tidyverse, ya que este ecosistema de herramientas facilita la importación, organización y transformación de la información. En este punto, resulta esencial familiarizarnos con la estructura de los datos, reconocer las variables disponibles y aplicar operaciones que nos permitan manipular y preparar la base de forma eficiente. Este proceso no solo brinda una comprensión profunda de la data, sino que también sienta los cimientos para un análisis riguroso y para el éxito de las fases posteriores de modelamiento.\nTe sugiero ver este video introductorio sobre el Tidyverse:\n\nAbrimos la librería tidyverse:\n\nlibrary(tidyverse)\n\ndplyr es un paquete del Tidyverse que sirve para manipular tablas y transformarlas. Tiene una amplia gama de verbos con los cuales podemos realizar las tareas más recurrentes de la manipulación de datos.\n\n\n2.2.1 Importación de archivos\nPodríamos abrir este archivo con la función read_xlsx():\n\nlibrary(readxl)\ndata&lt;-read_xlsx(\"data/CPI.xlsx\")\n\nVisualizamos la data y solicitamos que nos diga la clase:\n\ndata\n\n# A tibble: 1,086 × 5\n   country               year iso3  region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AFG   AP            24\n 2 Albania               2022 ALB   ECA           36\n 3 United Arab Emirates  2022 ARE   MENA          67\n 4 Angola                2022 AGO   SSA           33\n 5 Argentina             2022 ARG   AME           38\n 6 Armenia               2022 ARM   ECA           46\n 7 Australia             2022 AUS   AP            75\n 8 Austria               2022 AUT   WE/EU         71\n 9 Azerbaijan            2022 AZE   ECA           23\n10 Bahamas               2022 BHS   AME           64\n# ℹ 1,076 more rows\n\nclass(data)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n\nNota\n\n\n\nUn tibble es una versión moderna del dataframe en R, parte del tidyverse, diseñado para facilitar el trabajo con datos tabulares.\n\n\nExploración del dataset y configuración de variables\nVemos la estructura rápidamente:\n\nstr(data)\n\ntibble [1,086 × 5] (S3: tbl_df/tbl/data.frame)\n $ country  : chr [1:1086] \"Afghanistan\" \"Albania\" \"United Arab Emirates\" \"Angola\" ...\n $ year     : num [1:1086] 2022 2022 2022 2022 2022 ...\n $ iso3     : chr [1:1086] \"AFG\" \"ALB\" \"ARE\" \"AGO\" ...\n $ region   : chr [1:1086] \"AP\" \"ECA\" \"MENA\" \"SSA\" ...\n $ cpi_score: num [1:1086] 24 36 67 33 38 46 75 71 23 64 ...\n\n\nAl ejecutar names() sobre un conjunto de datos, se nos devuelve un vector con los nombres de todas las columnas en el orden en que aparecen.\n\nnames(data)\n\n[1] \"country\"   \"year\"      \"iso3\"      \"region\"    \"cpi_score\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nAntes de proseguir con el análisis exploratorio de datos, es fundamental que comprendas claramente qué representan las filas y las columnas en tus datos.\n\n\nCorroboramos que el score del AML esté adecuadamente configurado.\n\nclass(data$cpi_score)\n\n[1] \"numeric\"\n\n\nDe acuerdo, podemos proseguir.\nExaminemos la base original. Vamos a editar la tabla con diversos verbos de dplyr.\n\n\n2.2.2 Select()\nLa función select() es utilizada para seleccionar o excluir columnas de un data frame o tibble en R. Va más allá de simplemente escoger columnas por nombre, ya que permite una amplia gama de criterios y operaciones.\nFuncionamiento básico:\n\nEntrada: Un data frame o tibble y un conjunto de nombres de columnas o criterios para seleccionar columnas.\nSalida: Un objeto de la misma clase que el de entrada (data frame o tibble) que contiene solo las columnas seleccionadas.\n\nVamos a seleccionar sólo ciertas columnas:\n\ndata1&lt;-data |&gt; \n  select(country, year, region, cpi_score)\ndata1\n\n# A tibble: 1,086 × 4\n   country               year region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AP            24\n 2 Albania               2022 ECA           36\n 3 United Arab Emirates  2022 MENA          67\n 4 Angola                2022 SSA           33\n 5 Argentina             2022 AME           38\n 6 Armenia               2022 ECA           46\n 7 Australia             2022 AP            75\n 8 Austria               2022 WE/EU         71\n 9 Azerbaijan            2022 ECA           23\n10 Bahamas               2022 AME           64\n# ℹ 1,076 more rows\n\n\nTener en cuenta que puedes:\n\nSeleccionar por nombre\nSeleccionar por el número de la columna\n(Des)seleccionar colocando un “-” antes del nombre/número de columna.\nSeleccionar un rango colocando por ejemplo 2:4 lo que significa “desde la columna 2 hasta la columna 4).\nPuedes combinar todas las anteriores y tener más de un criterio a la vez separándolo por coma.\n\n\n\n2.2.3 Filter()\nLa función filter() se utiliza para filtrar filas de un data frame o tibble en R en función de condiciones específicas, permitiendo crear un subconjunto de datos.\nAl crear subconjuntos nuestros datos de forma precisa, podemos focalizar nuestro análisis, mejorar la eficiencia computacional y obtener resultados más claros y relevantes.\nCaracterísticas principales:\n\nCondiciones múltiples: Puedes usar múltiples condiciones para filtrar tus datos. Estas se combinan utilizando operadores lógicos como & (y), | (o) y ! (no).\nUso de operadores de comparación: Los operadores estándar como ==, &gt;, &lt;, &gt;=, &lt;=, y != se utilizan para establecer condiciones.\nFunciones auxiliares: dplyr proporciona funciones como between(), que pueden ser útiles para establecer condiciones. Por ejemplo, between(x, 1, 10) es equivalente a x &gt;= 1 & x &lt;= 10.\n\nEn este caso vamos a seleccionar aquellos países cuya medición es del año 2022.\n\ndata2&lt;-data1 %&gt;%                   \n  filter(year==2022)\ndata2\n\n# A tibble: 181 × 4\n   country               year region cpi_score\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Afghanistan           2022 AP            24\n 2 Albania               2022 ECA           36\n 3 United Arab Emirates  2022 MENA          67\n 4 Angola                2022 SSA           33\n 5 Argentina             2022 AME           38\n 6 Armenia               2022 ECA           46\n 7 Australia             2022 AP            75\n 8 Austria               2022 WE/EU         71\n 9 Azerbaijan            2022 ECA           23\n10 Bahamas               2022 AME           64\n# ℹ 171 more rows\n\n\n\n\n2.2.4 Arrange()\nSe utiliza para ordenar (o reordenar) un data frame o tibble según una o más columnas.\nFuncionamiento básico:\n\nOrdenación simple: Si proporcionas una columna a arrange(), ordenará el data frame en función de esa columna en orden ascendente por defecto.\nOrdenación descendente: Si deseas ordenar en dirección descendente, puedes usar la función desc(). Por ejemplo: df |&gt; arrange(desc(edad)) ordenará el data frame por la columna “edad” en orden descendente.\nOrdenación múltiple: Puedes proporcionar múltiples columnas para ordenar, y arrange() las usará en el orden proporcionado para determinar el ordenamiento. Por ejemplo, si deseas ordenar primero por “grupo” y luego por “edad” dentro de cada grupo, usarías: df |&gt; arrange(grupo, edad).\n\n\ndata3&lt;-data2 |&gt;    \n  arrange(desc(cpi_score))\ndata3\n\n# A tibble: 181 × 4\n   country      year region cpi_score\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU         90\n 2 Finland      2022 WE/EU         87\n 3 New Zealand  2022 AP            87\n 4 Norway       2022 WE/EU         84\n 5 Singapore    2022 AP            83\n 6 Sweden       2022 WE/EU         83\n 7 Switzerland  2022 WE/EU         82\n 8 Netherlands  2022 WE/EU         80\n 9 Germany      2022 WE/EU         79\n10 Ireland      2022 WE/EU         77\n# ℹ 171 more rows\n\n\n\n\n2.2.5 Mutate()\nLa función mutate() está diseñada para crear o modificar columnas dentro de un data frame o tibble en R. Mientras que el data frame original se mantiene inalterado, mutate() devuelve una copia con las columnas especificadas añadidas o alteradas.\nEn este caso vamos a crear una variable cambiando la escala del score del CPI.\nEn la medida original 0 representaba alta corrupción y 100 escasa corrupción. Ahora, si realizamos la operación “100 - cpi_score”, los valores cercanos a 0 tendrán poca corrupción y los cercanos a 100 alta corrupción, siendo más intuitivo.\nEsta transformación puede ser útil para ajustar la interpretación de los datos a contextos donde es más intuitivo trabajar con escalas donde un número mayor indica mayor intensidad de un fenómeno (Corrupción, en este caso), dependiendo del análisis que se desea realizar.\n\ndata4&lt;-data3 |&gt;   \n  mutate(cpi_score2=100-cpi_score) \ndata4\n\n# A tibble: 181 × 5\n   country      year region cpi_score cpi_score2\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU         90         10\n 2 Finland      2022 WE/EU         87         13\n 3 New Zealand  2022 AP            87         13\n 4 Norway       2022 WE/EU         84         16\n 5 Singapore    2022 AP            83         17\n 6 Sweden       2022 WE/EU         83         17\n 7 Switzerland  2022 WE/EU         82         18\n 8 Netherlands  2022 WE/EU         80         20\n 9 Germany      2022 WE/EU         79         21\n10 Ireland      2022 WE/EU         77         23\n# ℹ 171 more rows\n\n\n\n\n2.2.6 Summarise()\nSe utiliza para crear resúmenes estadísticos de un data frame o tibble.\nDentro de los resúmenes puedes disponer de por ejemplo:\nMedidas de tendencia central: Estas funciones describen un valor central o típico dentro de un conjunto de datos.\n\nMedia: mean(x)\nMediana: median(x)\n\nCómo calcularíamos la media de forma directa (tradicional)?\n\nsummary(data4$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   45.00   60.50   57.02   70.25   88.00       1 \n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n¿Qué significa NA´s? ¿Por qué se produce esto en una data como la de Corruption Perception Index?\n\n\nCómo lo calculamos con tidyverse?\n\ndata4 |&gt;   \n  summarise(mean(cpi_score2, na.rm = T))    \n\n# A tibble: 1 × 1\n  `mean(cpi_score2, na.rm = T)`\n                          &lt;dbl&gt;\n1                          57.0\n\n\n\n\n\n\n\n\nNota\n\n\n\nCuando aplicamos un summarise lo que nos devuelve es un valor o conjunto de valores. Por otro lado, el argumento na.rm = TRUE se utiliza para especificar que los valores perdidos (NA) deben ser ignorados en el cálculo. De esta forma, le indicamos a la función que proceda con la operación excluyendo dichos valores ausentes.\n\n\n\n\n2.2.7 Utilizando pipe: |&gt;\nProbablemente hayas observado que hemos creado un conjunto de datos para cada verbo utilizado, lo cual en situaciones reales resultaría excesivamente repetitivo. Haciendo una analogía con la escritura de un libro, sería como si estuviéramos limitados a usar únicamente oraciones, lo cual haría el proceso tedioso.\nEl operador |&gt; (pipe) en R, introducido en la versión 4.1, permite realizar operaciones en cadena, facilitando la secuencia de funciones y transformaciones en un flujo más legible y ordenado.\nEs evidente que, mediante el uso del operador pipe, podemos encadenar verbos de manera fluida y evitar la creación innecesaria de objetos, ya que este operador permite que el resultado a la izquierda se convierta automáticamente en el argumento de la función a la derecha.\n\ndata |&gt;\n  select(country, year, region, cpi_score) |&gt; \n  filter(year==2022) |&gt; \n  arrange(desc(cpi_score)) |&gt; \n  mutate(cpi_score2=100-cpi_score) |&gt; \n  summarise(mean(cpi_score2, na.rm=T))  \n\n# A tibble: 1 × 1\n  `mean(cpi_score2, na.rm = T)`\n                          &lt;dbl&gt;\n1                          57.0\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n¿Qué sucede si a esta cadena de pipes le doy un nombre? ¿Cuál sería el objeto creado?\n\n\n\ndata_final&lt;-data |&gt;\n  select(country, year, region, cpi_score) |&gt; \n  filter(year==2022) |&gt;\n  arrange(desc(cpi_score)) |&gt;\n  mutate(cpi_score2=100-cpi_score) \n\n\n\n2.2.8 Agrupando con group_by()\nSe utiliza para dividir un conjunto de datos en grupos según valores de una o más variables (normalmente de tipo categórica). Una vez que los datos están agrupados, es posible realizar operaciones específicas dentro de cada grupo.\n\ndata |&gt;   \n  group_by(year) |&gt; \n  summarise(Media=mean(cpi_score, na.rm = T)) \n\n# A tibble: 6 × 2\n   year Media\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2017  43.1\n2  2018  43.1\n3  2019  43.2\n4  2020  43.3\n5  2021  43.3\n6  2022  43.0\n\n\n\n\n2.2.9 Contar con count()\nFacilita el conteo de observaciones dentro de categorías específicas de una o más variables en un dataframe. Esta función agrupa el conjunto de datos por las variables especificadas y luego calcula el número de observaciones dentro de cada categoría, retornando un nuevo dataframe con las categorías y sus respectivos conteos. Es una herramienta esencial para obtener resúmenes rápidos y frecuencias de variables categóricas en datos estructurados.\n\ndata_final |&gt;   \n  count(region) |&gt;  \n  arrange(desc(n))  \n\n# A tibble: 6 × 2\n  region     n\n  &lt;chr&gt;  &lt;int&gt;\n1 SSA       49\n2 AME       32\n3 AP        32\n4 WE/EU     31\n5 ECA       19\n6 MENA      18\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn la última línea de código, indicamos a R que ordene los datos de acuerdo a la variable ‘n’, la cual fue definida en la línea de código precedente. Es importante recordar que la ejecución de acciones o funciones en R se realiza de manera secuencial y acumulativa.\n\n\n\n\n2.2.10 Renombrar con rename()\nPermite cambiar los nombres de las columnas de un dataframe. Para ello, se especifica el nuevo nombre deseado y el nombre actual de la columna. Esta función es útil cuando se necesita ajustar o estandarizar los nombres de las columnas en un conjunto de datos, facilitando así análisis posteriores y asegurando la claridad y consistencia en la manipulación de los datos.\nPrimero debes escribir el nuevo nombre y luego el nombre original de la variable.\n\ndata_final |&gt;   \n  rename(zona=region)   # Renombro la columna \"region\" (nombre original) como \"zona\" (nombre nuevo)\n\n# A tibble: 181 × 5\n   country      year zona  cpi_score cpi_score2\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Denmark      2022 WE/EU        90         10\n 2 Finland      2022 WE/EU        87         13\n 3 New Zealand  2022 AP           87         13\n 4 Norway       2022 WE/EU        84         16\n 5 Singapore    2022 AP           83         17\n 6 Sweden       2022 WE/EU        83         17\n 7 Switzerland  2022 WE/EU        82         18\n 8 Netherlands  2022 WE/EU        80         20\n 9 Germany      2022 WE/EU        79         21\n10 Ireland      2022 WE/EU        77         23\n# ℹ 171 more rows\n\n\n\n\n2.2.11 Recodificar con case_when()\nLa función case_when() del paquete tidyverse en R sirve para recodificar datos y crear nuevas variables o modificar variables existentes basándose en múltiples condiciones.\nPermite evaluar varias condiciones utilizando una sintaxis similar a una instrucción “if-else”. Esta función es particularmente útil cuando necesitamos recodificar una variable en varias categorías o cuando tenemos múltiples condiciones a evaluar.\nSe coloca primero la condición (fórmula) seguido del símbolo ~ (alt+126) y la etiqueta.\nAl final se coloca TRUE, lo que indica todos aquellos casos que no cumplen con las condiciones anteriores.\n\n\nsummary(data_final$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   45.00   60.50   57.02   70.25   88.00       1 \n\n\n\ndata_final&lt;-data_final |&gt; \n            drop_na(cpi_score2)\n            summary(data_final$cpi_score2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   45.00   60.50   57.02   70.25   88.00 \n\n\nPodemos realizar:\n\ndata_final&lt;-data_final  |&gt;   \n  mutate(corrupcion=case_when(cpi_score2&lt;30~\"Bajo\", \n                              cpi_score2&lt;60~\"Medio\", \n                              cpi_score2&lt;=100~\"Alto\")) \n\nLe asignamos la configuración adecuada:\n\ndata_final$corrupcion&lt;-factor(data_final$corrupcion,\n                          levels = c(\"Bajo\", \"Medio\", \"Alto\"),\n                          ordered = TRUE)\n\nFinalmente, ya contamos una nueva nueva variable ordinal creada a partir de una variable numérica:\n\nstr(data_final$corrupcion)\n\n Ord.factor w/ 3 levels \"Bajo\"&lt;\"Medio\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Primeros pasos con tidyverse</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#valores-perdidos",
    "href": "C3_EDA2.html#valores-perdidos",
    "title": "3  Exploración avanzada",
    "section": "3.5 Valores perdidos",
    "text": "3.5 Valores perdidos\nEn la clase anterior ya te había comentado sobre la importancia de detectar los NA o también conocido como valores perdidos. Estos los podemos visualizar variable por variable utilizando la función summary() o también podemos utilizar otros paquetes como skimr:\n\n#install.packages(\"skimr\")\nlibrary(skimr)\n\nPara una rápida exploración podría utilizar la librería skimr:\n\nskim(data)\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n95\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npais\n0\n1.00\n4\n24\n0\n95\n0\n\n\ncontinente\n0\n1.00\n4\n8\n0\n5\n0\n\n\nregion\n0\n1.00\n9\n29\n0\n18\n0\n\n\ndemocracy_index_cat\n5\n0.95\n13\n16\n0\n4\n0\n\n\norganized_crime_index_cat\n0\n1.00\n4\n5\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naml_index\n16\n0.83\n5.26\n1.15\n3.00\n4.61\n5.16\n5.84\n8.14\n▂▇▇▃▂\n\n\nmatricula\n0\n1.00\n67.81\n25.85\n11.29\n43.78\n77.47\n90.06\n99.84\n▁▃▂▃▇\n\n\npbi_pc\n0\n1.00\n11823.83\n16615.00\n293.96\n1544.74\n4900.76\n15059.01\n82708.51\n▇▁▁▁▁\n\n\npobreza\n0\n1.00\n27.93\n16.84\n2.60\n15.05\n23.40\n36.80\n72.30\n▆▇▃▃▂\n\n\nurbano\n0\n1.00\n57.00\n22.28\n13.30\n41.05\n57.60\n75.90\n98.10\n▅▅▇▇▃\n\n\neducacion\n0\n1.00\n4.43\n1.36\n1.11\n3.59\n4.56\n5.26\n7.67\n▁▅▇▅▂\n\n\ncpi_index\n3\n0.97\n57.32\n17.20\n10.00\n47.75\n62.00\n70.00\n83.00\n▁▂▃▇▆\n\n\nrule_of_law\n16\n0.83\n0.55\n0.14\n0.32\n0.45\n0.50\n0.63\n0.90\n▃▇▃▂▂\n\n\ndemocracy_index\n5\n0.95\n5.59\n2.15\n0.32\n3.79\n5.94\n7.11\n9.39\n▂▅▅▇▅\n\n\norganized_crime_index\n0\n1.00\n5.35\n1.05\n2.82\n4.58\n5.35\n5.95\n7.75\n▁▅▇▅▂\n\n\n\n\n\nEn este caso también podemos visualizar los valores perdidos de nuestra base de datos a fin de poder tener una imagen de lo que nos estamos enfrentando y luego evaluar, durante el preprocesamiento, qué estrategia será la más adecuada para solucionarlos.\n\n#install.packages(\"naniar\")\nlibrary(naniar)\n\nEl gráfico vis_miss() nos muestra la presencia de valores faltantes en cada variable. Cada columna representa una variable del dataset, y cada fila corresponde a una observación. Las celdas grises indican datos presentes y las celdas negras indican datos faltantes (NA). En la parte superior aparece el porcentaje de NA en cada variable. Para interpretarlo, debemos fijarnos en: (1) qué variables tienen más valores faltantes, (2) si los NA están distribuidos aleatoriamente o concentrados en ciertos casos, y (3) qué tan grande es el porcentaje total de NA. Esta información nos ayuda a decidir en el preprocesamiento si imputamos, eliminamos casos o descartamos variables.\n\ndata |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGuíate con estos criterios. Nos importa analizar por variable.\n\nPorcentaje BAJO (&lt;5%):Puedes eliminar esas filas sin perder mucha información o imputar.\nPorcentaje MEDIO (5-20%): Conviene imputar.\nPorcentaje ALTO (&gt;30%): Esa variable podría ser candidata a eliminarse, salvo que tenga relevancia en el análisis.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#diagnóstico-final-de-nuestro-dataset",
    "href": "C3_EDA2.html#diagnóstico-final-de-nuestro-dataset",
    "title": "3  Exploración avanzada",
    "section": "3.6 Diagnóstico final de nuestro dataset",
    "text": "3.6 Diagnóstico final de nuestro dataset\nPara nuestra data podemos construir el siguiente reporte con cada uno de los pasos que dimos hasta el momento.\n\n\n\n\n\n\n\nCriterio\nDiagnóstico\n\n\n\n\nPredictoras categóricas\nSí, se identifican factores → requieren codificación\n\n\nValores perdidos\nPocos NA en general → viable imputar (mediana/moda) o eliminar casos puntuales.\n\n\nOutliers\nVariables como pbi_pc tienen valores extremos → revisar si son errores o reales; aplicar transformaciones si corresponde.\n\n\nSesgo\nDistribuciones sesgadas (ej. pbi_pc) → aplicar transformaciones, si corresponde.\n\n\nEscalas diferentes\nVariables en distintas unidades (ej. miles vs %) → aplicar transformaciones.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C3_EDA2.html#visualización-de-datos",
    "href": "C3_EDA2.html#visualización-de-datos",
    "title": "3  Exploración avanzada",
    "section": "",
    "text": "3.1.1 Retomamos nuestro dataset 📊\n\nlibrary(tidyverse)\nlibrary(readxl)\ndata &lt;- read_xlsx(\"data/AML_2.xlsx\")\n\n\n\n\n\n\n\n\nVariable\nCómo debe leerse\n\n\n\n\npais\nNombre del país. Identificación de la unidad de análisis.\n\n\ncontinente\nContinente al que pertenece el país (ej. América, Europa, Asia).\n\n\nregion\nSubregión geográfica más específica (ej. Sudamérica, África Occidental).\n\n\naml_index\nÍndice de Lavado de Activos según Basel AML Index. Valores más altos indican mayor riesgo de lavado de activos y financiamiento del terrorismo.\n\n\nmatricula\nPorcentaje de la población en edad escolar que está matriculada en el sistema educativo.\n\n\npbi_pc\nProducto Bruto Interno per cápita (ingreso promedio por persona en dólares).\n\n\npobreza\nNivel de pobreza en el país (porcentaje de la población bajo la línea de pobreza).\n\n\nurbano\nPorcentaje de la población que reside en áreas urbanas.\n\n\neducacion\nPorcentaje del PBI nacional destinado a educación.\n\n\ncpi_index\nÍndice de Percepción de la Corrupción de Transparencia Internacional. Valores más altos significan menor percepción de corrupción.\n\n\nrule_of_law\nÍndice de Estado de Derecho. Valores más altos indican mayor confianza en las leyes, justicia y cumplimiento normativo.\n\n\ndemocracy_index\nÍndice de democracia (Economist Intelligence Unit). Valores más altos indican regímenes más democráticos.\n\n\ndemocracy_index_cat\nCategoría del índice de democracia (ej. Democracia plena, Democracia defectuosa, Régimen híbrido, Régimen autoritario).\n\n\norganized_crime_index\nÍndice de criminalidad organizada. Valores más altos reflejan mayor presencia e influencia de organizaciones criminales.\n\n\norganized_crime_index_cat\nCategoría del índice de criminalidad organizada (ej. Baja, Media, Alta).\n\n\n\nCon la finalidad de que cpi_index tenga una intepretación intuitiva vamos a invertir su escala:\n\ndata &lt;- data |&gt; mutate(cpi_index=100-cpi_index)\n\n\n\n3.1.2 Anatomía de un ggplot\n\nggplot2 es un popular paquete de visualización de datos para el lenguaje de programación R, basado en los principios de la “Gramática de Gráficos”. Esta filosofía de diseño permite a los usuarios construir gráficos complejos y estéticamente agradables a partir de componentes básicos de forma intuitiva y flexible.\nEl núcleo de ggplot2 radica en su sistema de capas, donde cada gráfico se construye agregando capas que pueden incluir, entre otros, los datos, las estéticas (como color, forma y tamaño), los objetos geométricos (como puntos, líneas y barras), las escalas, y las anotaciones. Este enfoque modular no solo facilita la personalización y optimización de los gráficos sino que también promueve una estructura de código clara y comprensible.\nVamos a hacer un ejemplo paso a paso:\n\n3.1.2.1 1ra capa: Datos\nEs el conjunto de datos a visualizar.\nNuestra primera capa siempre va a ser la data. Sobre esta iniciamos la función ggplot y corroboramos que tenemos un lienzo en blanco.\n\ndata |&gt; \n  ggplot()\n\n\n\n\n\n\n\n\n\n\n3.1.2.2 2da capa: Estéticas\nEs el diseño básico del gráfico (Aesthetics).\nMapeo de variables a propiedades visuales como color, forma o tamaño, definidas con aes().\nA diferencia del lienzo en blanco, ya contamos con un diseño. En este caso, hemos indicado al R que el eje X será la variable Pobreza.\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn ggplot2, las capas de un gráfico se van adicionando secuencialmente utilizando el operador +.\n\n\n\n\n3.1.2.3 3ra capa: Geometrías (Geoms)\nSon representaciones gráficas de los datos, como puntos, líneas o barras (geom_point(), geom_line(), geom_bar(), etc.).\nEn nuestro ejemplo, podemos agregar la geometría de puntos para hacer un scatterplot o diagrama de dispersión:\n\ndata |&gt; \n  ggplot()+\n  aes(x=aml_index)+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 16 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn el paquete {ggplot2} existen 30 geometrías disponibles. Puedes ver el detalle de estos en la documentación del paquete.\n\n\nEsta estructura de capas hace que ggplot2 sea extremadamente útil para explorar y presentar datos de manera efectiva, permitiendo a los usuarios desde principiantes hasta expertos crear visualizaciones de datos complejas y personalizadas con relativa facilidad.",
    "crumbs": [
      "Análisis exploratorio de datos para ML",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploración avanzada</span>"
    ]
  },
  {
    "objectID": "C4_RL.html#métodos-supervisados",
    "href": "C4_RL.html#métodos-supervisados",
    "title": "4  Regresión Lineal",
    "section": "",
    "text": "Nota\n\n\n\n¡NUEVO TÉRMINO!\nUn parámetro es un valor numérico que forma parte del modelo matemático o estadístico y que determina cómo se comporta ese modelo. Los parámetros no los escogemos “a mano”, sino que el modelo los aprende de los datos.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "C4_RL.html#regresión-lineal-simple",
    "href": "C4_RL.html#regresión-lineal-simple",
    "title": "4  Regresión Lineal",
    "section": "4.2 Regresión Lineal Simple",
    "text": "4.2 Regresión Lineal Simple\n\n4.2.1 Recordar la ecuación de la recta\nDebemos acordarnos algunos elementos básicos que aprendimos desde la escuela:\n\\[\ny = \\beta_0 + \\beta_1x\n\\]\nDonde:\ny es la variable dependiente que se quiere predecir o estimar.\nx es la variable independiente que se utiliza para predecir y.\nβ₀ es la intersección de la línea y.\nβ₁ es la pendiente de la línea (indica cuánto varía Y por cada unidad de X).\nTener en cuenta que si:\n\nSi β₁ es positivo, Y aumenta cuando X aumenta.Es una relación directa / positiva.\nSi β₁ es negativo, Y aumenta cuando X disminuye. Es una relación inversa / negativa.\nSi β₁ es cero.Y no cambia cuando X varía. No existe relación entre las variables.\n\n\n\n4.2.2 Objetivo\nLa regresión lineal es uno de los modelos más simples y fundamentales dentro del aprendizaje supervisado. Su objetivo es predecir un valor numérico y continua a partir de una o varias variables de entrada. Se basa en la idea de que existe una relación (aproximadamente lineal) entre esas variables explicativas y la variable que queremos predecir.\nPara ello lo que hace es ajustar una línea recta (o un hiperplano, si hay varias variables) que mejor resuma la relación entre las variables de entrada (predictoras) y la variable de salida (respuesta).\n\n\n\n4.2.3 Función de costo\nLa función de costo es una fórmula matemática que mide qué tan bien (o mal) el modelo se ajusta a los datos.\n¿Qué significa?\n\nCada vez que el modelo predice un valor, podemo compararlo con el valor real.\nLa función de costo resume todos esos errores en un solo número.\nEse número nos indica la “calidad” del modelo: cuanto más pequeño sea, mejor está ajustada la recta a los datos.\n\n\\[\nJ(\\beta) = \\frac{1}{n} \\sum_{i=1}^{m} \\big(y_i - \\hat{y}_i\\big)^2\n\\]\n\n\n4.2.4 Explicación vs Predicción\nEn el campo del análisis de datos y del machine learning suele aparecer una tensión entre dos objetivos distintos: explicar fenómenos o predecir resultados futuros.\nExplicación\nEl objetivo principal es entender las relaciones entre las variables.\nSe busca interpretar los parámetros de un modelo: por ejemplo, cómo influye la educación en los ingresos, o qué efecto tiene una política pública en la reducción de la pobreza.\nLa prioridad no es tanto acertar en nuevas observaciones, sino tener coeficientes confiables y significativos que respalden hipótesis teóricas.\nSe preocupa mucho por los supuestos estadísticos, la validez de las inferencias, la significancia y la causalidad.\nEjemplo: un economista que estima un modelo para probar si la inflación depende del precio de los metales.\n\n\n\n\n\n\n\nNota\n\n\n\nUna analogía al propósito de la explicación.\nEl interés está en saber si la obra refleja fielmente la realidad, es decir, si los datos o el modelo “explican” lo que realmente ocurrió. La pintura sirve como ejemplo de representación: ¿es una descripción precisa de lo que había en la escena?\n\n\nPredicción\nEl objetivo es obtener el mayor nivel posible de acierto en datos nuevos.\nImporta más el desempeño predictivo que la interpretación de los parámetros.\nPor ello que, en este ámbito, los modelos tienen licencia para ser “cajas negras” (random forests, redes neuronales) si eso mejora la precisión.\nSe aceptan técnicas como regularización, ensambles o validación cruzada, que priorizan generalización más que interpretación.\nEjemplo: un banco que quiere predecir si un cliente dejará de pagar un crédito, sin importar tanto cuáles variables explican el fenómeno.\n\n\n\n\n\n\n\nNota\n\n\n\nUna analogía al propósito de la predicción\nA partir del cuadro, podemos reconstruir una parte que falta (¿podemos “llenar” el trozo ausente basándonos en lo que sí vemos?)\n\n\nEn estadística clásica y ciencias sociales, la tendencia ha sido hacia la explicación: probar teorías y entender causalidad.\nEn machine learning aplicado la tendencia es hacia la predicción: lograr resultados prácticos aunque el modelo no sea interpretable.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "C4_RL.html#una-regresión-paso-a-paso",
    "href": "C4_RL.html#una-regresión-paso-a-paso",
    "title": "4  Regresión Lineal",
    "section": "4.3 Una regresión paso a paso",
    "text": "4.3 Una regresión paso a paso\nEn esta sección aprenderemos, paso a paso, cómo construir un modelo de machine learning utilizando regresión lineal. Veremos desde la preparación de los datos y la definición del modelo hasta su entrenamiento, evaluación y visualización de resultados, todo dentro del flujo de trabajo de tidymodels en R.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.2     ✔ tailor       0.1.0\n✔ infer        1.0.9     ✔ tune         2.0.0\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n✔ recipes      1.3.1     ✔ yardstick    1.3.2\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\ndata &lt;- read_xlsx(\"data/AML_2.xlsx\")\n\nMira nuestra data, hemos identificado que tenemos ciertos países en los cuales NO TENEMOS la medida de aml_index, es decir, tenemos un NA.\n\naml_faltante &lt;- data |&gt; \n  select(pais, aml_index, pobreza) |&gt; \n  filter(is.na(aml_index))\n\n\naml_faltante\n\n# A tibble: 16 × 3\n   pais                     aml_index pobreza\n   &lt;chr&gt;                        &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan                     NA    54.5\n 2 Argentina                       NA    25.7\n 3 Belize                          NA    41  \n 4 Brazil                          NA     4.2\n 5 Burundi                         NA    64.6\n 6 Central African Republic        NA    62  \n 7 Comoros                         NA    44.8\n 8 Djibouti                        NA    23  \n 9 El Salvador                     NA    32.7\n10 Eritrea                         NA    50  \n11 Guyana                          NA    35  \n12 India                           NA    21.9\n13 Lesotho                         NA    57  \n14 Nepal                           NA    25.2\n15 Papua New Guinea                NA    37  \n16 Rwanda                          NA    39.1\n\n\nQueremos predecir su AML_index con la variable pobreza. Por eso vamos a utilizar la data completa en la que sí está la variable aml_index:\n\ndata&lt;- data |&gt; \n        filter(!is.na(aml_index))\n\n\n4.3.1 Paso 1: Análisis Exploratorio de Datos (EDA)\nEn este punto, utilizaremos los hallazgos detectados en las últimas dos clases.\n\nsummary(data$aml_index)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   4.605   5.160   5.261   5.835   8.140 \n\n\n\nsummary(data$pobreza)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.60   14.35   22.20   25.77   31.75   72.30 \n\n\n\n\n4.3.2 Paso 1: Splitear la data\nDividir los datos en training y testing es un paso fundamental en machine learning. La idea es entrenar el modelo con una parte de la información y reservar otra parte, nunca vista por el modelo, para evaluar su capacidad de generalizar. Esto evita el sesgo de pensar que un modelo es “bueno” solo porque se ajusta bien a los datos con los que fue entrenado.\n¿Por qué se hace el split?\n🔹 Entrenamiento: el modelo aprende los patrones usando solo la porción de training.\n🔹 Evaluación: el conjunto de test sirve para medir el poder predictivo en datos nuevos.\n🔹 Prevención de sobreajuste (overfitting): si el modelo se ajusta demasiado a training, su desempeño en test revelará esa debilidad.\n🔹 Realismo: simula lo que pasa en la práctica, cuando usamos el modelo para predecir casos que nunca había visto.\n🔹 Comparación: permite elegir entre varios modelos el que realmente generaliza mejor.\n\n# Dividimos nuestro dataset en dos partes: training y testing\nset.seed(2025)\nindex &lt;- initial_split(data)     \n# Crea un objeto que contiene la \"partición\" de los datos.\n# Por defecto, 75% de las filas se van al training y 25% al testing.\n# (Se puede ajustar con el argumento prop = 0.8, por ejemplo).\n\ntraining_data &lt;- training(index)  \n# Extrae del split anterior la parte de entrenamiento,\n# es decir, el subconjunto de datos que usaremos para\n# ajustar (entrenar) nuestro modelo.\n\ntesting_data &lt;- testing(index)    \n# Extrae del split anterior la parte de prueba,\n# es decir, el subconjunto de datos que NO verá el modelo\n# durante el entrenamiento y que servirá para evaluar\n# su capacidad de generalizar a datos nuevos.\n\nCómo vemos las particiones creadas?\n\ndim(training_data)\n\n[1] 59 15\n\n\n\ndim(testing_data)\n\n[1] 20 15\n\n\n\n\n4.3.3 Paso 3: Preprocesamiento de datos (Feature Engineering)\nEl paquete recipes de tidymodels permite definir de manera ordenada y reproducible los pasos de preprocesamiento de los datos antes de entrenar un modelo de machine learning.\nPrimero se define la receta principal de tu modelo, que identifica la variable predicha y las predictoras, similar a una ecuación. En este casos utilizamos esta fórmula:\n\\[\nVariablePredicha \\sim Predictor\n\\]\nLuego, si lo deseamos, podemos especificar qué transformaciones se aplicarán a las variables: desde tareas sencillas como eliminar valores perdidos o normalizar predictores, hasta imputaciones, creación de variables dummy o reducción de dimensionalidad. Cada transformación se añade como un step, y el flujo se encarga de aprender sus parámetros a partir del conjunto de entrenamiento y aplicarlos también al conjunto de prueba, evitando fugas de información (data leakage). De esta forma, recipes ofrece un marco flexible y seguro para preparar los datos de forma consistente en todo el proceso de modelado.\nPor el momento, vamos a definir nuestra receta:\n\nmi_receta &lt;- recipe(aml_index ~ pobreza, data = training_data)\n\nLuego de crear la receta, la podemos solicitar para ver qué es lo esta considerando:\n\nmi_receta\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n\n\n\nNota\n\n\n\nMás adelante, en este paso también vamos a poder realizar algunas transformaciones dentro de nuestras variables. Por ejemplo:\n\nstep_naomit() → elimina filas con valores perdidos.\nstep_meanimpute() / step_medianimpute() → imputan NA con la media o mediana.\nstep_modeimpute() → imputación de NA con la moda en variables categóricas.\nstep_dummy() → convierte variables categóricas en variables dummy (0/1).\nstep_normalize() → estandariza predictores (media = 0, sd = 1).\nEntre otros.\n\n\n\n\n\n4.3.4 Paso 4: Seleccionamos el modelo\nEn la fase de modelamiento, el primer paso es definir el modelo que utilizaremos. En este caso empleamos linear_reg(), que especifica una regresión lineal, y le asignamos un motor de cálculo mediante set_engine(“lm”).\n\nmi_modelo_lm &lt;- linear_reg() |&gt; \n                   set_engine(\"lm\")\n\n\n\n4.3.5 Paso 5: Entrenamos el modelo\nA continuación, iniciamos un workflow(), que es una estructura de tidymodels diseñada para integrar en un solo flujo el preprocesamiento de datos (receta) y el entrenamiento. Esto garantiza que todo el proceso se ejecute de manera ordenada, reproducible y sin fugas de información. Añadimos al workflow tanto la receta definida en la etapa de preprocesamiento como el modelo lineal.\n\nflujo_ml&lt;-workflow() |&gt; \n            add_recipe(mi_receta) |&gt; \n             add_model(mi_modelo_lm)\nflujo_ml\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nFinalmente, y entrenamos este flujo con los datos de entrenamiento. De esta manera, se obtiene un objeto ajustado que estará listo para realizar predicciones y ser evaluado en el conjunto de prueba.\n\nmodelo_entrenado &lt;- flujo_ml %&gt;% \n                      fit(data = training_data) # Con el de ENTRENAMIENTO!\n\nSi deseamos ver los coeficientes (estimates) del modelo podemos solicitarlo con tidy():\n\ntidy(modelo_entrenado)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.27     0.221       19.3  1.04e-26\n2 pobreza       0.0405   0.00770      5.26 2.23e- 6\n\n\nEntonces, en este caso el modelo para la predicción del AML_index sería el siguiente:\n\\[\nAML = 4.27 + 0.04 * POBREZA\n\\]\n\n\n4.3.6 Paso 6: Evaluamos el modelo\nUna vez que el modelo ha sido entrenado, el siguiente paso es evaluar su desempeño. La evaluación consiste en medir qué tan bien el modelo logra predecir los valores de la variable de interés, comparando las predicciones con los valores reales. Para ello se utilizan métricas de error, como el RMSE (Root Mean Squared Error), que nos permiten cuantificar la calidad del ajuste y, sobre todo, estimar su capacidad de generalización cuando se aplica a nuevos datos.\nPara ello, primero utilizamos el modelo generado para predecir con la nueva data:\n\nprediccion_test&lt;-modelo_entrenado |&gt; \n                  predict(testing_data) |&gt; \n                  bind_cols(valor_real=testing_data$aml_index)\nprediccion_test\n\n# A tibble: 20 × 2\n   .pred valor_real\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  4.85       4.75\n 2  4.89       4.13\n 3  4.76       5.89\n 4  5.49       6.75\n 5  5.51       5.21\n 6  5.13       3   \n 7  6.18       7.69\n 8  4.85       4.9 \n 9  6.47       7.17\n10  5.17       3.47\n11  7.14       7.43\n12  6.33       5.63\n13  6.15       5.21\n14  5.19       4.81\n15  5.87       5.23\n16  4.84       3.57\n17  5.13       3.96\n18  4.54       4.05\n19  6.51       6.95\n20  4.43       5.08\n\n\nAhora vamos a medir cómo funciona nuestro modelo utilizándolo con data de testeo. Recuerda que en nuestra data de testeo podemos validar en contraste con el valor real.\n\nyardstick::rmse(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.990\n\n\nEl R cuadrado es otra medida, aunque menos utilizada en machine learning, que dice cuánto es explicado por nuestro modelo.\n\nrsq(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.512\n\n\nGraficamos:\n\nprediccion_test |&gt; \n  ggplot()+\n  aes(x = valor_real, y = .pred)+\n  geom_point(color = \"blue\", size = 2) +\n  labs(\n    x = \"Valor real\",\n    y = \"Valor predicho\",\n    title = \"Valores reales vs predicciones\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPaso 7: Colocamos el modelo en operación\nSi lo que queremos es identificar con este modelo el aml_index perdido de la data inicial, cuál sería nuestra predicción?\n\nmodelo_entrenado |&gt; \n                  predict(aml_faltante) |&gt; \n                  bind_cols(aml_faltante$pais)\n\nNew names:\n• `` -&gt; `...2`\n\n\n# A tibble: 16 × 2\n   .pred ...2                    \n   &lt;dbl&gt; &lt;chr&gt;                   \n 1  6.48 Afghanistan             \n 2  5.32 Argentina               \n 3  5.94 Belize                  \n 4  4.44 Brazil                  \n 5  6.89 Burundi                 \n 6  6.79 Central African Republic\n 7  6.09 Comoros                 \n 8  5.21 Djibouti                \n 9  5.60 El Salvador             \n10  6.30 Eritrea                 \n11  5.69 Guyana                  \n12  5.16 India                   \n13  6.58 Lesotho                 \n14  5.30 Nepal                   \n15  5.77 Papua New Guinea        \n16  5.86 Rwanda",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html",
    "href": "C5_RL2.html",
    "title": "5  Preprocesamiento y validación cruzada",
    "section": "",
    "text": "5.1 Punto de partida\nHabíamos establecido en la clase anterior que queríamos utilizar nuestra data para construir buenos modelos predictivos y para eso utilizamos la regresión lineal.\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_ix_i\n\\]\nTambién, acuérdate que utilizamos una función de costo principalmente para evaluar la predicción, la cual era el RMSE (raíz del error cuadrático medio):\n\\[\nJ(\\beta) \\;=\\; \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i - \\hat{y}_i\\big)^2}\n\\]\nAsimismo, lo que hicimos en el ejercicio fue construir modelos con Corrupción (cpi_index), el nivel de pobreza (pobreza) y el PBI per cápita (pbi_pc)\nVamos a expandir nuestor uso de tidymodels.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preprocesamiento y validación cruzada</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#preprocesamiento-con-recipe",
    "href": "C5_RL2.html#preprocesamiento-con-recipe",
    "title": "5  Preprocesamiento y validación cruzada",
    "section": "5.2 Preprocesamiento con recipe",
    "text": "5.2 Preprocesamiento con recipe\n\nComo vimos en la clase pasada, con recipe no solo definimos la variable objetivo y los predictores; también podemos encadenar, de forma ordenada y reproducible, los pasos de preprocesamiento que aplicaremos al conjunto de datos (imputación, codificación, escalado, etc.).\nVamos a revisar los más recurrente.\n\n5.2.1 step_impute_mean(): Imputación de NA\n\nUtilizamos nuestra base de datos:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tidymodels)\ndata &lt;- read_xlsx(\"data/AML_2.xlsx\")\ndata&lt;- data |&gt; \n        filter(!is.na(aml_index))\n\nImagina que ahora deseo utilizar rule_of_law como un predictor pero me doy cuenta que tiene valores perdidos.\n\nsummary(data$rule_of_law)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.3458  0.4432  0.5147  0.5549  0.6647  0.8995       9 \n\n\nAntes ya habíamos identificado que efectivamente es posible imputar debido a que el número de perdido no es significativo.\n\nlibrary(naniar)\ndata |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nLa imputación de datos perdidos es el proceso de reemplazar valores faltantes (NA) por valores plausibles para poder analizar y modelar sin descartar observaciones. Busca preservar el tamaño muestral, reducir sesgos y permitir que los algoritmos funcionen (muchos no aceptan NA).\n\n\nLa imputación debe aplicarse siempre, SIEMPRE, sobre la data de entrenamiento.\n\nset.seed(2025)\nindex &lt;- initial_split(data)     \ntraining_data &lt;- training(index)  \ntesting_data &lt;- testing(index)    \n\nEntonces:\n\nmi_receta &lt;- recipe(aml_index ~ rule_of_law, data = training_data) |&gt; \n              step_impute_mean(rule_of_law)\n\n¿Qué es lo que está haciendo esta función? step_impute_mean() imputa los valores faltantes de variables numéricas con la media calculada en el conjunto de entrenamiento. Es decir, con este paso, nos va a permitir utilizar TODOS LOS CASOS, los cual al inicio tenía perdidos. Este es uno de los métodos de imputación más simples.\nAhora bien, antes de seguir, si quieres ver qué hace este step a nuestra data, podemos inspeccionarlo preparando la receta solo con el conjunto de entrenamiento (ahí la receta “aprende” parámetros como las medias) y luego mirar el entrenamiento ya transformado con juice() y el conjunto de prueba horneado con bake().\nEn la jerga de tidymodels, hornear significa aplicar a datos nuevos (p. ej., el test) exactamente las mismas transformaciones que se estimaron con el train —imputar con las medias del train, escalar con sus desvíos, crear las mismas dummies, etc.— sin recalcular nada y sin modificar tu objeto original; bake() simplemente devuelve un tibble ya transformado, listo para predecir.\nEsto aplica tanto en nuestro training data:\n\ntraining_data |&gt; \n  select(rule_of_law)\n\n# A tibble: 59 × 1\n   rule_of_law\n         &lt;dbl&gt;\n 1      NA    \n 2       0.703\n 3       0.534\n 4       0.834\n 5       0.569\n 6      NA    \n 7       0.630\n 8       0.556\n 9       0.800\n10       0.549\n# ℹ 49 more rows\n\n\nQue ahora se vería así:\n\nmi_receta |&gt; \n     prep() |&gt; # es cuando la receta aprende con el train\n     juice()   # devuelve el train ya transformado según lo aprendido en prep\n\n# A tibble: 59 × 2\n   rule_of_law aml_index\n         &lt;dbl&gt;     &lt;dbl&gt;\n 1       0.557      8.14\n 2       0.703      4.3 \n 3       0.534      4.71\n 4       0.834      4.29\n 5       0.569      5.85\n 6       0.557      4.7 \n 7       0.630      4.9 \n 8       0.556      5.16\n 9       0.800      4.1 \n10       0.549      5.29\n# ℹ 49 more rows\n\n\nComo en nuestro testing data, que antes se veía así:\n\ntesting_data |&gt; \n  select(rule_of_law)\n\n# A tibble: 20 × 1\n   rule_of_law\n         &lt;dbl&gt;\n 1       0.484\n 2       0.784\n 3      NA    \n 4       0.354\n 5       0.488\n 6       0.817\n 7       0.413\n 8       0.548\n 9       0.437\n10       0.768\n11       0.430\n12       0.522\n13       0.417\n14       0.486\n15      NA    \n16       0.688\n17       0.721\n18      NA    \n19       0.454\n20       0.485\n\n\nPero luego de aplicar la receta vamos a ver:\n\nmi_receta |&gt; \n      prep() |&gt; # es cuando la receta aprende con el train\n      bake(new_data=testing_data) # devuelve el test ya transformado según lo aprendido en prep\n\n# A tibble: 20 × 2\n   rule_of_law aml_index\n         &lt;dbl&gt;     &lt;dbl&gt;\n 1       0.484      4.75\n 2       0.784      4.13\n 3       0.557      5.89\n 4       0.354      6.75\n 5       0.488      5.21\n 6       0.817      3   \n 7       0.413      7.69\n 8       0.548      4.9 \n 9       0.437      7.17\n10       0.768      3.47\n11       0.430      7.43\n12       0.522      5.63\n13       0.417      5.21\n14       0.486      4.81\n15       0.557      5.23\n16       0.688      3.57\n17       0.721      3.96\n18       0.557      4.05\n19       0.454      6.95\n20       0.485      5.08\n\n\n\n\n5.2.2 step_normalize(): Normalizar variables\nDefinición\nEstandariza variables numéricas mediante z-score: a cada columna seleccionada le resta su media y la divide por su desviación estándar.\n\nCuándo usarlo:\n\nModelos sensibles a la escala o basados en distancia: k-NN, SVM, regresiones penalizadas (glmnet: Lasso/Ridge/Elastic Net), redes neuronales.\nNo es necesario para árboles, random forest o boosting basados en árboles.\n\nRecuerda que debes hacer esto a las variables numéricas.\nDespués de normalizar verás:\n\nLos números cambian de escala: cada predictor numérico queda con media ≈ 0 y desviación estándar ≈ 1. Verás muchos valores negativos (por debajo del promedio) y positivos (por encima).\nLas unidades desaparecen: ya no están en soles, años o %; ahora todos están en “desviaciones estándar”, por eso son comparables entre sí.\nLas formas no se rompen: el orden de los casos no cambia y las correlaciones entre variables se conservan; solo cambia la escala.\nEn tablas/gráficos: histogramas centrados en 0; resúmenes tipo mean ~ 0 y sd ~ 1. Si haces scatterplots, la nube luce igual, pero con ejes reescalados.\nEn el modelo: algoritmos sensibles a escala (k-NN, SVM, Lasso/Ridge/Elastic Net, PCA, k-means) se vuelven más estables y los coeficientes en modelos lineales son más comparables en magnitud.\n\nEjemplo simple\nVeamos lo que hacemos con un ejemplo:\n\ndf2 &lt;- tibble(\n  edad   = c(22, 25, 28, 30, 34, 37, 41, 45, 52, 58),\n  sueldo = c(1200, 1350, 1500, 1650, 2100, 2400, 3000, 3500, 4200, 5000)\n)\n\nNormalizar implica:\n\ndf2 &lt;- df2 |&gt; \n  mutate(edad_normalizado=(edad-mean(edad))/sd(edad), \n         sueldo_normalizado=(sueldo-mean(sueldo))/sd(sueldo))\ndf2\n\n# A tibble: 10 × 4\n    edad sueldo edad_normalizado sueldo_normalizado\n   &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n 1    22   1200          -1.29               -1.07 \n 2    25   1350          -1.03               -0.954\n 3    28   1500          -0.780              -0.838\n 4    30   1650          -0.610              -0.723\n 5    34   2100          -0.271              -0.377\n 6    37   2400          -0.0169             -0.146\n 7    41   3000           0.322               0.315\n 8    45   3500           0.661               0.700\n 9    52   4200           1.25                1.24 \n10    58   5000           1.76                1.85 \n\n\nEntonces, ahora la escala cambia y son comparables:\n\nsummary(df2$edad_normalizado)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.2879 -0.7372 -0.1440  0.0000  0.5762  1.7624 \n\n\n\nsummary(df2$sueldo_normalizado)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.0691 -0.8095 -0.2615  0.0000  0.6038  1.8536 \n\n\nAplicación con tidymodels\nEsto que hemos hecho manualmente, también lo podemos hacer con tidymodels y la función step_normalize(). Piensa en las variables rule_of_law y cpi_index.\n\nmi_receta &lt;- recipe(aml_index ~ rule_of_law + pbi_pc, data = training_data) |&gt; \n              step_impute_mean(rule_of_law) |&gt; \n              step_normalize(pbi_pc)\n\nVemos nuestro dataset de entrenamiento:\n\nmi_receta |&gt; \n     prep() |&gt; # es cuando la receta aprende con el train\n     juice()   # devuelve el train ya transformado según lo aprendido en prep\n\n# A tibble: 59 × 3\n   rule_of_law  pbi_pc aml_index\n         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1       0.557 -0.746       8.14\n 2       0.703  2.95        4.3 \n 3       0.534 -0.208       4.71\n 4       0.834  2.04        4.29\n 5       0.569 -0.411       5.85\n 6       0.557 -0.417       4.7 \n 7       0.630 -0.0594      4.9 \n 8       0.556 -0.231       5.16\n 9       0.800  2.26        4.1 \n10       0.549 -0.659       5.29\n# ℹ 49 more rows\n\n\nY nuestor dataset de test:\n\nmi_receta |&gt; \n      prep() |&gt; # es cuando la receta aprende con el train\n      bake(new_data=testing_data) # devuelve el test ya transformado según lo aprendido en prep\n\n# A tibble: 20 × 3\n   rule_of_law pbi_pc aml_index\n         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1       0.484 -0.479      4.75\n 2       0.784  2.02       4.13\n 3       0.557 -0.580      5.89\n 4       0.354 -0.699      6.75\n 5       0.488 -0.335      5.21\n 6       0.817  0.593      3   \n 7       0.413 -0.734      7.69\n 8       0.548 -0.538      4.9 \n 9       0.437 -0.764      7.17\n10       0.768  0.345      3.47\n11       0.430 -0.758      7.43\n12       0.522 -0.766      5.63\n13       0.417 -0.213      5.21\n14       0.486 -0.377      4.81\n15       0.557  0.184      5.23\n16       0.688  0.757      3.57\n17       0.721  1.02       3.96\n18       0.557  4.13       4.05\n19       0.454 -0.751      6.95\n20       0.485 -0.614      5.08\n\n\n\n\n5.2.3 step_dummy(): Predictoras categóricas\nDefinición\nPara usar variables categóricas en muchos algoritmos de aprendizaje (por ejemplo, regresiones penalizadas como glmnet, SVM, redes, XGBoost), primero hay que convertirlas a variables numéricas. La forma estándar es el one-hot encoding: cada categoría de una variable se transforma en una columna binaria que vale 1 si el registro pertenece a esa categoría y 0 si no.\n\nPor ejemplo, si color tiene valores (rojo, azul, verde), se crean columnas como color_rojo, color_azul y color_verde. Esto permite que el modelo “entienda” información categórica sin asignar números arbitrarios que introducirían un orden falso.\nEjemplo simple\nHagamos un ejemplo básico:\n\ndf &lt;- tibble(\n  id     = 1:5,\n  edad   = c(15,21,30,57,62),\n  sector = c(\"publico\", \"privado\", \"publico\", \"privado\", \"privado\"))\ndf\n\n# A tibble: 5 × 3\n     id  edad sector \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n1     1    15 publico\n2     2    21 privado\n3     3    30 publico\n4     4    57 privado\n5     5    62 privado\n\n\nConvertir a dummy, significa que vamos a:\n\ndf |&gt; \n  mutate(sector_public=ifelse(sector==\"publico\", 1, 0),\n         sector_privado=ifelse(sector==\"privado\", 1, 0))\n\n# A tibble: 5 × 5\n     id  edad sector  sector_public sector_privado\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1     1    15 publico             1              0\n2     2    21 privado             0              1\n3     3    30 publico             1              0\n4     4    57 privado             0              1\n5     5    62 privado             0              1\n\n\nComo ves en este ejemplo, a la derecha hemos creado dos nuevas variables “sector_publico” que coloca un “1” cuando efectivamente el caso pertenece a ese sector y “sector_privado” cuando ocurre lo propio. En caso no se cumpla la condición, existirá un cero.\nAplicación con tidymodels\nAhora bien, podemos hacer esta transformación de una forma más sencilla utilizando recipes.\n\nmi_receta &lt;- recipe(aml_index ~ rule_of_law + pbi_pc + continente, data = training_data) |&gt; \n              step_impute_mean(rule_of_law) |&gt; \n              step_normalize(pbi_pc, rule_of_law) |&gt; \n              step_dummy(continente)\n\nAhora vemos cómo se vería nuestra data de entrenamiento.\n\nmi_receta |&gt; \n     prep() |&gt; # es cuando la receta aprende con el train\n     juice()\n\n# A tibble: 59 × 7\n   rule_of_law  pbi_pc aml_index continente_Americas continente_Asia\n         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;\n 1     0       -0.746       8.14                   0               0\n 2     1.10     2.95        4.3                    1               0\n 3    -0.179   -0.208       4.71                   0               1\n 4     2.08     2.04        4.29                   0               0\n 5     0.0895  -0.411       5.85                   0               0\n 6     0       -0.417       4.7                    0               0\n 7     0.549   -0.0594      4.9                    0               0\n 8    -0.00893 -0.231       5.16                   0               0\n 9     1.82     2.26        4.1                    0               0\n10    -0.0636  -0.659       5.29                   0               0\n# ℹ 49 more rows\n# ℹ 2 more variables: continente_Europe &lt;dbl&gt;, continente_Oceania &lt;dbl&gt;\n\n\nY nuestra data de testing:\n\nmi_receta |&gt; \n      prep() |&gt; # es cuando la receta aprende con el train\n      bake(new_data=testing_data) # devuelve el test ya transformado según lo aprendido en prep\n\n# A tibble: 20 × 7\n   rule_of_law pbi_pc aml_index continente_Americas continente_Asia\n         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;\n 1     -0.551  -0.479      4.75                   0               0\n 2      1.70    2.02       4.13                   0               0\n 3      0      -0.580      5.89                   0               1\n 4     -1.53   -0.699      6.75                   0               0\n 5     -0.522  -0.335      5.21                   1               0\n 6      1.95    0.593      3                      0               0\n 7     -1.09   -0.734      7.69                   0               0\n 8     -0.0721 -0.538      4.9                    0               1\n 9     -0.904  -0.764      7.17                   0               0\n10      1.58    0.345      3.47                   0               0\n11     -0.959  -0.758      7.43                   0               0\n12     -0.268  -0.766      5.63                   0               0\n13     -1.05   -0.213      5.21                   1               0\n14     -0.535  -0.377      4.81                   1               0\n15      0       0.184      5.23                   0               0\n16      0.979   0.757      3.57                   0               0\n17      1.23    1.02       3.96                   0               0\n18      0       4.13       4.05                   0               0\n19     -0.779  -0.751      6.95                   0               0\n20     -0.542  -0.614      5.08                   0               0\n# ℹ 2 more variables: continente_Europe &lt;dbl&gt;, continente_Oceania &lt;dbl&gt;",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preprocesamiento y validación cruzada</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#probando-la-nueva-receta-con-lm",
    "href": "C5_RL2.html#probando-la-nueva-receta-con-lm",
    "title": "5  RL con preprocesamiento y Random Forest",
    "section": "5.3 Probando la nueva receta con lm",
    "text": "5.3 Probando la nueva receta con lm\nRecapitulamos la receta que tenemos:\n\nmi_receta\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: rule_of_law\n\n\n• Centering and scaling for: pbi_pc rule_of_law\n\n\n• Dummy variables from: continente\n\n\n\nmi_modelo_lm &lt;- linear_reg() |&gt; \n                   set_engine(\"lm\")\n\nflujo_ml&lt;-workflow() |&gt; \n            add_recipe(mi_receta) |&gt; \n             add_model(mi_modelo_lm)\n\nmodelo_entrenado &lt;- flujo_ml %&gt;% \n                      fit(data = training_data) # Con el de ENTRENAMIENTO!\n\nSi deseamos ver los coeficientes (estimates) del modelo podemos solicitarlo con tidy():\n\ntidy(modelo_entrenado)\n\n# A tibble: 7 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           6.12       0.172    35.7   3.20e-38\n2 rule_of_law          -0.441      0.154    -2.87  5.88e- 3\n3 pbi_pc               -0.0921     0.159    -0.580 5.65e- 1\n4 continente_Americas  -1.14       0.242    -4.70  1.95e- 5\n5 continente_Asia      -1.00       0.235    -4.26  8.58e- 5\n6 continente_Europe    -1.31       0.274    -4.79  1.45e- 5\n7 continente_Oceania   -0.600      0.460    -1.30  1.98e- 1\n\n\nQué puedes notar en los coeficientes?",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>RL con preprocesamiento y Random Forest</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#evaluamos-el-modelo-con-test",
    "href": "C5_RL2.html#evaluamos-el-modelo-con-test",
    "title": "5  RL con preprocesamiento y Random Forest",
    "section": "5.4 Evaluamos el modelo con test",
    "text": "5.4 Evaluamos el modelo con test\nUna vez que el modelo ha sido entrenado, el siguiente paso es evaluar su desempeño. La evaluación consiste en medir qué tan bien el modelo logra predecir los valores de la variable de interés, comparando las predicciones con los valores reales. Para ello se utilizan métricas de error, como el RMSE (Root Mean Squared Error), que nos permiten cuantificar la calidad del ajuste y, sobre todo, estimar su capacidad de generalización cuando se aplica a nuevos datos.\nPara ello, primero utilizamos el modelo generado para predecir con la nueva data:\n\nprediccion_test&lt;-modelo_entrenado |&gt; \n                  predict(testing_data) |&gt; \n                  bind_cols(valor_real=testing_data$aml_index)\nprediccion_test\n\n# A tibble: 20 × 2\n   .pred valor_real\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  5.10       4.75\n 2  3.87       4.13\n 3  5.18       5.89\n 4  6.86       6.75\n 5  5.24       5.21\n 6  3.90       3   \n 7  6.67       7.69\n 8  5.20       4.9 \n 9  6.59       7.17\n10  4.08       3.47\n11  6.62       7.43\n12  6.31       5.63\n13  5.47       5.21\n14  5.25       4.81\n15  6.11       5.23\n16  4.31       3.57\n17  4.17       3.96\n18  4.43       4.05\n19  6.54       6.95\n20  5.11       5.08\n\n\nAhora vamos a medir cómo funciona nuestro modelo utilizándolo con data de testeo. Recuerda que en nuestra data de testeo podemos validar en contraste con el valor real.\n\nyardstick::rmse(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.567\n\n\nEl R cuadrado es otra medida, aunque menos utilizada en machine learning, que dice cuánto es explicado por nuestro modelo.\n\nrsq(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.867\n\n\nGraficamos:\n\nprediccion_test |&gt; \n  ggplot()+\n  aes(x = valor_real, y = .pred)+\n  geom_point(color = \"blue\", size = 2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +  # línea 1:1\n  labs(\n    x = \"Valor real\",\n    y = \"Valor predicho\",\n    title = \"Valores reales vs predicciones\"\n  ) +\n  xlim(0,10)+ ylim(0,10)+\n  theme_minimal()",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>RL con preprocesamiento y Random Forest</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#section",
    "href": "C5_RL2.html#section",
    "title": "5  Más tidymodels",
    "section": "5.5 ",
    "text": "5.5",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Más tidymodels</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#remuestreo-para-evaluar-el-rendimiento-cross-validation",
    "href": "C5_RL2.html#remuestreo-para-evaluar-el-rendimiento-cross-validation",
    "title": "5  Preprocesamiento y validación cruzada",
    "section": "5.4 Remuestreo para evaluar el rendimiento: cross-validation",
    "text": "5.4 Remuestreo para evaluar el rendimiento: cross-validation\nHasta aquí te había comentado que era necesario separar nuestra data en data de entrenamiento (train) y data de evaluación (test).\n\nSin embargo, el depender de un único split (partición) es un riesgo para nosotros. Si te pones a pensar es como evaluar a un deportista con una sóla carrera. En otras palabras, la “nota” del modelo queda a merced del azar del muestreo. Por eso, se establece que antes de irnos a evaluar al modelo con el testing dataset debe pasar por un proceso de validación.\nEste proceso de validación requeriría hacer un split adicional. Normalmente, en un escenario óptimo, vamos a tener la suficiente cantidad de información para crear una partición más.\n\nSin embargo, lo que usualmente ocurre es que no se cuenta con la suficiente cantidad de información. Esto nos deja una salida: usar una técnica que se llama cross-validation o validación cruzada.\n\nLa idea detrás del remuestreo es usar tu muestra disponible como si fuera el universo para simular muchos “nuevos” conjuntos de datos y así estimar con mayor fiabilidad cómo rendiría un modelo o un estimador fuera de esa muestra. En vez de depender de un único corte train/test (muy sensible al azar), divides o vuelves a muestrear tus datos muchas veces, entrenas y evalúas repetidamente, y promedias los resultados.\nPrimero debemos crear nuestras particiones utilizando nuestro training data. En este caso vamos a comenzar utilizando sólo cuatro particiones o folds.\n\nset.seed(2025)\nfolds &lt;- vfold_cv(training_data, v= 4)\n\nSi entramos a nuestro primer fold, vemos que hemos divido la data de entrenamiento (59) en dos partes. Una primera que fungirá como data para analizar nuestro modelo (aquí para evitar la confusión con el término entrenamiento, se utiliza la palabra analysis) y un fold de 15 casos para el assess.\n\nfolds$splits[[1]]\n\n&lt;Analysis/Assess/Total&gt;\n&lt;44/15/59&gt;\n\n\nPara correr nuevamente nuestro modelo utilizando cross-validation necesitamos seguir los mismos pasos del workflow, pero utilizando el fit en las nuevas particiones:\n\nmi_modelo_lm &lt;- linear_reg() |&gt; \n                   set_engine(\"lm\")\n\nflujo_ml&lt;-workflow() |&gt; \n            add_recipe(mi_receta) |&gt; \n             add_model(mi_modelo_lm)\n\nmodelos_entrenados_cv &lt;- flujo_ml %&gt;% \n                          fit_resamples(resamples = folds) #DIFERENTE\n\n→ A | warning: ! There are new levels in `continente`: \"Oceania\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\nAhora, podemos solicitar las métricas para cada uno de los folds creado:\n\nmodelos_entrenados_cv |&gt; \n  collect_metrics(summarize = FALSE)\n\n# A tibble: 8 × 5\n  id    .metric .estimator .estimate .config        \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Fold1 rmse    standard       0.682 pre0_mod0_post0\n2 Fold1 rsq     standard       0.756 pre0_mod0_post0\n3 Fold2 rmse    standard       0.430 pre0_mod0_post0\n4 Fold2 rsq     standard       0.811 pre0_mod0_post0\n5 Fold3 rmse    standard       0.656 pre0_mod0_post0\n6 Fold3 rsq     standard       0.803 pre0_mod0_post0\n7 Fold4 rmse    standard       0.637 pre0_mod0_post0\n8 Fold4 rsq     standard       0.589 pre0_mod0_post0\n\n\nFinalmente, solicitamos el promedio:\n\nmodelos_entrenados_cv |&gt; \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   0.601     4  0.0577 pre0_mod0_post0\n2 rsq     standard   0.740     4  0.0518 pre0_mod0_post0\n\n\nEste fit_samples se recomienda hacerlo antes del fit con la data de testeo. De tal forma que antes de la evaluación, ya sabes que tu modelo puede funcionar con distintos subdataset creados a partir de tu data de entrenamiento original.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preprocesamiento y validación cruzada</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#árboles-de-decisión",
    "href": "C5_RL2.html#árboles-de-decisión",
    "title": "5  RL con preprocesamiento y modelos basados en árboles",
    "section": "5.4 Árboles de decisión",
    "text": "5.4 Árboles de decisión\n\n5.4.1 Definición\nUn árbol de decisión es un modelo predictivo que representa un conjunto de reglas “si… entonces…” organizadas en forma de árbol. Parte de un nodo raíz (todos los datos) y, en cada nodo interno, divide los datos según el valor de una variable predictora para reducir la impureza (en regresión: varianza/MSE).\n\nEl proceso continúa hasta cumplir criterios de parada (profundidad, mínimo de casos, complejidad) y las hojas entregan la predicción final: promedio de la variable objetivo (regresión).\nTambién se puede aplicar a clasificación. Eso lo veremos más adelante!\n\n\n5.4.2 Ventajas y desventajas\nVentajas:\n\nInterpretables: reglas “si… entonces…” fáciles de explicar.\nCapturan no linealidades e interacciones sin ingeniería previa.\nPoco preprocesamiento: no requieren normalizar; toleran outliers mejor que modelos lineales.\nMixtos: funcionan con variables numéricas y categóricas.\nRápidos de entrenar y útiles para generar hipótesis/segmentaciones.\n\nDesventajas:\n\nSobreajustan e inestables: pequeños cambios en los datos alteran mucho el árbol (alta varianza).\nFronteras en “escalera” y predicciones por tramos; no extrapolan bien.\nSesgo en splits hacia variables con muchos niveles; cuidado al interpretar “importancia”.\n\n\n\n5.4.3 Ejemplo simple de partición\nImagina que tenemos la siguiente data:\n\ndf &lt;- tibble::tibble(\n  id             = 1:10,\n  horas_estudio  = c(1, 2, 2, 3, 3, 5, 4, 6, 2, 5),\n  ingreso           = c(22, 35, 33, 40, 45, 35, 40, 21, 28, 24)\n) \n\nLo visualizamos en un gráfico de dispersión.\n\ndf |&gt; \n  ggplot()+\n  aes(x=horas_estudio, y=ingreso)+\n  geom_point()\n\n\n\n\n\n\n\n\nGeneramos una primera partición:\n\ndf |&gt; \n  ggplot()+\n  aes(x=horas_estudio, y=ingreso)+\n  geom_point()+\n  geom_vline(xintercept = 1.5, linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\nDentro de cada una de estas secciones vamos a calcular un indicador, el cual es el RMSE a la media de cada uno.\nPara el caso de la primera partición, como el error es 0, al tener sólo una observación, su RMSE será 0.\nPara el caso de la segunda partición, calculamos los errores, los elevamos al cuadrado, los sumamos y sacamos promedio:\n\n\ndf |&gt; \n  filter(horas_estudio&gt;=1.5) |&gt; \n  summarise(mean(ingreso))\n\n# A tibble: 1 × 1\n  `mean(ingreso)`\n            &lt;dbl&gt;\n1            33.4\n\n\n\ndf |&gt; \n  filter(horas_estudio&gt;=1.5) |&gt; \n  mutate(error_ingreso=ingreso-mean(ingreso)) |&gt;  # calculamos el error\n  mutate(error_ingreso_al2=error_ingreso**2) |&gt; \n  summarise(RMSE=sum(error_ingreso_al2)/n()) |&gt; \n  mutate(RMSE*0.9)\n\n# A tibble: 1 × 2\n   RMSE `RMSE * 0.9`\n  &lt;dbl&gt;        &lt;dbl&gt;\n1  55.4         49.8\n\n\n\nAhora, conociendo el grado de dispersión de cada una de las agrupaciones, podemos encontrar un puntaje para el umbral seleccionado.\nEste umbral tendrá un puntaje que será la suma de la dispersión de cada sección. En este caso: 49.8\nYa tenemos una métrica para evaluar qué tan buena ha sido esta partición tomando como punto de corte 1.5. Y cómo serán las demás particiones posibles?\nUna vez tengamos la función de costo de todas las particiones posibles, el modelo escogerá aquella en la cual el costo será el menor. Ok, ya tenemos el primer nodo.\n\n\n\n5.4.4 Aplicación con una predictora numérica\n\nmi_receta_arbol &lt;- recipe(aml_index ~ pbi_pc, data = training_data)\n\n\narbol &lt;- decision_tree(min_n=5) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\n\nwf_arbol &lt;-  workflow()  |&gt; \n  add_recipe(mi_receta_arbol) |&gt; \n  add_model(arbol)\n\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 59 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 59 66.593350 5.266441  \n   2) pbi_pc&gt;=3813.495 39 18.884070 4.723333  \n     4) pbi_pc&gt;=15749.22 15  3.088373 4.074667  \n       8) pbi_pc&gt;=54674.97 3  0.706400 3.620000 *\n       9) pbi_pc&lt; 54674.97 12  1.606767 4.188333 *\n     5) pbi_pc&lt; 15749.22 24  5.539462 5.128750 *\n   3) pbi_pc&lt; 3813.495 20 13.773500 6.325500  \n     6) pbi_pc&gt;=1118.465 11  4.157018 5.922727 *\n     7) pbi_pc&lt; 1118.465 9  5.650956 6.817778  \n      14) pbi_pc&gt;=553.69 7  4.194143 6.627143  \n        28) pbi_pc&lt; 735.42 2  0.605000 6.090000 *\n        29) pbi_pc&gt;=735.42 5  2.781280 6.842000  \n          58) pbi_pc&gt;=823.395 3  0.673400 6.530000 *\n          59) pbi_pc&lt; 823.395 2  1.377800 7.310000 *\n      15) pbi_pc&lt; 553.69 2  0.312050 7.485000 *\n\n\nEl resultado del modelo lo podemos visualizar con:\n\nlibrary(rpart.plot)\n\nCargando paquete requerido: rpart\n\n\n\nAdjuntando el paquete: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (model.frame: objeto '..y' no encontrado).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\nEn este caso, el primer número de cada recuadro es el valor promedio de y (aml_index) en cada sección. Luego aparece el promedio del nodo original que se va a tal o cual nodo.\n\n\n5.4.5 Aplicación con dos predictoras numéricas\n\nmi_receta_arbol &lt;- recipe(aml_index ~ pbi_pc + rule_of_law, data = training_data)\n\narbol &lt;- decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\nwf_arbol &lt;-  workflow()  |&gt; \n  add_recipe(mi_receta_arbol) |&gt; \n  add_model(arbol)\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 59 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 59 66.593350 5.266441  \n   2) pbi_pc&gt;=3813.495 39 18.884070 4.723333  \n     4) pbi_pc&gt;=15749.22 15  3.088373 4.074667 *\n     5) pbi_pc&lt; 15749.22 24  5.539462 5.128750  \n      10) rule_of_law&gt;=0.5721363 7  0.404600 4.780000 *\n      11) rule_of_law&lt; 0.5721363 17  3.932906 5.272353 *\n   3) pbi_pc&lt; 3813.495 20 13.773500 6.325500  \n     6) pbi_pc&gt;=1118.465 11  4.157018 5.922727 *\n     7) pbi_pc&lt; 1118.465 9  5.650956 6.817778 *\n\n\nEl resultado del modelo lo podemos visualizar con:\n\nlibrary(rpart.plot)\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (model.frame: objeto '..y' no encontrado).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\n\n\n5.4.6 Aplicación con dos predictoras numéricas y un factor\n\nmi_receta_arbol &lt;- recipe(aml_index ~ pbi_pc + \n                            rule_of_law+\n                            continente, \n                          data = training_data)\n\narbol &lt;- decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\nwf_arbol &lt;-  workflow()  |&gt; \n  add_recipe(mi_receta_arbol) |&gt; \n  add_model(arbol)\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 59 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 59 66.5933500 5.266441  \n   2) pbi_pc&gt;=3813.495 39 18.8840700 4.723333  \n     4) pbi_pc&gt;=15749.22 15  3.0883730 4.074667 *\n     5) pbi_pc&lt; 15749.22 24  5.5394620 5.128750  \n      10) rule_of_law&gt;=0.5721363 7  0.4046000 4.780000 *\n      11) rule_of_law&lt; 0.5721363 17  3.9329060 5.272353 *\n   3) pbi_pc&lt; 3813.495 20 13.7735000 6.325500  \n     6) continente=Americas,Asia 7  0.9899714 5.704286 *\n     7) continente=Africa 13  8.6276000 6.660000 *\n\n\nLo visualizamos:\n\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (model.frame: objeto '..y' no encontrado).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\n\n\n5.4.7 Importancia de las variables\nLa importancia de variables es un ranking que indica cuánto aporta cada predictor a las mejoras de predicción del árbol.\nPara qué sirve: priorizar predictores, simplificar cuestionarios/inputs (coste), detectar variables inútiles o fugas de información, y comunicar qué factores “pesan” más en términos predictivos.\n\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  vip()\n\n\n\n\n\n\n\n\n\n\n5.4.8 Comparación RL y Árbol de Decisión\nConsiderando las mismas variables:\n\nmi_receta &lt;- recipe(aml_index ~ pbi_pc + rule_of_law+ continente, \n                          data = training_data)\n\n¿Cuál modelo logrará la mejor predicción, la regresión lineal o el árbol de decisión?\n\narbol &lt;- decision_tree() |&gt; \n         set_engine(\"rpart\") |&gt; \n         set_mode(\"regression\")\n\nwf_arbol &lt;-  workflow()  |&gt; \n               add_recipe(mi_receta_arbol) |&gt; \n               add_model(arbol)\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol |&gt; \n            predict(testing_data) |&gt; \n            bind_cols(valor_real=testing_data$aml_index) |&gt; \n            rmse(truth = valor_real,\n                 estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.602\n\n\nEn este caso, la regresión lineal ha probado seguir siendo la mejor opción (RMSE = 0.56). No obstante, se encuentran muy cerca en cuanto a su rendimiento. \n\n\n5.4.9 Problema!\n\nSobreajuste (overfitting): Crecen hasta “memorizar” el ruido si no se limitan.\nAlta varianza / inestabilidad: Pequeños cambios en los datos pueden cambiar mucho la estructura del árbol.\n\nSOLUCIÓN: RANDOM FOREST!",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>RL con preprocesamiento y modelos basados en árboles</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#regresión-lineal",
    "href": "C5_RL2.html#regresión-lineal",
    "title": "5  Preprocesamiento y validación cruzada",
    "section": "5.3 Regresión lineal",
    "text": "5.3 Regresión lineal\n\n5.3.1 Probando la nueva receta\nRecapitulamos la receta que tenemos con preprocesamiento:\n\nmi_receta\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: rule_of_law\n\n\n• Centering and scaling for: pbi_pc rule_of_law\n\n\n• Dummy variables from: continente\n\n\nGeneramos nuestro modelo:\n\nmi_modelo_lm &lt;- linear_reg() |&gt; \n                   set_engine(\"lm\")\n\nflujo_ml&lt;-workflow() |&gt; \n            add_recipe(mi_receta) |&gt; \n             add_model(mi_modelo_lm)\n\nmodelo_entrenado &lt;- flujo_ml %&gt;% \n                      fit(data = training_data) # Con el de ENTRENAMIENTO!\n\nSi deseamos ver los coeficientes (estimates) del modelo podemos solicitarlo con tidy():\n\ntidy(modelo_entrenado)\n\n# A tibble: 7 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           6.12       0.172    35.7   3.20e-38\n2 rule_of_law          -0.441      0.154    -2.87  5.88e- 3\n3 pbi_pc               -0.0921     0.159    -0.580 5.65e- 1\n4 continente_Americas  -1.14       0.242    -4.70  1.95e- 5\n5 continente_Asia      -1.00       0.235    -4.26  8.58e- 5\n6 continente_Europe    -1.31       0.274    -4.79  1.45e- 5\n7 continente_Oceania   -0.600      0.460    -1.30  1.98e- 1\n\n\nQué puedes notar en los coeficientes?\n\n\n5.3.2 Evaluamos el modelo con test\nUna vez que el modelo ha sido entrenado, el siguiente paso es evaluar su desempeño. La evaluación consiste en medir qué tan bien el modelo logra predecir los valores de la variable de interés, comparando las predicciones con los valores reales. Para ello se utilizan métricas de error, como el RMSE (Root Mean Squared Error), que nos permiten cuantificar la calidad del ajuste y, sobre todo, estimar su capacidad de generalización cuando se aplica a nuevos datos.\nPara ello, primero utilizamos el modelo generado para predecir con la nueva data:\n\nprediccion_test&lt;-modelo_entrenado |&gt; \n                  predict(testing_data) |&gt; \n                  bind_cols(valor_real=testing_data$aml_index)\nprediccion_test\n\n# A tibble: 20 × 2\n   .pred valor_real\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  5.10       4.75\n 2  3.87       4.13\n 3  5.18       5.89\n 4  6.86       6.75\n 5  5.24       5.21\n 6  3.90       3   \n 7  6.67       7.69\n 8  5.20       4.9 \n 9  6.59       7.17\n10  4.08       3.47\n11  6.62       7.43\n12  6.31       5.63\n13  5.47       5.21\n14  5.25       4.81\n15  6.11       5.23\n16  4.31       3.57\n17  4.17       3.96\n18  4.43       4.05\n19  6.54       6.95\n20  5.11       5.08\n\n\nAhora vamos a medir cómo funciona nuestro modelo utilizándolo con data de testeo. Recuerda que en nuestra data de testeo podemos validar en contraste con el valor real.\n\nyardstick::rmse(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.567\n\n\nEl R cuadrado es otra medida, aunque menos utilizada en machine learning, que dice cuánto es explicado por nuestro modelo.\n\nrsq(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.867\n\n\nGraficamos:\n\nprediccion_test |&gt; \n  ggplot()+\n  aes(x = valor_real, y = .pred)+\n  geom_point(color = \"blue\", size = 2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +  # línea 1:1\n  labs(\n    x = \"Valor real\",\n    y = \"Valor predicho\",\n    title = \"Valores reales vs predicciones\"\n  ) +\n  xlim(0,10)+ ylim(0,10)+\n  theme_minimal()",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preprocesamiento y validación cruzada</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#random-forest",
    "href": "C5_RL2.html#random-forest",
    "title": "5  RL con preprocesamiento y modelos basados en árboles",
    "section": "5.5 Random Forest",
    "text": "5.5 Random Forest\n\n5.5.1 Definición\nRandom Forest es un modelo de ensamble que combina muchos árboles de decisión para mejorar la capacidad de generalización. Cada árbol se entrena sobre una muestra bootstrap (con reemplazo) del conjunto de datos y, en cada división, considera solo un subconjunto aleatorio de predictores (mtry).\n\nEn regresión, la predicción final es el promedio de los árboles. Esta aleatoriedad + agregación reduce la varianza del árbol individual y hace al modelo robusto sin requerir preprocesamientos complejos.\n\n\n5.5.2 Ventajas y desventajas\nVentajas:\n\nCaptura no linealidades e interacciones automáticamente.\nSuele rendir bien “out-of-the-box” y es estable frente a ruido.\nNo requiere normalización de variables; funciona con muchas x.\nProporciona importancia de variables.\n\nDesventajas:\n\nMenor interpretabilidad que un árbol único.\nPuede ser pesado en memoria/tiempo con muchos árboles o datos muy grandes.\n\n\n\n5.5.3 Aplicación con tidymodels\nVamos con la misma receta del árbol:\n\nmi_receta &lt;- recipe(aml_index ~ pbi_pc + rule_of_law+ continente, \n                          data = training_data)\n\nAplicamos el workflow utilizando la función rand_forest():\n\nbosque_model &lt;- rand_forest(trees = 1000, min_n = 5) |&gt; \n              set_engine(\"ranger\", importance = \"permutation\") |&gt; \n              set_mode(\"regression\")\n\nIniciamos el workflow():\n\nbosque_wf &lt;- workflow() |&gt; \n                add_recipe(mi_receta) |&gt; \n                add_model(bosque_model)\n\nAhora fiteamos el modelo y calculamos el RMSE.\n\nfit_bosque &lt;- bosque_wf |&gt; \n              fit(training_data)\n\n\n\n5.5.4 Comparación RL, DT, RF\nExcelente!!! Ahora conseguir una mejor performance predictiva!!\n\nfit_bosque |&gt; \n            predict(testing_data) |&gt; \n            bind_cols(valor_real=testing_data$aml_index) |&gt; \n            rmse(truth = valor_real,\n                 estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.513\n\n\nTambién podemos comprobar la importancia de las variables:\n\nfit_bosque |&gt; \n  extract_fit_engine() |&gt; \n  vip()",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>RL con preprocesamiento y modelos basados en árboles</span>"
    ]
  },
  {
    "objectID": "C6_arboles.html",
    "href": "C6_arboles.html",
    "title": "6  Árboles de decisión y Random Forest",
    "section": "",
    "text": "6.1 Árboles de decisión\nLuego de utilizar la regresión lineal como un método de aprendizaje supervisado, podemos ahora saltar a un algoritmo más reciente: los árboles de decisión.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y Random Forest</span>"
    ]
  },
  {
    "objectID": "C6_arboles.html#árboles-de-decisión",
    "href": "C6_arboles.html#árboles-de-decisión",
    "title": "6  Árboles de decisión y Random Forest",
    "section": "",
    "text": "6.1.1 Definición\nUn árbol de decisión es un modelo predictivo que representa un conjunto de reglas “si… entonces…” organizadas en forma de árbol. Parte de un nodo raíz (todos los datos) y, en cada nodo interno, divide los datos según el valor de una variable predictora para reducir la varianza /MSE.\n\nEl proceso continúa hasta cumplir ciertos criterios de parada (profundidad, mínimo de casos, complejidad) y las hojas entregan la predicción final: promedio de la variable objetivo (regresión).\nTambién se puede aplicar a clasificación. Eso lo veremos más adelante!\n\n\n6.1.2 Ventajas y desventajas\nVentajas:\n\nInterpretables: reglas “si… entonces…” fáciles de explicar.\nCapturan no linealidades e interacciones sin ingeniería previa.\nPoco preprocesamiento: no requieren normalizar; toleran outliers mejor que modelos lineales.\nMixtos: funcionan con variables numéricas y categóricas.\nRápidos de entrenar y útiles para generar hipótesis/segmentaciones.\n\nDesventajas:\n\nSobreajustan e inestables: pequeños cambios en los datos alteran mucho el árbol (alta varianza).\nFronteras en “escalera” y predicciones por tramos; no extrapolan bien.\nSesgo en splits hacia variables con muchos niveles; cuidado al interpretar “importancia”.\n\n\n\n6.1.3 Ejemplo simple de partición\nImagina que tenemos la siguiente data:\n\ndf &lt;- tibble::tibble(\n  id             = 1:10,\n  horas_estudio  = c(1, 2, 2, 3, 3, 5, 4, 6, 2, 5),\n  ingreso           = c(22, 35, 33, 40, 45, 35, 40, 21, 28, 24)\n) \n\nLo visualizamos en un gráfico de dispersión.\n\ndf |&gt; \n  ggplot()+\n  aes(x=horas_estudio, y=ingreso)+\n  geom_point()\n\n\n\n\n\n\n\n\nGeneramos una primera partición:\n\ndf |&gt; \n  ggplot()+\n  aes(x=horas_estudio, y=ingreso)+\n  geom_point()+\n  geom_vline(xintercept = 1.5, linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\nDentro de cada una de estas secciones vamos a calcular un indicador, el cual es el RMSE a la media de cada uno.\nPara el caso de la primera partición, como el error es 0, al tener sólo una observación, su RMSE será 0.\nPara el caso de la segunda partición, calculamos los errores, los elevamos al cuadrado, los sumamos y sacamos promedio:\n\n\ndf |&gt; \n  filter(horas_estudio&gt;=1.5) |&gt; \n  summarise(mean(ingreso))\n\n# A tibble: 1 × 1\n  `mean(ingreso)`\n            &lt;dbl&gt;\n1            33.4\n\n\n\ndf |&gt; \n  filter(horas_estudio&gt;=1.5) |&gt; \n  mutate(error_ingreso=ingreso-mean(ingreso)) |&gt;  # calculamos el error\n  mutate(error_ingreso_al2=error_ingreso**2) |&gt; \n  summarise(RMSE=sum(error_ingreso_al2)/n()) |&gt; \n  mutate(RMSE*0.9)\n\n# A tibble: 1 × 2\n   RMSE `RMSE * 0.9`\n  &lt;dbl&gt;        &lt;dbl&gt;\n1  55.4         49.8\n\n\n\nAhora, conociendo el grado de dispersión de cada una de las agrupaciones, podemos encontrar un puntaje para el umbral seleccionado.\nEste umbral tendrá un puntaje que será la suma de la dispersión de cada sección. En este caso: 49.8\nYa tenemos una métrica para evaluar qué tan buena ha sido esta partición tomando como punto de corte 1.5. Y cómo serán las demás particiones posibles?\nUna vez tengamos la función de costo de todas las particiones posibles, el modelo escogerá aquella en la cual el costo será el menor. Ok, ya tenemos el primer nodo.\n\n\n\n6.1.4 Aplicación con una predictora numérica\nUtilizamos nuestra base de datos:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tidymodels)\ndata &lt;- read_xlsx(\"data/AML_2.xlsx\")\ndata&lt;- data |&gt; \n        filter(!is.na(aml_index))\n\n\nset.seed(2025)\nindex &lt;- initial_split(data)     \ntraining_data &lt;- training(index)  \ntesting_data &lt;- testing(index)    \n\n\nmi_receta_arbol &lt;- recipe(aml_index ~ pbi_pc, data = training_data)\n\n\narbol &lt;- decision_tree(min_n=5) |&gt; \n            set_engine(\"rpart\") |&gt; \n            set_mode(\"regression\")\n\n\nwf_arbol &lt;-  workflow()  |&gt; \n  add_recipe(mi_receta_arbol) |&gt; \n  add_model(arbol)\n\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 59 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 59 66.593350 5.266441  \n   2) pbi_pc&gt;=3813.495 39 18.884070 4.723333  \n     4) pbi_pc&gt;=15749.22 15  3.088373 4.074667  \n       8) pbi_pc&gt;=54674.97 3  0.706400 3.620000 *\n       9) pbi_pc&lt; 54674.97 12  1.606767 4.188333 *\n     5) pbi_pc&lt; 15749.22 24  5.539462 5.128750 *\n   3) pbi_pc&lt; 3813.495 20 13.773500 6.325500  \n     6) pbi_pc&gt;=1118.465 11  4.157018 5.922727 *\n     7) pbi_pc&lt; 1118.465 9  5.650956 6.817778  \n      14) pbi_pc&gt;=553.69 7  4.194143 6.627143  \n        28) pbi_pc&lt; 735.42 2  0.605000 6.090000 *\n        29) pbi_pc&gt;=735.42 5  2.781280 6.842000  \n          58) pbi_pc&gt;=823.395 3  0.673400 6.530000 *\n          59) pbi_pc&lt; 823.395 2  1.377800 7.310000 *\n      15) pbi_pc&lt; 553.69 2  0.312050 7.485000 *\n\n# ══ Workflow [trained] ════════════════════════════════════════════════════\n# Preprocessor: Recipe\n# Model: decision_tree()\n# \n# ── Preprocessor ──────────────────────────────────────────────────────────\n# 0 Recipe Steps\n# \n# ── Model ─────────────────────────────────────────────────────────────────\n# n= 59 \n# \n# node), split, n, deviance, yval\n#       * denotes terminal node\n# \n#  1) root 59 66.593350 5.266441  \n#    2) pbi_pc&gt;=3813.495 39 18.884070 4.723333  \n#      4) pbi_pc&gt;=15749.22 15  3.088373 4.074667  \n#        8) pbi_pc&gt;=54674.97 3  0.706400 3.620000 *\n#        9) pbi_pc&lt; 54674.97 12  1.606767 4.188333 *\n#      5) pbi_pc&lt; 15749.22 24  5.539462 5.128750 *\n#    3) pbi_pc&lt; 3813.495 20 13.773500 6.325500  \n#      6) pbi_pc&gt;=1118.465 11  4.157018 5.922727 *\n#      7) pbi_pc&lt; 1118.465 9  5.650956 6.817778  \n#       14) pbi_pc&gt;=553.69 7  4.194143 6.627143  \n#         28) pbi_pc&lt; 735.42 2  0.605000 6.090000 *\n#         29) pbi_pc&gt;=735.42 5  2.781280 6.842000  \n#           58) pbi_pc&gt;=823.395 3  0.673400 6.530000 *\n#           59) pbi_pc&lt; 823.395 2  1.377800 7.310000 *\n#       15) pbi_pc&lt; 553.69 2  0.312050 7.485000 *\n\nEl resultado del modelo lo podemos visualizar con:\n\nlibrary(rpart.plot)\n\nCargando paquete requerido: rpart\n\n\n\nAdjuntando el paquete: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (model.frame: objeto '..y' no encontrado).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\nEn este caso, el primer número de cada recuadro es el valor promedio de y (aml_index) en cada sección. Luego aparece el promedio del nodo original que se va a tal o cual nodo.\n\n\n6.1.5 Aplicación con dos predictoras numéricas\n\nmi_receta_arbol &lt;- recipe(aml_index ~ pbi_pc + rule_of_law, data = training_data)\n\narbol &lt;- decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\nwf_arbol &lt;-  workflow()  |&gt; \n  add_recipe(mi_receta_arbol) |&gt; \n  add_model(arbol)\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 59 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 59 66.593350 5.266441  \n   2) pbi_pc&gt;=3813.495 39 18.884070 4.723333  \n     4) pbi_pc&gt;=15749.22 15  3.088373 4.074667 *\n     5) pbi_pc&lt; 15749.22 24  5.539462 5.128750  \n      10) rule_of_law&gt;=0.5721363 7  0.404600 4.780000 *\n      11) rule_of_law&lt; 0.5721363 17  3.932906 5.272353 *\n   3) pbi_pc&lt; 3813.495 20 13.773500 6.325500  \n     6) pbi_pc&gt;=1118.465 11  4.157018 5.922727 *\n     7) pbi_pc&lt; 1118.465 9  5.650956 6.817778 *\n\n\nEl resultado del modelo lo podemos visualizar con:\n\nlibrary(rpart.plot)\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (model.frame: objeto '..y' no encontrado).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\n\n\n6.1.6 Aplicación con dos predictoras numéricas y un factor\n\nmi_receta_arbol &lt;- recipe(aml_index ~ pbi_pc + \n                            rule_of_law+\n                            continente, \n                          data = training_data)\n\narbol &lt;- decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\nwf_arbol &lt;-  workflow()  |&gt; \n  add_recipe(mi_receta_arbol) |&gt; \n  add_model(arbol)\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 59 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 59 66.5933500 5.266441  \n   2) pbi_pc&gt;=3813.495 39 18.8840700 4.723333  \n     4) pbi_pc&gt;=15749.22 15  3.0883730 4.074667 *\n     5) pbi_pc&lt; 15749.22 24  5.5394620 5.128750  \n      10) rule_of_law&gt;=0.5721363 7  0.4046000 4.780000 *\n      11) rule_of_law&lt; 0.5721363 17  3.9329060 5.272353 *\n   3) pbi_pc&lt; 3813.495 20 13.7735000 6.325500  \n     6) continente=Americas,Asia 7  0.9899714 5.704286 *\n     7) continente=Africa 13  8.6276000 6.660000 *\n\n\nLo visualizamos:\n\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (model.frame: objeto '..y' no encontrado).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\n\n\n6.1.7 Importancia de las variables\nLa importancia de variables es un ranking que indica cuánto aporta cada predictor a las mejoras de predicción del árbol.\nPara qué sirve: priorizar predictores, simplificar cuestionarios/inputs (coste), detectar variables inútiles o fugas de información, y comunicar qué factores “pesan” más en términos predictivos.\n\nlibrary(vip)\nfit_arbol |&gt; \n  extract_fit_engine() |&gt; \n  vip()\n\n\n\n\n\n\n\n\n\n\n6.1.8 Comparación RL y Árbol de Decisión\nConsiderando las mismas variables:\n\nmi_receta &lt;- recipe(aml_index ~ pbi_pc + rule_of_law+ continente, \n                          data = training_data)\n\n¿Cuál modelo logrará la mejor predicción, la regresión lineal o el árbol de decisión?\n\narbol &lt;- decision_tree() |&gt; \n         set_engine(\"rpart\") |&gt; \n         set_mode(\"regression\")\n\nwf_arbol &lt;-  workflow()  |&gt; \n               add_recipe(mi_receta_arbol) |&gt; \n               add_model(arbol)\n\nfit_arbol &lt;- wf_arbol |&gt; \n              fit(training_data)\n\nfit_arbol |&gt; \n            predict(testing_data) |&gt; \n            bind_cols(valor_real=testing_data$aml_index) |&gt; \n            rmse(truth = valor_real,\n                 estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.602\n\n\nEn este caso, la regresión lineal ha probado seguir siendo la mejor opción (RMSE = 0.56). No obstante, se encuentran muy cerca en cuanto a su rendimiento.\n\n\n\n6.1.9 Encontrar el balance entre sesgo y varianza\nEn el aprendizaje automático, el desempeño de un modelo predictivo puede entenderse a partir de dos fuentes principales de error: sesgo y varianza.\nALTO SESGO = UNDERFITTING\n\nEl sesgo (bias) hace referencia al error sistemático que introduce un modelo al imponer supuestos demasiado rígidos sobre la relación entre las variables. Un modelo con alto sesgo es incapaz de capturar patrones relevantes de los datos y produce predicciones alejadas de la realidad.\nEste fenómeno se conoce como subajuste (underfitting): el modelo resulta demasiado simple para la complejidad del problema.\nALTA VARIANZA = OVERFITTING\n\nRefleja la sensibilidad del modelo a las particularidades de los datos de entrenamiento. Un modelo con alta varianza se ajusta en exceso a las muestras disponibles, incluso capturando ruido o fluctuaciones aleatorias.\nComo consecuencia, presenta un buen desempeño en el conjunto de entrenamiento, pero pierde capacidad de generalización frente a nuevos datos. A este problema se le denomina sobreajuste (overfitting).\nBALANCE\n\nEl objetivo del modelado es encontrar un equilibrio adecuado entre ambos, logrando así un modelo con capacidad de generalizar: ni tan simple que subajuste, ni tan complejo que sobreajuste.\n\n\n\n\n\n\nTip\n\n\n\nPara solucionar este problema en el caso de los árboles podemos probar otro algoritmo denominado RANDON FOREST!",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y Random Forest</span>"
    ]
  },
  {
    "objectID": "C6_arboles.html#random-forest",
    "href": "C6_arboles.html#random-forest",
    "title": "6  Árboles de decisión y Random Forest",
    "section": "6.2 Random Forest",
    "text": "6.2 Random Forest\n\n6.2.1 Definición\nRandom Forest es un modelo de ensamble que combina muchos árboles de decisión para mejorar la capacidad de generalización. Cada árbol se entrena sobre una muestra bootstrap (con reemplazo) del conjunto de datos y, en cada división, considera solo un subconjunto aleatorio de predictores.\n\nEn regresión, la predicción final es el promedio de los árboles. Esta aleatoriedad + agregación reduce la varianza del árbol individual y hace al modelo robusto sin requerir preprocesamientos complejos.\n\n\n6.2.2 Ventajas y desventajas\nVentajas:\n\nCaptura no linealidades e interacciones automáticamente.\nSuele rendir bien “out-of-the-box” y es estable frente a ruido.\nNo requiere normalización de variables; funciona con muchas x.\nProporciona importancia de variables.\n\nDesventajas:\n\nMenor interpretabilidad que un árbol único.\nPuede ser pesado en memoria/tiempo con muchos árboles o datos muy grandes.\n\n\n\n6.2.3 Aplicación con tidymodels\nVamos con la misma receta del árbol:\n\nmi_receta &lt;- recipe(aml_index ~ pbi_pc + rule_of_law+ continente, \n                          data = training_data)\n\nAplicamos el workflow utilizando la función rand_forest():\n\nbosque_model &lt;- rand_forest(trees = 1000, \n                            min_n = 5) |&gt; \n                  set_engine(\"ranger\", importance = \"permutation\") |&gt; \n                  set_mode(\"regression\")\n\nIniciamos el workflow():\n\nbosque_wf &lt;- workflow() |&gt; \n                add_recipe(mi_receta) |&gt; \n                add_model(bosque_model)\n\nAhora fiteamos el modelo y calculamos el RMSE.\n\nfit_bosque &lt;- bosque_wf |&gt; \n              fit(training_data)\n\n\n\n6.2.4 Comparación RL, DT, RF\nExcelente!!! Ahora conseguir una mejor performance predictiva!!\n\nfit_bosque |&gt; \n            predict(testing_data) |&gt; \n            bind_cols(valor_real=testing_data$aml_index) |&gt; \n            rmse(truth = valor_real,\n                 estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.513\n\n\n\n\n6.2.5 Importancia de variables\nTambién podemos comprobar la importancia de las variables:\n\nfit_bosque |&gt; \n  extract_fit_engine() |&gt; \n  vip()",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y Random Forest</span>"
    ]
  },
  {
    "objectID": "C5_RL2.html#regresión-lineal-con-receta",
    "href": "C5_RL2.html#regresión-lineal-con-receta",
    "title": "5  Preprocesamiento y validación cruzada",
    "section": "5.3 Regresión lineal con receta",
    "text": "5.3 Regresión lineal con receta\n\n5.3.1 Probando la nueva receta\nRecapitulamos la receta que tenemos con preprocesamiento:\n\nmi_receta\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: rule_of_law\n\n\n• Centering and scaling for: pbi_pc rule_of_law\n\n\n• Dummy variables from: continente\n\n\nGeneramos nuestro modelo:\n\nmi_modelo_lm &lt;- linear_reg() |&gt; \n                   set_engine(\"lm\")\n\nflujo_ml&lt;-workflow() |&gt; \n            add_recipe(mi_receta) |&gt; \n             add_model(mi_modelo_lm)\n\nmodelo_entrenado &lt;- flujo_ml %&gt;% \n                      fit(data = training_data) # Con el de ENTRENAMIENTO!\n\nSi deseamos ver los coeficientes (estimates) del modelo podemos solicitarlo con tidy():\n\ntidy(modelo_entrenado)\n\n# A tibble: 7 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           6.12       0.172    35.7   3.20e-38\n2 rule_of_law          -0.441      0.154    -2.87  5.88e- 3\n3 pbi_pc               -0.0921     0.159    -0.580 5.65e- 1\n4 continente_Americas  -1.14       0.242    -4.70  1.95e- 5\n5 continente_Asia      -1.00       0.235    -4.26  8.58e- 5\n6 continente_Europe    -1.31       0.274    -4.79  1.45e- 5\n7 continente_Oceania   -0.600      0.460    -1.30  1.98e- 1\n\n\nQué puedes notar en los coeficientes?\n\n\n5.3.2 Evaluamos el modelo con test\nUna vez que el modelo ha sido entrenado, el siguiente paso es evaluar su desempeño. La evaluación consiste en medir qué tan bien el modelo logra predecir los valores de la variable de interés, comparando las predicciones con los valores reales. Para ello se utilizan métricas de error, como el RMSE (Root Mean Squared Error), que nos permiten cuantificar la calidad del ajuste y, sobre todo, estimar su capacidad de generalización cuando se aplica a nuevos datos.\nPara ello, primero utilizamos el modelo generado para predecir con la nueva data:\n\nprediccion_test&lt;-modelo_entrenado |&gt; \n                  predict(testing_data) |&gt; \n                  bind_cols(valor_real=testing_data$aml_index)\nprediccion_test\n\n# A tibble: 20 × 2\n   .pred valor_real\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  5.10       4.75\n 2  3.87       4.13\n 3  5.18       5.89\n 4  6.86       6.75\n 5  5.24       5.21\n 6  3.90       3   \n 7  6.67       7.69\n 8  5.20       4.9 \n 9  6.59       7.17\n10  4.08       3.47\n11  6.62       7.43\n12  6.31       5.63\n13  5.47       5.21\n14  5.25       4.81\n15  6.11       5.23\n16  4.31       3.57\n17  4.17       3.96\n18  4.43       4.05\n19  6.54       6.95\n20  5.11       5.08\n\n\nAhora vamos a medir cómo funciona nuestro modelo utilizándolo con data de testeo. Recuerda que en nuestra data de testeo podemos validar en contraste con el valor real.\n\nyardstick::rmse(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.567\n\n\nEl R cuadrado es otra medida, aunque menos utilizada en machine learning, que dice cuánto es explicado por nuestro modelo.\n\nrsq(prediccion_test,\n    truth = valor_real,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.867\n\n\nGraficamos:\n\nprediccion_test |&gt; \n  ggplot()+\n  aes(x = valor_real, y = .pred)+\n  geom_point(color = \"blue\", size = 2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +  # línea 1:1\n  labs(\n    x = \"Valor real\",\n    y = \"Valor predicho\",\n    title = \"Valores reales vs predicciones\"\n  ) +\n  xlim(0,10)+ ylim(0,10)+\n  theme_minimal()",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preprocesamiento y validación cruzada</span>"
    ]
  },
  {
    "objectID": "C6_arboles.html#tuneo-de-hiperparámetros",
    "href": "C6_arboles.html#tuneo-de-hiperparámetros",
    "title": "6  Árboles de decisión y Random Forest",
    "section": "6.3 Tuneo de hiperparámetros",
    "text": "6.3 Tuneo de hiperparámetros\n\nLos modelos de aprendizaje automático no solo aprenden a partir de los datos, sino que también dependen de una serie de hiperparámetros, es decir, configuraciones definidas por el usuario que controlan cómo se entrena el modelo. Estos hiperparámetros no se ajustan automáticamente durante el entrenamiento (a diferencia de los parámetros internos, como los coeficientes de una regresión), sino que deben seleccionarse por el usuario o analista.\nEl proceso de tuning o ajuste de hiperparámetros busca encontrar la combinación de valores que produzca el mejor equilibrio entre sesgo y varianza, logrando así un modelo con mayor capacidad de generalización. Para ello, se utilizan técnicas como la validación cruzada (cross-validation) que aprendimos la clase pasada, la cual permitirá evaluar el desempeño de múltiples configuraciones en diferentes particiones de los datos de entrenamiento.\n\n6.3.1 Tuneo de Random Forest\nAunque el Random Forest ofrece un buen desempeño incluso con sus valores por defecto, el ajuste de hiperparámetros puede mejorar significativamente su capacidad predictiva y eficiencia. Los parámetros más relevantes son mtry (número de predictores considerados en cada división), min_n (tamaño mínimo de los nodos terminales) y trees (número total de árboles). Ajustarlos permite encontrar un mejor balance entre sesgo, varianza y costo computacional, aunque el algoritmo ya se considera robusto frente a problemas de sobreajuste sin necesidad de un tuning intensivo.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y Random Forest</span>"
    ]
  },
  {
    "objectID": "C6_arboles.html#tuneo-de-hiperparámetros-randon-forest",
    "href": "C6_arboles.html#tuneo-de-hiperparámetros-randon-forest",
    "title": "6  Árboles de decisión y Random Forest",
    "section": "6.3 Tuneo de hiperparámetros: Randon Forest",
    "text": "6.3 Tuneo de hiperparámetros: Randon Forest\n\nLos modelos de aprendizaje automático no solo aprenden a partir de los datos, sino que también dependen de una serie de hiperparámetros, es decir, configuraciones definidas por el usuario que controlan cómo se entrena el modelo. Estos hiperparámetros no se ajustan automáticamente durante el entrenamiento (a diferencia de los parámetros internos, como los coeficientes de una regresión), sino que deben seleccionarse por el usuario o analista.\nEl proceso de tuning o ajuste de hiperparámetros busca encontrar la combinación de valores que produzca el mejor equilibrio entre sesgo y varianza, logrando así un modelo con mayor capacidad de generalización. Para ello, se utilizan técnicas como la validación cruzada (cross-validation) que aprendimos la clase pasada, la cual permitirá evaluar el desempeño de múltiples configuraciones en diferentes particiones de los datos de entrenamiento.\n\n6.3.1 Indicamos los hiperparámetros a tunear\nAunque el Random Forest ofrece un buen desempeño incluso con sus valores por defecto, el ajuste de hiperparámetros puede mejorar significativamente su capacidad predictiva y eficiencia. Los parámetros más relevantes son mtry (número de predictores considerados en cada división), min_n (tamaño mínimo de los nodos terminales) y trees (número total de árboles). Ajustarlos permite encontrar un mejor balance entre sesgo, varianza y costo computacional, aunque el algoritmo ya se considera robusto frente a problemas de sobreajuste sin necesidad de un tuning intensivo.\nAplicamos el workflow utilizando la función rand_forest() y colocamos tune() en el hiperparámetro que deseamos tunear. En este caso seleccionamos min_n, el cual indica el número mínimo de casos que existirá en las ramificaciones.\n\nbosque_model_tune &lt;- rand_forest(trees = 1000, \n                            min_n = tune()) |&gt; # OJO CON tune()!!!!!!!\n                  set_engine(\"ranger\", \n                             importance = \"permutation\") |&gt; \n                  set_mode(\"regression\")\n\nIniciamos el workflow():\n\nbosque_wf_tune &lt;- workflow() |&gt; \n                add_recipe(mi_receta) |&gt; \n                add_model(bosque_model_tune)\n\n\n\n6.3.2 Creamos un remuestreo CV\nPrimero, crearemos un conjunto de remuestreos de validación cruzada para el ajuste. Esto es necesario porque no podemos aprender los valores correctos al entrenar un solo modelo, pero sí podemos entrenar varios modelos y ver cuáles funcionan mejor.\n\nset.seed(2025)\nfolds &lt;- vfold_cv(training_data, v= 5)\n\nAhora seleccionamos una grilla (o una tabla) con los valores que vamos a probar en los distintos ajustes:\n\ntune_rf &lt;- tune_grid(bosque_wf_tune,\n                     resamples = folds, \n                     grid = 10)\n\ntune_rf\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits          id    .metrics          .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [47/12]&gt; Fold1 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [47/12]&gt; Fold2 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [47/12]&gt; Fold3 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [47/12]&gt; Fold4 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [48/11]&gt; Fold5 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 4]&gt;\n\n#       splits      id    .metrics   .notes  \n# &lt;S3: vfold_split&gt; Fold1   &lt;tibble&gt;    &lt;tibble&gt;    \n# &lt;S3: vfold_split&gt; Fold2   &lt;tibble&gt;    &lt;tibble&gt;    \n# &lt;S3: vfold_split&gt; Fold3   &lt;tibble&gt;    &lt;tibble&gt;    \n# &lt;S3: vfold_split&gt; Fold4   &lt;tibble&gt;    &lt;tibble&gt;    \n# &lt;S3: vfold_split&gt; Fold5   &lt;tibble&gt;    &lt;tibble&gt;    \n\nVeamos los resultados.\n\ntune_rf |&gt; \n  collect_metrics() |&gt; \n  filter(.metric==\"rmse\")\n\n# A tibble: 10 × 7\n   min_n .metric .estimator  mean     n std_err .config         \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1     2 rmse    standard   0.627     5  0.0558 pre0_mod01_post0\n 2     6 rmse    standard   0.619     5  0.0514 pre0_mod02_post0\n 3    10 rmse    standard   0.622     5  0.0497 pre0_mod03_post0\n 4    15 rmse    standard   0.633     5  0.0518 pre0_mod04_post0\n 5    19 rmse    standard   0.633     5  0.0487 pre0_mod05_post0\n 6    22 rmse    standard   0.638     5  0.0491 pre0_mod06_post0\n 7    25 rmse    standard   0.649     5  0.0480 pre0_mod07_post0\n 8    29 rmse    standard   0.661     5  0.0465 pre0_mod08_post0\n 9    34 rmse    standard   0.686     5  0.0388 pre0_mod09_post0\n10    40 rmse    standard   0.718     5  0.0341 pre0_mod10_post0\n\n\nAl parecer los valores bajos de min_n funcionan mejor que los altos. Esto claramente se debe al poco número de casos que tiene nuestra base de datos. Probemos con otros valores bajos a ver si conseguimos una mejor performance.\n\nrf_grid &lt;-  grid_regular(min_n(range=c(2,8)), levels = 5)\nrf_grid\n\n# A tibble: 5 × 1\n  min_n\n  &lt;int&gt;\n1     2\n2     3\n3     5\n4     6\n5     8\n\n\nSolicitamos nuevamente el ajuste del modelo:\n\ntune_rf &lt;- tune_grid(bosque_wf_tune,\n                     resamples = folds, \n                     grid = rf_grid) ### AHORA LO ESPECIFICAMOS AÚN MÁS!\n\ntune_rf\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits          id    .metrics          .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [47/12]&gt; Fold1 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [47/12]&gt; Fold2 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [47/12]&gt; Fold3 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [47/12]&gt; Fold4 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [48/11]&gt; Fold5 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 4]&gt;\n\n\nVemos nuevamente los resultados:\n\ntune_rf |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 7\n   min_n .metric .estimator  mean     n std_err .config        \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n 1     2 rmse    standard   0.625     5  0.0556 pre0_mod1_post0\n 2     2 rsq     standard   0.690     5  0.0767 pre0_mod1_post0\n 3     3 rmse    standard   0.625     5  0.0546 pre0_mod2_post0\n 4     3 rsq     standard   0.690     5  0.0778 pre0_mod2_post0\n 5     5 rmse    standard   0.626     5  0.0517 pre0_mod3_post0\n 6     5 rsq     standard   0.694     5  0.0744 pre0_mod3_post0\n 7     6 rmse    standard   0.622     5  0.0519 pre0_mod4_post0\n 8     6 rsq     standard   0.695     5  0.0741 pre0_mod4_post0\n 9     8 rmse    standard   0.619     5  0.0518 pre0_mod5_post0\n10     8 rsq     standard   0.699     5  0.0769 pre0_mod5_post0\n\n\n\n\n6.3.3 Seleccionamos el mejor modelo\nUna vez probado con diversas posibilidades para nuestro hiperparámetro en cuestión, vamos a seleccionar nuestro mejor resultado para finalmente ir a modelar.\n\nbest_rmse &lt;- select_best(tune_rf,        # Usamos nuestro último modelo\n                         metric =\"rmse\") # Seleccionamos el criterio\n\nColocamos finalizar modelo, porque vamos a finalizar el modelo con el tune().\n\nfinal_rf &lt;- finalize_model(bosque_model_tune,\n                           best_rmse)\n\nfinal_rf\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 8\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n\n\nExploremos el modelo final.\n\nfinal_wf &lt;- workflow() |&gt; \n              add_recipe(mi_receta) |&gt; \n              add_model(final_rf)\n\nfinal_evaluation &lt;- final_wf |&gt; \n                      last_fit(index)\n\n\n\n\n\n\n\nTip\n\n\n\n¿last_fit() vs fit()?\n¿Qué hace last_fit():\n\nToma un split de rsample (creado con initial_split()).\nEntrena el workflow en training y evalúa en testing automáticamente.\nDevuelve métricas en test (collect_metrics()), además del workflow final ajustado (útil para predict()).\n\n¿Por qué se prefiere a fit() para la evaluación final\n\nEvita fugas de información: nunca toca el test durante el entrenamiento.\nEstandariza el “examen final”: un solo paso que entrena en train y reporta métricas en test.\nRespeta la recipe tal como quedó definida (pasos de preprocesamiento se estiman solo con train y se aplican a test correctamente).\nProduce un objeto listo para inspección y despliegue (extraer modelo, predicciones, etc.).\n\n\n\nFinalmente, solicitamos las métricas finales.\n\nfinal_evaluation |&gt; \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       0.524 pre0_mod0_post0\n2 rsq     standard       0.892 pre0_mod0_post0",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y Random Forest</span>"
    ]
  },
  {
    "objectID": "C7_log.html",
    "href": "C7_log.html",
    "title": "7  Regresión logística",
    "section": "",
    "text": "7.1 Antecedentes\nLa regresión logística es un modelo estadístico utilizado para predecir la probabilidad de que ocurra un evento binario (por ejemplo, éxito/fracaso, sí/no, 1/0) en función de una o más variables explicativas. A diferencia de la regresión lineal, que modela una variable continua, la regresión logística modela la probabilidad de pertenecer a una de las dos categorías posibles del resultado.\nLa regresión logística es un modelo estadístico utilizado para predecir la probabilidad de que ocurra un evento binario (por ejemplo, éxito/fracaso, sí/no, 1/0) en función de una o más variables explicativas.\nA diferencia de la regresión lineal, que modela una variable continua, la regresión logística modela una probabilidad y utiliza una transformación llamada función logit.\nLa función logit se define como el logaritmo del cociente entre la probabilidad de éxito y la probabilidad de fracaso:\n\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1 - p}\\right)\n\\]\nEl modelo logístico general se expresa como:\n\\[\n\\text{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k\n\\]\ndonde:\nA partir de esta relación, la probabilidad puede obtenerse despejando \\(p\\):\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k}}\n\\]\nEsta función logística garantiza que los valores estimados de \\(p\\) estén siempre entre 0 y 1, lo cual la hace apropiada para modelar probabilidades.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regresión logística</span>"
    ]
  },
  {
    "objectID": "C7_log.html#antecedentes",
    "href": "C7_log.html#antecedentes",
    "title": "7  Regresión logística",
    "section": "",
    "text": "\\(p\\) es la probabilidad de que ocurra el evento de interés (por ejemplo, que ( Y = 1 ));\n\\(\\beta_0\\) es el intercepto;\n\\(\\beta_1\\), \\(\\beta_2\\), … son los coeficientes asociados a las variables independientes \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_k\\).\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nRecuerda que cada coeficiente \\(\\beta_i\\) representa el cambio en el logaritmo de las odds (razón de probabilidades) asociado a un incremento de una unidad en la variable \\(X_i\\), manteniendo las demás constantes.",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regresión logística</span>"
    ]
  },
  {
    "objectID": "C7_log.html#proceso",
    "href": "C7_log.html#proceso",
    "title": "7  Regresión logística",
    "section": "7.2 Proceso",
    "text": "7.2 Proceso\nEn este caso vamos a utilizar la data “Predict Students’ Dropout and Academic Success”, la cual pueden encontrar en esta página web de UCI Machine Learning Datasets.\nPara fines de esta clase, he preparado un subset de esta data original, haciendo algunas ediciones.\n\n\n\n\n\n\n\n\nVariable\nNombre\nDetalle\n\n\n\n\nEstado civil\nmarital_status\n1 – single · 2 – married · 3 – widower · 4 – divorced · 5 – facto union · 6 – legally separated\n\n\nTurno\nday_or_night\nFactor (Sí – No)\n\n\nCalificación previa a ingresar\nprevious_qualification\nContinua (entre 0 a 200)\n\n\nNota de admisión\nadmission_grade\nContinua (entre 0 a 200)\n\n\nDesplazado\ndisplaced\nFactor (Sí – No)\n\n\nGénero\ngender\nFactor (Hombre – Mujer)\n\n\nEdad\nage\nEdad del estudiante al matricularse\n\n\nInternacional\ninternational\nFactor (Sí – No)\n\n\nEstado (variable objetivo)\ntarget\nFactor (Dropout – Graduate)\n\n\n\nLa data la pueden encontrar en el PAIDEIA.\n\nPaso 1: Apertura de dataset\nAbrimos nuestra base de datos:\n\nlibrary(rio)\nlibrary(tidyverse)\ndata&lt;-import(\"data/dropout2.csv\")\n\nA continuación, convertimos las variables categóricas a factores para que R las reconozca correctamente. Esto es esencial para el análisis y modelado posterior.\n\ndata$marital_status &lt;- as.factor(data$marital_status)\ndata$day_or_night &lt;- as.factor(data$day_or_night)\ndata$displaced &lt;- as.factor(data$displaced)\ndata$gender &lt;- as.factor(data$gender)\ndata$international &lt;- as.factor(data$international)\n\n\n\nPaso 2: EDA\nSiempre realiza tu análisis exploratorio de datos antes de iniciar el flujo de machine learning. Realizamos una exploración visual de los datos para identificar patrones y diferencias entre grupos según la variable objetivo (target).\nPrimero, comparamos la distribución de edad según el estado del estudiante:\n\ndata |&gt; \n  ggplot()+\n  aes(y= age, color=target) + \n  geom_boxplot() +\n  facet_wrap(~target)\n\n\n\n\n\n\n\n\nAhora, hacemos lo mismo para la variable admission_grade:\n\ndata |&gt; \n  ggplot()+\n  aes(y= admission_grade, color=target) + \n  geom_boxplot() +\n  facet_wrap(~target)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué buscamos? Separabilidad entre clases:\nBuscamos que las distribuciones de las variables numéricas sean diferentes entre las categorías de la variable objetivo (target). Si los boxplots se superponen completamente (son muy parecidos), esa variable probablemente no será útil para discriminar entre clases. Si están bien separados, es una señal de que la variable tiene poder predictivo.\n\n\nTambién podemos realizar una tabla resumen usando el paquete gtsummary. Mientras que los boxplots solo muestran variables numéricas de forma individual, tbl_summary() genera una tabla que incluye todas las variables del dataset (numéricas y categóricas) en un solo lugar, estratificadas por la variable objetivo.\n\nlibrary(gtsummary)\n\n\ntheme_gtsummary_language(\n  language = \"es\",\n  decimal.mark = \".\"\n)\n\n\ndata |&gt;\n  tbl_summary(by = target,  # Va a utilizar mi target como columnas\n              include = everything(),# Incluye todas las variables\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\")) \n\n\n\n\n\n\n\n\n\n\n\n\nCaracterística\nDropout\nN = 1,4211\nGraduate\nN = 2,2091\n\n\n\n\nmarital_status\n\n\n\n\n\n\n    casado\n179 (13%)\n148 (6.7%)\n\n\n    divorciado\n42 (3.0%)\n33 (1.5%)\n\n\n    separado\n4 (0.3%)\n1 (&lt;0.1%)\n\n\n    soltero\n1,184 (83%)\n2,015 (91%)\n\n\n    union_facto\n11 (0.8%)\n11 (0.5%)\n\n\n    viudo\n1 (&lt;0.1%)\n1 (&lt;0.1%)\n\n\nday_or_night\n\n\n\n\n\n\n    No\n207 (15%)\n201 (9.1%)\n\n\n    Si\n1,214 (85%)\n2,008 (91%)\n\n\nprevious_qualification\n131 (13)\n134 (13)\n\n\nadmission_grade\n125 (15)\n129 (14)\n\n\ndisplaced\n\n\n\n\n\n\n    No\n752 (53%)\n885 (40%)\n\n\n    Si\n669 (47%)\n1,324 (60%)\n\n\ngender\n\n\n\n\n\n\n    Hombre\n701 (49%)\n548 (25%)\n\n\n    Mujer\n720 (51%)\n1,661 (75%)\n\n\nage\n26 (9)\n22 (7)\n\n\ninternational\n\n\n\n\n\n\n    No\n1,389 (98%)\n2,155 (98%)\n\n\n    Si\n32 (2.3%)\n54 (2.4%)\n\n\n\n1 n (%); Media (DE)\n\n\n\n\n\n\n\n\n\n\nPaso 3: Split de la data\nAl igual que en las clases pasadas, dividimos los datos en conjuntos de entrenamiento y prueba (75%-25% por defecto). Esto nos permite evaluar el modelo en datos que no ha visto durante el entrenamiento, lo cual es fundamental para medir su capacidad de generalización.\n\nlibrary(tidymodels)\nset.seed(2025)\nsplit &lt;- initial_split(data)\ntraining_data &lt;- training(split)\ntesting_data &lt;- testing(split)\n\nCreamos 5 folds para validación cruzada en los datos de entrenamiento. La validación cruzada nos ayuda a obtener una estimación más robusta del rendimiento del modelo.\n\nfolds &lt;- vfold_cv(training_data, v= 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits             id   \n  &lt;list&gt;             &lt;chr&gt;\n1 &lt;split [2177/545]&gt; Fold1\n2 &lt;split [2177/545]&gt; Fold2\n3 &lt;split [2178/544]&gt; Fold3\n4 &lt;split [2178/544]&gt; Fold4\n5 &lt;split [2178/544]&gt; Fold5\n\n\n\n\n\n\n\n\nNota\n\n\n\nRecuerda que la validación cruzada es un paso intermedio crucial antes del entrenamiento final.\nSi entrenamos directamente con todos los datos de entrenamiento y evaluamos solo en el conjunto de prueba, obtenemos una única estimación del rendimiento. Con CV obtenemos múltiples estimaciones (5 en tu caso), lo que nos da:\n\nUn promedio más confiable del rendimiento real del modelo: Ese promedio va a ser un indicador más robusto que te puede guiar antes de dar el paso final de evaluar en el testing data.\nUna medida de la variabilidad: ¿el modelo es consistente o su rendimiento fluctúa mucho?\nPreservar la integridad del conjunto de prueba: El conjunto de prueba debe permanecer completamente intocado hasta el final. Es tu “juez final” que simula datos nuevos que el modelo nunca ha visto. Si lo usas durante el desarrollo, pierdes esta garantía.\n\n\n\n\n\nPaso 4: Construcción de WF: Receta + Modelo + WF\nDefinimos los tres componentes principales para nuestro proceso de machine learning: la receta, el modelo a utilizar y en el ensamble en un workflow()\n\nlr_receta &lt;- recipe(target~age, data=training_data)\n\nlr_modelo &lt;- logistic_reg() |&gt; \n              set_engine(\"glm\") |&gt; \n              set_mode(\"classification\")\n\nlr_wf &lt;- workflow() |&gt; \n          add_model(lr_modelo) |&gt; \n          add_recipe(lr_receta)\n\n\n\nPaso 5: Entrenamos el modelo con CV y analizamos rendimiento\nEntrenamos el modelo utilizando validación cruzada con los 5 folds definidos anteriormente. Evaluamos múltiples métricas (accuracy, specificity, sensitivity y roc_auc) para tener una visión completa del rendimiento.\n\n# Creamos un listado de métricas que solicitaremos\nmis_metricas &lt;- metric_set(accuracy, specificity, sensitivity, roc_auc)\n\n# Realizamos un entrenamiento con remuestreo utilizando los folds creados\n\nlr_fit_cv&lt;-lr_wf |&gt; \n          fit_resamples(resamples = folds, #Nuestros folds!\n                        metrics = mis_metricas,  # Nuestras métricas!\n                        control = control_resamples(save_pred = TRUE, \n                                                    verbose = TRUE))\n\ni Fold1: preprocessor 1/1\n\n\ni Fold1: preprocessor 1/1, model 1/1\n\n\ni Fold1: preprocessor 1/1, model 1/1 (predictions)\n\n\ni Fold2: preprocessor 1/1\n\n\ni Fold2: preprocessor 1/1, model 1/1\n\n\ni Fold2: preprocessor 1/1, model 1/1 (predictions)\n\n\ni Fold3: preprocessor 1/1\n\n\ni Fold3: preprocessor 1/1, model 1/1\n\n\ni Fold3: preprocessor 1/1, model 1/1 (predictions)\n\n\ni Fold4: preprocessor 1/1\n\n\ni Fold4: preprocessor 1/1, model 1/1\n\n\ni Fold4: preprocessor 1/1, model 1/1 (predictions)\n\n\ni Fold5: preprocessor 1/1\n\n\ni Fold5: preprocessor 1/1, model 1/1\n\n\ni Fold5: preprocessor 1/1, model 1/1 (predictions)\n\n\nPrimero revisamos las métricas para cada fold individualmente, lo que nos permite identificar la variabilidad en el rendimiento:\n\ncollect_metrics(lr_fit_cv,\n                summarize = FALSE)\n\n# A tibble: 20 × 5\n   id    .metric     .estimator .estimate .config        \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n 1 Fold1 accuracy    binary         0.639 pre0_mod0_post0\n 2 Fold1 specificity binary         0.905 pre0_mod0_post0\n 3 Fold1 sensitivity binary         0.242 pre0_mod0_post0\n 4 Fold1 roc_auc     binary         0.658 pre0_mod0_post0\n 5 Fold2 accuracy    binary         0.644 pre0_mod0_post0\n 6 Fold2 specificity binary         0.879 pre0_mod0_post0\n 7 Fold2 sensitivity binary         0.261 pre0_mod0_post0\n 8 Fold2 roc_auc     binary         0.684 pre0_mod0_post0\n 9 Fold3 accuracy    binary         0.643 pre0_mod0_post0\n10 Fold3 specificity binary         0.896 pre0_mod0_post0\n11 Fold3 sensitivity binary         0.263 pre0_mod0_post0\n12 Fold3 roc_auc     binary         0.673 pre0_mod0_post0\n13 Fold4 accuracy    binary         0.621 pre0_mod0_post0\n14 Fold4 specificity binary         0.877 pre0_mod0_post0\n15 Fold4 sensitivity binary         0.264 pre0_mod0_post0\n16 Fold4 roc_auc     binary         0.694 pre0_mod0_post0\n17 Fold5 accuracy    binary         0.662 pre0_mod0_post0\n18 Fold5 specificity binary         0.888 pre0_mod0_post0\n19 Fold5 sensitivity binary         0.313 pre0_mod0_post0\n20 Fold5 roc_auc     binary         0.693 pre0_mod0_post0\n\n\nLuego obtenemos el promedio de las métricas a través de todos los folds:\n\ncollect_metrics(lr_fit_cv)\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n std_err .config        \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary     0.642     5 0.00647 pre0_mod0_post0\n2 roc_auc     binary     0.680     5 0.00676 pre0_mod0_post0\n3 sensitivity binary     0.269     5 0.0118  pre0_mod0_post0\n4 specificity binary     0.889     5 0.00527 pre0_mod0_post0\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn la evaluación de modelos de clasificación, especialmente en regresión logística, se utilizan diversas métricas derivadas de la matriz de confusión para medir su desempeño.\nAccuracy (Exactitud): Mide la proporción total de predicciones correctas sobre el total de observaciones. Es útil cuando las clases están balanceadas.\nSensitivity (Sensibilidad o Recall): Indica la capacidad del modelo para detectar correctamente los casos positivos reales. Es prioritaria cuando es más costoso no detectar un positivo (por ejemplo, en detección de fraude o enfermedad).\nSpecificity (Especificidad) Mide la capacidad del modelo para identificar correctamente los negativos reales. Es especialmente importante cuando un falso positivo genera consecuencias indeseadas o costosas. Por ejemplo, en un sistema de detección de lavado de activos, una baja especificidad haría que muchas operaciones legítimas sean marcadas como sospechosas, generando sobrecarga en los analistas y pérdida de confianza en el sistema.\n\n\n\n\nPaso 6: Fiteamos y evaluamos con todo el entrenamiento con last_fit()\nEntrenamos el modelo final usando todos los datos de entrenamiento y lo evaluamos en el conjunto de prueba.\n\nmodelo_logistico_final &lt;- lr_wf |&gt; last_fit(split, metrics = mis_metricas)\n\nUna vez entrenado el modelo final podemos recolectar las predicciones que ha realizado (sobre nuestra data de test, es decir, la data nueva):\n\ncollect_predictions(modelo_logistico_final)\n\n# A tibble: 908 × 7\n   .pred_class .pred_Dropout .pred_Graduate id              target  .row .config\n   &lt;fct&gt;               &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;           &lt;fct&gt;  &lt;int&gt; &lt;chr&gt;  \n 1 Dropout             0.804          0.196 train/test spl… Gradu…     6 pre0_m…\n 2 Graduate            0.372          0.628 train/test spl… Dropo…     8 pre0_m…\n 3 Graduate            0.310          0.690 train/test spl… Dropo…    10 pre0_m…\n 4 Graduate            0.356          0.644 train/test spl… Gradu…    14 pre0_m…\n 5 Graduate            0.310          0.690 train/test spl… Gradu…    15 pre0_m…\n 6 Graduate            0.310          0.690 train/test spl… Gradu…    17 pre0_m…\n 7 Graduate            0.325          0.675 train/test spl… Gradu…    22 pre0_m…\n 8 Graduate            0.325          0.675 train/test spl… Gradu…    26 pre0_m…\n 9 Dropout             0.716          0.284 train/test spl… Dropo…    29 pre0_m…\n10 Graduate            0.356          0.644 train/test spl… Gradu…    40 pre0_m…\n# ℹ 898 more rows\n\n\nMira que para cada uno de los casos u observaciones de nuestra data de testo nos brinda:\n\nLa clase predicha.\nLa probabilidad que sea la clase/evento analizado (Dropout)\nLa probabilidad que no sea la clase/evento analizado (Graduate).\n\nAsimismo, podemos recolectar las métricas:\n\ncollect_metrics(modelo_logistico_final)\n\n# A tibble: 4 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.673 pre0_mod0_post0\n2 specificity binary         0.905 pre0_mod0_post0\n3 sensitivity binary         0.279 pre0_mod0_post0\n4 roc_auc     binary         0.694 pre0_mod0_post0\n\n\nLa matriz de confusión nos muestra el desempeño detallado del modelo en términos de clasificaciones correctas e incorrectas.\n\ncollect_predictions(modelo_logistico_final) %&gt;%\n  conf_mat(truth=target, \n           estimate=.pred_class) |&gt; \n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\nEJERCICIO: Prueba calculando las métricas (accuracy, specificity, sensitivity) manualmente usando la matriz de confusión!",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regresión logística</span>"
    ]
  },
  {
    "objectID": "C7_log.html#extra-interpretabilidad-de-la-regresión-logística",
    "href": "C7_log.html#extra-interpretabilidad-de-la-regresión-logística",
    "title": "7  Regresión logística",
    "section": "7.3 Extra: Interpretabilidad de la regresión logística",
    "text": "7.3 Extra: Interpretabilidad de la regresión logística\nLa regresión logística destaca por su alta interpretabilidad comparada con otros algoritmos de machine learning. A diferencia de modelos de “caja negra” como redes neuronales, random forests u otros como los gradient boosting, la regresión logística nos permite entender exactamente cómo y cuánto cada variable influye en la predicción a través de sus coeficientes. Los odds ratios (coeficientes exponenciados) tienen una interpretación directa y significativa: nos dicen cuántas veces aumentan o disminuyen las probabilidades del evento cuando cambia una variable predictora.\nPor ejemplo, podemos decir con precisión: “este estudiante tiene 3.5 veces más probabilidad de desertar debido a X característica”.\nPara nuestro ejercicio, podemos extraer los coeficientes del modelo para interpretar la relación entre las variables predictoras y la variable objetivo.\nPrimero, vemos los coeficientes en escala logarítmica:\n\nmodelo_logistico_final$.workflow[[1]] |&gt; \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   2.04     0.135        15.1 8.45e-52\n2 age          -0.0691   0.00551     -12.5 4.04e-36\n\n\nLuego, calculamos los coeficientes exponenciados (odds ratios), que son más fáciles de interpretar.\n\nmodelo_logistico_final$.workflow[[1]] |&gt; \n  tidy(exponentiate =TRUE)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    7.72    0.135        15.1 8.45e-52\n2 age            0.933   0.00551     -12.5 4.04e-36\n\n\n\n\n\n\n\n\nNota\n\n\n\nIntercepto ((Intercept) = 7.72)\nEsto representa el odds (razón de probabilidades) de que el evento ocurra (Dropout) cuando todas las variables predictoras valen 0. En este caso, el modelo predice que las probabilidades de graduarse son 7.72 veces mayores que las de no graduarse cuando la edad es 0 (lo cual, claro, no es interpretable literalmente, pero sirve como punto de referencia matemático).\nEdad (age = 0.933)\nEs el cambio en las odds del evento por cada año adicional.\n\nOR = 0.933 ⇒ por cada +1 año, las odds *disminuyen 6.7%**: (1 - 0.933 = 0.067).\n\nAhora bien, podría ser más intuitivo expresar el efecto en la dirección opuesta.\n\nLa inversa de 0.933 es 1 / 0.933 = 1.07.\nEsto significa que una disminución de un año en la edad aumenta las odds del evento en 7%, o, equivalentemente, que las odds se multiplican por 1.07..",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regresión logística</span>"
    ]
  },
  {
    "objectID": "C7_log.html#comparación-entre-rl-y-rf",
    "href": "C7_log.html#comparación-entre-rl-y-rf",
    "title": "7  Regresión logística",
    "section": "7.4 Comparación entre RL Y RF",
    "text": "7.4 Comparación entre RL Y RF\nCompara el rendimiento entre la regresión logística con el Random Forest. ¿Con qué algoritmo lograrías una mejor performance predictiva?",
    "crumbs": [
      "Métodos supervisados",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regresión logística</span>"
    ]
  }
]